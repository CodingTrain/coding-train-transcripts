{
  "id": "32wud5zbttncrp7m5yi7uj7cue",
  "version": "91ee9c0c3df30478510ff8c8a3a545add1ad0259ad3a9f78fba57fbc05ee64f7",
  "input": {
    "audio": "https://upcdn.io/FW25b4F/raw/coding-train/lz2L-sT8bG0.m4a"
  },
  "logs": "Transcribe with large-v2 model\nDetected language: English\n  0%|          | 0/107926 [00:00<?, ?frames/s]\n  2%|▏         | 2656/107926 [00:07<05:13, 335.45frames/s]\n  5%|▍         | 5228/107926 [00:14<04:43, 361.73frames/s]\n  8%|▊         | 8224/107926 [00:23<04:39, 356.57frames/s]\n 10%|█         | 11120/107926 [00:31<04:36, 350.73frames/s]\n 13%|█▎        | 14036/107926 [00:41<04:42, 332.77frames/s]\n 16%|█▌        | 17004/107926 [00:50<04:42, 322.21frames/s]\n 18%|█▊        | 19952/107926 [01:00<04:37, 317.44frames/s]\n 21%|██        | 22716/107926 [01:09<04:31, 313.85frames/s]\n 24%|██▎       | 25592/107926 [01:15<03:53, 351.98frames/s]\n 26%|██▋       | 28444/107926 [01:22<03:36, 366.32frames/s]\n 29%|██▉       | 31444/107926 [01:30<03:27, 369.26frames/s]\n 32%|███▏      | 34340/107926 [01:37<03:10, 387.15frames/s]\n 35%|███▍      | 37340/107926 [01:43<02:51, 411.10frames/s]\n 37%|███▋      | 40340/107926 [01:51<02:50, 395.63frames/s]\n 40%|████      | 43188/107926 [01:58<02:41, 400.06frames/s]\n 43%|████▎     | 46008/107926 [02:05<02:31, 409.62frames/s]\n 45%|████▌     | 48732/107926 [02:10<02:18, 428.15frames/s]\n 48%|████▊     | 51524/107926 [02:18<02:21, 399.55frames/s]\n 50%|█████     | 54416/107926 [02:25<02:11, 406.98frames/s]\n 53%|█████▎    | 57324/107926 [02:31<01:59, 422.38frames/s]\n 56%|█████▌    | 60304/107926 [02:37<01:47, 442.73frames/s]\n 59%|█████▊    | 63180/107926 [02:44<01:39, 447.96frames/s]\n 61%|██████    | 66008/107926 [02:51<01:36, 434.13frames/s]\n 64%|██████▍   | 68954/107926 [03:01<01:43, 376.62frames/s]\n 67%|██████▋   | 71862/107926 [03:09<01:36, 375.11frames/s]\n 69%|██████▉   | 74454/107926 [03:15<01:27, 383.83frames/s]\n 72%|███████▏  | 77282/107926 [03:23<01:21, 377.92frames/s]\n 74%|███████▍  | 80098/107926 [03:30<01:13, 377.33frames/s]\n 77%|███████▋  | 82898/107926 [03:37<01:04, 387.52frames/s]\n 79%|███████▉  | 85562/107926 [03:43<00:56, 394.98frames/s]\n 82%|████████▏ | 88530/107926 [03:52<00:51, 377.54frames/s]\n 85%|████████▍ | 91498/107926 [04:00<00:42, 383.78frames/s]\n 87%|████████▋ | 94246/107926 [04:06<00:35, 389.52frames/s]\n 90%|█████████ | 97158/107926 [04:13<00:26, 411.92frames/s]\n 93%|█████████▎| 99990/107926 [04:18<00:18, 436.56frames/s]\n 95%|█████████▌| 102958/107926 [04:28<00:12, 389.46frames/s]\n 98%|█████████▊| 105954/107926 [04:36<00:05, 376.13frames/s]\n 99%|█████████▉| 106842/107926 [04:39<00:02, 364.99frames/s]\n100%|██████████| 107926/107926 [04:43<00:00, 338.99frames/s]\n100%|██████████| 107926/107926 [04:43<00:00, 380.11frames/s]\n",
  "output": {
    "detected_language": "english",
    "segments": [
      {
        "avg_logprob": -0.27067759779633066,
        "compression_ratio": 1.651685393258427,
        "end": 3.9,
        "id": 0,
        "no_speech_prob": 0.002511170692741871,
        "seek": 0,
        "start": 0,
        "temperature": 0,
        "text": " All right, this is getting tiring, but I am back",
        "tokens": [
          50364,
          1057,
          558,
          11,
          341,
          307,
          1242,
          35182,
          11,
          457,
          286,
          669,
          646,
          50559
        ]
      },
      {
        "avg_logprob": -0.27067759779633066,
        "compression_ratio": 1.651685393258427,
        "end": 6.0600000000000005,
        "id": 1,
        "no_speech_prob": 0.002511170692741871,
        "seek": 0,
        "start": 3.9,
        "temperature": 0,
        "text": " and I have yet another video",
        "tokens": [
          50559,
          293,
          286,
          362,
          1939,
          1071,
          960,
          50667
        ]
      },
      {
        "avg_logprob": -0.27067759779633066,
        "compression_ratio": 1.651685393258427,
        "end": 10.4,
        "id": 2,
        "no_speech_prob": 0.002511170692741871,
        "seek": 0,
        "start": 7.68,
        "temperature": 0,
        "text": " in this building your own custom color classifier",
        "tokens": [
          50748,
          294,
          341,
          2390,
          428,
          1065,
          2375,
          2017,
          1508,
          9902,
          50884
        ]
      },
      {
        "avg_logprob": -0.27067759779633066,
        "compression_ratio": 1.651685393258427,
        "end": 11.52,
        "id": 3,
        "no_speech_prob": 0.002511170692741871,
        "seek": 0,
        "start": 10.4,
        "temperature": 0,
        "text": " with TensorFlow.js series.",
        "tokens": [
          50884,
          365,
          37624,
          13,
          25530,
          2638,
          13,
          50940
        ]
      },
      {
        "avg_logprob": -0.27067759779633066,
        "compression_ratio": 1.651685393258427,
        "end": 15.32,
        "id": 4,
        "no_speech_prob": 0.002511170692741871,
        "seek": 0,
        "start": 11.52,
        "temperature": 0,
        "text": " Now, the thing that I want to add to this video,",
        "tokens": [
          50940,
          823,
          11,
          264,
          551,
          300,
          286,
          528,
          281,
          909,
          281,
          341,
          960,
          11,
          51130
        ]
      },
      {
        "avg_logprob": -0.27067759779633066,
        "compression_ratio": 1.651685393258427,
        "end": 17.68,
        "id": 5,
        "no_speech_prob": 0.002511170692741871,
        "seek": 0,
        "start": 15.32,
        "temperature": 0,
        "text": " and by the way, this line moving across is pointless.",
        "tokens": [
          51130,
          293,
          538,
          264,
          636,
          11,
          341,
          1622,
          2684,
          2108,
          307,
          32824,
          13,
          51248
        ]
      },
      {
        "avg_logprob": -0.27067759779633066,
        "compression_ratio": 1.651685393258427,
        "end": 19,
        "id": 6,
        "no_speech_prob": 0.002511170692741871,
        "seek": 0,
        "start": 17.68,
        "temperature": 0,
        "text": " I just have it there so that I can see",
        "tokens": [
          51248,
          286,
          445,
          362,
          309,
          456,
          370,
          300,
          286,
          393,
          536,
          51314
        ]
      },
      {
        "avg_logprob": -0.27067759779633066,
        "compression_ratio": 1.651685393258427,
        "end": 22.86,
        "id": 7,
        "no_speech_prob": 0.002511170692741871,
        "seek": 0,
        "start": 19,
        "temperature": 0,
        "text": " that the draw loop is animating, that I haven't blocked it.",
        "tokens": [
          51314,
          300,
          264,
          2642,
          6367,
          307,
          2383,
          990,
          11,
          300,
          286,
          2378,
          380,
          15470,
          309,
          13,
          51507
        ]
      },
      {
        "avg_logprob": -0.27067759779633066,
        "compression_ratio": 1.651685393258427,
        "end": 23.94,
        "id": 8,
        "no_speech_prob": 0.002511170692741871,
        "seek": 0,
        "start": 22.86,
        "temperature": 0,
        "text": " There's two things that I missed",
        "tokens": [
          51507,
          821,
          311,
          732,
          721,
          300,
          286,
          6721,
          51561
        ]
      },
      {
        "avg_logprob": -0.27067759779633066,
        "compression_ratio": 1.651685393258427,
        "end": 26.560000000000002,
        "id": 9,
        "no_speech_prob": 0.002511170692741871,
        "seek": 0,
        "start": 23.94,
        "temperature": 0,
        "text": " that are kind of important from the previous video.",
        "tokens": [
          51561,
          300,
          366,
          733,
          295,
          1021,
          490,
          264,
          3894,
          960,
          13,
          51692
        ]
      },
      {
        "avg_logprob": -0.27017505352313703,
        "compression_ratio": 1.6097560975609757,
        "end": 31.759999999999998,
        "id": 10,
        "no_speech_prob": 0.00007484614616259933,
        "seek": 2656,
        "start": 26.759999999999998,
        "temperature": 0,
        "text": " One is, this is actually not the validation data loss.",
        "tokens": [
          50374,
          1485,
          307,
          11,
          341,
          307,
          767,
          406,
          264,
          24071,
          1412,
          4470,
          13,
          50624
        ]
      },
      {
        "avg_logprob": -0.27017505352313703,
        "compression_ratio": 1.6097560975609757,
        "end": 36.28,
        "id": 11,
        "no_speech_prob": 0.00007484614616259933,
        "seek": 2656,
        "start": 31.799999999999997,
        "temperature": 0,
        "text": " I didn't realize this, but I'm going to change this here.",
        "tokens": [
          50626,
          286,
          994,
          380,
          4325,
          341,
          11,
          457,
          286,
          478,
          516,
          281,
          1319,
          341,
          510,
          13,
          50850
        ]
      },
      {
        "avg_logprob": -0.27017505352313703,
        "compression_ratio": 1.6097560975609757,
        "end": 41.28,
        "id": 12,
        "no_speech_prob": 0.00007484614616259933,
        "seek": 2656,
        "start": 36.28,
        "temperature": 0,
        "text": " I'm going to console.log the full logs object.",
        "tokens": [
          50850,
          286,
          478,
          516,
          281,
          11076,
          13,
          4987,
          264,
          1577,
          20820,
          2657,
          13,
          51100
        ]
      },
      {
        "avg_logprob": -0.27017505352313703,
        "compression_ratio": 1.6097560975609757,
        "end": 46.12,
        "id": 13,
        "no_speech_prob": 0.00007484614616259933,
        "seek": 2656,
        "start": 41.64,
        "temperature": 0,
        "text": " So, what I'm putting onto the screen is logs.loss.",
        "tokens": [
          51118,
          407,
          11,
          437,
          286,
          478,
          3372,
          3911,
          264,
          2568,
          307,
          20820,
          13,
          75,
          772,
          13,
          51342
        ]
      },
      {
        "avg_logprob": -0.27017505352313703,
        "compression_ratio": 1.6097560975609757,
        "end": 48.56,
        "id": 14,
        "no_speech_prob": 0.00007484614616259933,
        "seek": 2656,
        "start": 46.12,
        "temperature": 0,
        "text": " Let me console.log what's there.",
        "tokens": [
          51342,
          961,
          385,
          11076,
          13,
          4987,
          437,
          311,
          456,
          13,
          51464
        ]
      },
      {
        "avg_logprob": -0.27017505352313703,
        "compression_ratio": 1.6097560975609757,
        "end": 49.56,
        "id": 15,
        "no_speech_prob": 0.00007484614616259933,
        "seek": 2656,
        "start": 48.56,
        "temperature": 0,
        "text": " So, again, we have to wait a minute",
        "tokens": [
          51464,
          407,
          11,
          797,
          11,
          321,
          362,
          281,
          1699,
          257,
          3456,
          51514
        ]
      },
      {
        "avg_logprob": -0.27017505352313703,
        "compression_ratio": 1.6097560975609757,
        "end": 50.96,
        "id": 16,
        "no_speech_prob": 0.00007484614616259933,
        "seek": 2656,
        "start": 49.56,
        "temperature": 0,
        "text": " for the first epoch to finish.",
        "tokens": [
          51514,
          337,
          264,
          700,
          30992,
          339,
          281,
          2413,
          13,
          51584
        ]
      },
      {
        "avg_logprob": -0.27017505352313703,
        "compression_ratio": 1.6097560975609757,
        "end": 52.28,
        "id": 17,
        "no_speech_prob": 0.00007484614616259933,
        "seek": 2656,
        "start": 50.96,
        "temperature": 0,
        "text": " Apologies for that.",
        "tokens": [
          51584,
          8723,
          6204,
          337,
          300,
          13,
          51650
        ]
      },
      {
        "avg_logprob": -0.2667766339851148,
        "compression_ratio": 1.959016393442623,
        "end": 54.160000000000004,
        "id": 18,
        "no_speech_prob": 0.000014738980098627508,
        "seek": 5228,
        "start": 52.76,
        "temperature": 0,
        "text": " Da, da, da, da, da, da, da, da.",
        "tokens": [
          50388,
          3933,
          11,
          1120,
          11,
          1120,
          11,
          1120,
          11,
          1120,
          11,
          1120,
          11,
          1120,
          11,
          1120,
          13,
          50458
        ]
      },
      {
        "avg_logprob": -0.2667766339851148,
        "compression_ratio": 1.959016393442623,
        "end": 57.46,
        "id": 19,
        "no_speech_prob": 0.000014738980098627508,
        "seek": 5228,
        "start": 54.160000000000004,
        "temperature": 0,
        "text": " Okay, there are actually two loss values.",
        "tokens": [
          50458,
          1033,
          11,
          456,
          366,
          767,
          732,
          4470,
          4190,
          13,
          50623
        ]
      },
      {
        "avg_logprob": -0.2667766339851148,
        "compression_ratio": 1.959016393442623,
        "end": 61.52,
        "id": 20,
        "no_speech_prob": 0.000014738980098627508,
        "seek": 5228,
        "start": 57.46,
        "temperature": 0,
        "text": " There's the loss function computed against the training data",
        "tokens": [
          50623,
          821,
          311,
          264,
          4470,
          2445,
          40610,
          1970,
          264,
          3097,
          1412,
          50826
        ]
      },
      {
        "avg_logprob": -0.2667766339851148,
        "compression_ratio": 1.959016393442623,
        "end": 63.22,
        "id": 21,
        "no_speech_prob": 0.000014738980098627508,
        "seek": 5228,
        "start": 61.52,
        "temperature": 0,
        "text": " and there's the loss function computed",
        "tokens": [
          50826,
          293,
          456,
          311,
          264,
          4470,
          2445,
          40610,
          50911
        ]
      },
      {
        "avg_logprob": -0.2667766339851148,
        "compression_ratio": 1.959016393442623,
        "end": 64.24000000000001,
        "id": 22,
        "no_speech_prob": 0.000014738980098627508,
        "seek": 5228,
        "start": 63.22,
        "temperature": 0,
        "text": " against the validation data.",
        "tokens": [
          50911,
          1970,
          264,
          24071,
          1412,
          13,
          50962
        ]
      },
      {
        "avg_logprob": -0.2667766339851148,
        "compression_ratio": 1.959016393442623,
        "end": 67.68,
        "id": 23,
        "no_speech_prob": 0.000014738980098627508,
        "seek": 5228,
        "start": 64.24000000000001,
        "temperature": 0,
        "text": " Now, to do this properly, I really should be using",
        "tokens": [
          50962,
          823,
          11,
          281,
          360,
          341,
          6108,
          11,
          286,
          534,
          820,
          312,
          1228,
          51134
        ]
      },
      {
        "avg_logprob": -0.2667766339851148,
        "compression_ratio": 1.959016393442623,
        "end": 70.72,
        "id": 24,
        "no_speech_prob": 0.000014738980098627508,
        "seek": 5228,
        "start": 67.68,
        "temperature": 0,
        "text": " the validation loss because that's data",
        "tokens": [
          51134,
          264,
          24071,
          4470,
          570,
          300,
          311,
          1412,
          51286
        ]
      },
      {
        "avg_logprob": -0.2667766339851148,
        "compression_ratio": 1.959016393442623,
        "end": 73.2,
        "id": 25,
        "no_speech_prob": 0.000014738980098627508,
        "seek": 5228,
        "start": 70.72,
        "temperature": 0,
        "text": " that hasn't been done with the training.",
        "tokens": [
          51286,
          300,
          6132,
          380,
          668,
          1096,
          365,
          264,
          3097,
          13,
          51410
        ]
      },
      {
        "avg_logprob": -0.2667766339851148,
        "compression_ratio": 1.959016393442623,
        "end": 76,
        "id": 26,
        "no_speech_prob": 0.000014738980098627508,
        "seek": 5228,
        "start": 73.2,
        "temperature": 0,
        "text": " That's protecting against overfitting,",
        "tokens": [
          51410,
          663,
          311,
          12316,
          1970,
          670,
          69,
          2414,
          11,
          51550
        ]
      },
      {
        "avg_logprob": -0.2667766339851148,
        "compression_ratio": 1.959016393442623,
        "end": 77.2,
        "id": 27,
        "no_speech_prob": 0.000014738980098627508,
        "seek": 5228,
        "start": 76,
        "temperature": 0,
        "text": " having my model work really well",
        "tokens": [
          51550,
          1419,
          452,
          2316,
          589,
          534,
          731,
          51610
        ]
      },
      {
        "avg_logprob": -0.2667766339851148,
        "compression_ratio": 1.959016393442623,
        "end": 79.54,
        "id": 28,
        "no_speech_prob": 0.000014738980098627508,
        "seek": 5228,
        "start": 77.2,
        "temperature": 0,
        "text": " with the training data only.",
        "tokens": [
          51610,
          365,
          264,
          3097,
          1412,
          787,
          13,
          51727
        ]
      },
      {
        "avg_logprob": -0.2667766339851148,
        "compression_ratio": 1.959016393442623,
        "end": 82.24000000000001,
        "id": 29,
        "no_speech_prob": 0.000014738980098627508,
        "seek": 5228,
        "start": 79.54,
        "temperature": 0,
        "text": " The thing is, I have a very small data set",
        "tokens": [
          51727,
          440,
          551,
          307,
          11,
          286,
          362,
          257,
          588,
          1359,
          1412,
          992,
          51862
        ]
      },
      {
        "avg_logprob": -0.21948654064233752,
        "compression_ratio": 1.6714801444043321,
        "end": 84.03999999999999,
        "id": 30,
        "no_speech_prob": 0.00012148159294156358,
        "seek": 8224,
        "start": 83.08,
        "temperature": 0,
        "text": " of 5,000 data points.",
        "tokens": [
          50406,
          295,
          1025,
          11,
          1360,
          1412,
          2793,
          13,
          50454
        ]
      },
      {
        "avg_logprob": -0.21948654064233752,
        "compression_ratio": 1.6714801444043321,
        "end": 86.39999999999999,
        "id": 31,
        "no_speech_prob": 0.00012148159294156358,
        "seek": 8224,
        "start": 84.03999999999999,
        "temperature": 0,
        "text": " I'm just using 10% as the validation data",
        "tokens": [
          50454,
          286,
          478,
          445,
          1228,
          1266,
          4,
          382,
          264,
          24071,
          1412,
          50572
        ]
      },
      {
        "avg_logprob": -0.21948654064233752,
        "compression_ratio": 1.6714801444043321,
        "end": 88.53999999999999,
        "id": 32,
        "no_speech_prob": 0.00012148159294156358,
        "seek": 8224,
        "start": 86.39999999999999,
        "temperature": 0,
        "text": " and the way TensorFlow.js works,",
        "tokens": [
          50572,
          293,
          264,
          636,
          37624,
          13,
          25530,
          1985,
          11,
          50679
        ]
      },
      {
        "avg_logprob": -0.21948654064233752,
        "compression_ratio": 1.6714801444043321,
        "end": 90.83999999999999,
        "id": 33,
        "no_speech_prob": 0.00012148159294156358,
        "seek": 8224,
        "start": 88.53999999999999,
        "temperature": 0,
        "text": " it also takes that 10% from the end",
        "tokens": [
          50679,
          309,
          611,
          2516,
          300,
          1266,
          4,
          490,
          264,
          917,
          50794
        ]
      },
      {
        "avg_logprob": -0.21948654064233752,
        "compression_ratio": 1.6714801444043321,
        "end": 93.75999999999999,
        "id": 34,
        "no_speech_prob": 0.00012148159294156358,
        "seek": 8224,
        "start": 90.83999999999999,
        "temperature": 0,
        "text": " and I wasn't careful about shuffling the data around.",
        "tokens": [
          50794,
          293,
          286,
          2067,
          380,
          5026,
          466,
          402,
          1245,
          1688,
          264,
          1412,
          926,
          13,
          50940
        ]
      },
      {
        "avg_logprob": -0.21948654064233752,
        "compression_ratio": 1.6714801444043321,
        "end": 95.67999999999999,
        "id": 35,
        "no_speech_prob": 0.00012148159294156358,
        "seek": 8224,
        "start": 93.75999999999999,
        "temperature": 0,
        "text": " So, this is something that I should come back to.",
        "tokens": [
          50940,
          407,
          11,
          341,
          307,
          746,
          300,
          286,
          820,
          808,
          646,
          281,
          13,
          51036
        ]
      },
      {
        "avg_logprob": -0.21948654064233752,
        "compression_ratio": 1.6714801444043321,
        "end": 99.24,
        "id": 36,
        "no_speech_prob": 0.00012148159294156358,
        "seek": 8224,
        "start": 95.67999999999999,
        "temperature": 0,
        "text": " I don't know, maybe this series will go on to infinity,",
        "tokens": [
          51036,
          286,
          500,
          380,
          458,
          11,
          1310,
          341,
          2638,
          486,
          352,
          322,
          281,
          13202,
          11,
          51214
        ]
      },
      {
        "avg_logprob": -0.21948654064233752,
        "compression_ratio": 1.6714801444043321,
        "end": 101.44,
        "id": 37,
        "no_speech_prob": 0.00012148159294156358,
        "seek": 8224,
        "start": 99.24,
        "temperature": 0,
        "text": " but if I were doing this properly,",
        "tokens": [
          51214,
          457,
          498,
          286,
          645,
          884,
          341,
          6108,
          11,
          51324
        ]
      },
      {
        "avg_logprob": -0.21948654064233752,
        "compression_ratio": 1.6714801444043321,
        "end": 104.72,
        "id": 38,
        "no_speech_prob": 0.00012148159294156358,
        "seek": 8224,
        "start": 101.44,
        "temperature": 0,
        "text": " I would actually want to show the validation loss here",
        "tokens": [
          51324,
          286,
          576,
          767,
          528,
          281,
          855,
          264,
          24071,
          4470,
          510,
          51488
        ]
      },
      {
        "avg_logprob": -0.21948654064233752,
        "compression_ratio": 1.6714801444043321,
        "end": 109.22,
        "id": 39,
        "no_speech_prob": 0.00012148159294156358,
        "seek": 8224,
        "start": 106.69999999999999,
        "temperature": 0,
        "text": " like this, logs.validationLoss.",
        "tokens": [
          51587,
          411,
          341,
          11,
          20820,
          13,
          3337,
          327,
          399,
          43,
          772,
          13,
          51713
        ]
      },
      {
        "avg_logprob": -0.21948654064233752,
        "compression_ratio": 1.6714801444043321,
        "end": 111.19999999999999,
        "id": 40,
        "no_speech_prob": 0.00012148159294156358,
        "seek": 8224,
        "start": 109.22,
        "temperature": 0,
        "text": " Maybe I want to show both and maybe I want to be",
        "tokens": [
          51713,
          2704,
          286,
          528,
          281,
          855,
          1293,
          293,
          1310,
          286,
          528,
          281,
          312,
          51812
        ]
      },
      {
        "avg_logprob": -0.24281208627175965,
        "compression_ratio": 1.7055016181229774,
        "end": 113.36,
        "id": 41,
        "no_speech_prob": 0.000020462923203012906,
        "seek": 11120,
        "start": 111.2,
        "temperature": 0,
        "text": " more thoughtful about shuffling the data first in advance.",
        "tokens": [
          50364,
          544,
          21566,
          466,
          402,
          1245,
          1688,
          264,
          1412,
          700,
          294,
          7295,
          13,
          50472
        ]
      },
      {
        "avg_logprob": -0.24281208627175965,
        "compression_ratio": 1.7055016181229774,
        "end": 115.96000000000001,
        "id": 42,
        "no_speech_prob": 0.000020462923203012906,
        "seek": 11120,
        "start": 113.36,
        "temperature": 0,
        "text": " But that's not what I said I was going to do",
        "tokens": [
          50472,
          583,
          300,
          311,
          406,
          437,
          286,
          848,
          286,
          390,
          516,
          281,
          360,
          50602
        ]
      },
      {
        "avg_logprob": -0.24281208627175965,
        "compression_ratio": 1.7055016181229774,
        "end": 118.96000000000001,
        "id": 43,
        "no_speech_prob": 0.000020462923203012906,
        "seek": 11120,
        "start": 115.96000000000001,
        "temperature": 0,
        "text": " in the next video, so I'm again leaving that temporarily",
        "tokens": [
          50602,
          294,
          264,
          958,
          960,
          11,
          370,
          286,
          478,
          797,
          5012,
          300,
          23750,
          50752
        ]
      },
      {
        "avg_logprob": -0.24281208627175965,
        "compression_ratio": 1.7055016181229774,
        "end": 121.34,
        "id": 44,
        "no_speech_prob": 0.000020462923203012906,
        "seek": 11120,
        "start": 118.96000000000001,
        "temperature": 0,
        "text": " as an exercise to the viewer or I'll come back",
        "tokens": [
          50752,
          382,
          364,
          5380,
          281,
          264,
          16767,
          420,
          286,
          603,
          808,
          646,
          50871
        ]
      },
      {
        "avg_logprob": -0.24281208627175965,
        "compression_ratio": 1.7055016181229774,
        "end": 123.64,
        "id": 45,
        "no_speech_prob": 0.000020462923203012906,
        "seek": 11120,
        "start": 121.34,
        "temperature": 0,
        "text": " and do it in a future video, I don't know yet.",
        "tokens": [
          50871,
          293,
          360,
          309,
          294,
          257,
          2027,
          960,
          11,
          286,
          500,
          380,
          458,
          1939,
          13,
          50986
        ]
      },
      {
        "avg_logprob": -0.24281208627175965,
        "compression_ratio": 1.7055016181229774,
        "end": 124.8,
        "id": 46,
        "no_speech_prob": 0.000020462923203012906,
        "seek": 11120,
        "start": 123.64,
        "temperature": 0,
        "text": " That's item number one.",
        "tokens": [
          50986,
          663,
          311,
          3174,
          1230,
          472,
          13,
          51044
        ]
      },
      {
        "avg_logprob": -0.24281208627175965,
        "compression_ratio": 1.7055016181229774,
        "end": 127.68,
        "id": 47,
        "no_speech_prob": 0.000020462923203012906,
        "seek": 11120,
        "start": 124.8,
        "temperature": 0,
        "text": " Item number two, thank you to me, I am so me",
        "tokens": [
          51044,
          31066,
          1230,
          732,
          11,
          1309,
          291,
          281,
          385,
          11,
          286,
          669,
          370,
          385,
          51188
        ]
      },
      {
        "avg_logprob": -0.24281208627175965,
        "compression_ratio": 1.7055016181229774,
        "end": 131.8,
        "id": 48,
        "no_speech_prob": 0.000020462923203012906,
        "seek": 11120,
        "start": 127.68,
        "temperature": 0,
        "text": " and others in the Coding Train sponsor patron group.",
        "tokens": [
          51188,
          293,
          2357,
          294,
          264,
          383,
          8616,
          28029,
          16198,
          21843,
          1594,
          13,
          51394
        ]
      },
      {
        "avg_logprob": -0.24281208627175965,
        "compression_ratio": 1.7055016181229774,
        "end": 134.24,
        "id": 49,
        "no_speech_prob": 0.000020462923203012906,
        "seek": 11120,
        "start": 131.8,
        "temperature": 0,
        "text": " I made this way more complicated than it needs",
        "tokens": [
          51394,
          286,
          1027,
          341,
          636,
          544,
          6179,
          813,
          309,
          2203,
          51516
        ]
      },
      {
        "avg_logprob": -0.24281208627175965,
        "compression_ratio": 1.7055016181229774,
        "end": 136.9,
        "id": 50,
        "no_speech_prob": 0.000020462923203012906,
        "seek": 11120,
        "start": 134.24,
        "temperature": 0,
        "text": " by trying to make this an async function in here.",
        "tokens": [
          51516,
          538,
          1382,
          281,
          652,
          341,
          364,
          382,
          34015,
          2445,
          294,
          510,
          13,
          51649
        ]
      },
      {
        "avg_logprob": -0.24281208627175965,
        "compression_ratio": 1.7055016181229774,
        "end": 140.36,
        "id": 51,
        "no_speech_prob": 0.000020462923203012906,
        "seek": 11120,
        "start": 136.9,
        "temperature": 0,
        "text": " Actually, this does not need to be an async function.",
        "tokens": [
          51649,
          5135,
          11,
          341,
          775,
          406,
          643,
          281,
          312,
          364,
          382,
          34015,
          2445,
          13,
          51822
        ]
      },
      {
        "avg_logprob": -0.2402850087483724,
        "compression_ratio": 1.8412698412698412,
        "end": 143.20000000000002,
        "id": 52,
        "no_speech_prob": 0.0008693593554198742,
        "seek": 14036,
        "start": 140.36,
        "temperature": 0,
        "text": " If I just return tf.nextFrame.",
        "tokens": [
          50364,
          759,
          286,
          445,
          2736,
          256,
          69,
          13,
          716,
          734,
          40305,
          529,
          13,
          50506
        ]
      },
      {
        "avg_logprob": -0.2402850087483724,
        "compression_ratio": 1.8412698412698412,
        "end": 146.18,
        "id": 53,
        "no_speech_prob": 0.0008693593554198742,
        "seek": 14036,
        "start": 143.20000000000002,
        "temperature": 0,
        "text": " So if I just return tf.nextFrame,",
        "tokens": [
          50506,
          407,
          498,
          286,
          445,
          2736,
          256,
          69,
          13,
          716,
          734,
          40305,
          529,
          11,
          50655
        ]
      },
      {
        "avg_logprob": -0.2402850087483724,
        "compression_ratio": 1.8412698412698412,
        "end": 147.76000000000002,
        "id": 54,
        "no_speech_prob": 0.0008693593554198742,
        "seek": 14036,
        "start": 146.18,
        "temperature": 0,
        "text": " it's actually returning the promise",
        "tokens": [
          50655,
          309,
          311,
          767,
          12678,
          264,
          6228,
          50734
        ]
      },
      {
        "avg_logprob": -0.2402850087483724,
        "compression_ratio": 1.8412698412698412,
        "end": 150.04000000000002,
        "id": 55,
        "no_speech_prob": 0.0008693593554198742,
        "seek": 14036,
        "start": 147.76000000000002,
        "temperature": 0,
        "text": " and unlocking the draw loop, so that makes it simpler.",
        "tokens": [
          50734,
          293,
          49620,
          264,
          2642,
          6367,
          11,
          370,
          300,
          1669,
          309,
          18587,
          13,
          50848
        ]
      },
      {
        "avg_logprob": -0.2402850087483724,
        "compression_ratio": 1.8412698412698412,
        "end": 152.52,
        "id": 56,
        "no_speech_prob": 0.0008693593554198742,
        "seek": 14036,
        "start": 150.04000000000002,
        "temperature": 0,
        "text": " And actually, what I really want to do,",
        "tokens": [
          50848,
          400,
          767,
          11,
          437,
          286,
          534,
          528,
          281,
          360,
          11,
          50972
        ]
      },
      {
        "avg_logprob": -0.2402850087483724,
        "compression_ratio": 1.8412698412698412,
        "end": 155.10000000000002,
        "id": 57,
        "no_speech_prob": 0.0008693593554198742,
        "seek": 14036,
        "start": 152.52,
        "temperature": 0,
        "text": " what I really want, I couldn't make this so simpler.",
        "tokens": [
          50972,
          437,
          286,
          534,
          528,
          11,
          286,
          2809,
          380,
          652,
          341,
          370,
          18587,
          13,
          51101
        ]
      },
      {
        "avg_logprob": -0.2402850087483724,
        "compression_ratio": 1.8412698412698412,
        "end": 156,
        "id": 58,
        "no_speech_prob": 0.0008693593554198742,
        "seek": 14036,
        "start": 155.10000000000002,
        "temperature": 0,
        "text": " What am I doing here?",
        "tokens": [
          51101,
          708,
          669,
          286,
          884,
          510,
          30,
          51146
        ]
      },
      {
        "avg_logprob": -0.2402850087483724,
        "compression_ratio": 1.8412698412698412,
        "end": 158.88000000000002,
        "id": 59,
        "no_speech_prob": 0.0008693593554198742,
        "seek": 14036,
        "start": 156,
        "temperature": 0,
        "text": " At the end of every batch, I want tf.nextFrame",
        "tokens": [
          51146,
          1711,
          264,
          917,
          295,
          633,
          15245,
          11,
          286,
          528,
          256,
          69,
          13,
          716,
          734,
          40305,
          529,
          51290
        ]
      },
      {
        "avg_logprob": -0.2402850087483724,
        "compression_ratio": 1.8412698412698412,
        "end": 161.72000000000003,
        "id": 60,
        "no_speech_prob": 0.0008693593554198742,
        "seek": 14036,
        "start": 158.88000000000002,
        "temperature": 0,
        "text": " to be executed and so I actually don't need",
        "tokens": [
          51290,
          281,
          312,
          17577,
          293,
          370,
          286,
          767,
          500,
          380,
          643,
          51432
        ]
      },
      {
        "avg_logprob": -0.2402850087483724,
        "compression_ratio": 1.8412698412698412,
        "end": 165.4,
        "id": 61,
        "no_speech_prob": 0.0008693593554198742,
        "seek": 14036,
        "start": 161.72000000000003,
        "temperature": 0,
        "text": " to write a wrapper function to execute tf.nextFrame.",
        "tokens": [
          51432,
          281,
          2464,
          257,
          46906,
          2445,
          281,
          14483,
          256,
          69,
          13,
          716,
          734,
          40305,
          529,
          13,
          51616
        ]
      },
      {
        "avg_logprob": -0.2402850087483724,
        "compression_ratio": 1.8412698412698412,
        "end": 170.04000000000002,
        "id": 62,
        "no_speech_prob": 0.0008693593554198742,
        "seek": 14036,
        "start": 166.48000000000002,
        "temperature": 0,
        "text": " What I could just do is set that as the callback.",
        "tokens": [
          51670,
          708,
          286,
          727,
          445,
          360,
          307,
          992,
          300,
          382,
          264,
          818,
          3207,
          13,
          51848
        ]
      },
      {
        "avg_logprob": -0.21589074545348716,
        "compression_ratio": 1.6586206896551725,
        "end": 172.88,
        "id": 63,
        "no_speech_prob": 0.000008013486876734532,
        "seek": 17004,
        "start": 170.6,
        "temperature": 0,
        "text": " Again, if I wanted to do more with onBatchEnd,",
        "tokens": [
          50392,
          3764,
          11,
          498,
          286,
          1415,
          281,
          360,
          544,
          365,
          322,
          33,
          852,
          36952,
          11,
          50506
        ]
      },
      {
        "avg_logprob": -0.21589074545348716,
        "compression_ratio": 1.6586206896551725,
        "end": 176.12,
        "id": 64,
        "no_speech_prob": 0.000008013486876734532,
        "seek": 17004,
        "start": 172.88,
        "temperature": 0,
        "text": " look at the loss and the logs, but really what I want",
        "tokens": [
          50506,
          574,
          412,
          264,
          4470,
          293,
          264,
          20820,
          11,
          457,
          534,
          437,
          286,
          528,
          50668
        ]
      },
      {
        "avg_logprob": -0.21589074545348716,
        "compression_ratio": 1.6586206896551725,
        "end": 178.32,
        "id": 65,
        "no_speech_prob": 0.000008013486876734532,
        "seek": 17004,
        "start": 176.12,
        "temperature": 0,
        "text": " is at the end of every batch to draw a new frame",
        "tokens": [
          50668,
          307,
          412,
          264,
          917,
          295,
          633,
          15245,
          281,
          2642,
          257,
          777,
          3920,
          50778
        ]
      },
      {
        "avg_logprob": -0.21589074545348716,
        "compression_ratio": 1.6586206896551725,
        "end": 181.2,
        "id": 66,
        "no_speech_prob": 0.000008013486876734532,
        "seek": 17004,
        "start": 178.32,
        "temperature": 0,
        "text": " of animation, I could just put tf.nextFrame",
        "tokens": [
          50778,
          295,
          9603,
          11,
          286,
          727,
          445,
          829,
          256,
          69,
          13,
          716,
          734,
          40305,
          529,
          50922
        ]
      },
      {
        "avg_logprob": -0.21589074545348716,
        "compression_ratio": 1.6586206896551725,
        "end": 183.76,
        "id": 67,
        "no_speech_prob": 0.000008013486876734532,
        "seek": 17004,
        "start": 181.2,
        "temperature": 0,
        "text": " as the function, which is the callback there.",
        "tokens": [
          50922,
          382,
          264,
          2445,
          11,
          597,
          307,
          264,
          818,
          3207,
          456,
          13,
          51050
        ]
      },
      {
        "avg_logprob": -0.21589074545348716,
        "compression_ratio": 1.6586206896551725,
        "end": 185.07999999999998,
        "id": 68,
        "no_speech_prob": 0.000008013486876734532,
        "seek": 17004,
        "start": 183.76,
        "temperature": 0,
        "text": " Okay?",
        "tokens": [
          51050,
          1033,
          30,
          51116
        ]
      },
      {
        "avg_logprob": -0.21589074545348716,
        "compression_ratio": 1.6586206896551725,
        "end": 188.23999999999998,
        "id": 69,
        "no_speech_prob": 0.000008013486876734532,
        "seek": 17004,
        "start": 185.07999999999998,
        "temperature": 0,
        "text": " So this is still working, that simplifies the code,",
        "tokens": [
          51116,
          407,
          341,
          307,
          920,
          1364,
          11,
          300,
          6883,
          11221,
          264,
          3089,
          11,
          51274
        ]
      },
      {
        "avg_logprob": -0.21589074545348716,
        "compression_ratio": 1.6586206896551725,
        "end": 189.7,
        "id": 70,
        "no_speech_prob": 0.000008013486876734532,
        "seek": 17004,
        "start": 188.23999999999998,
        "temperature": 0,
        "text": " makes it a little nicer to look at.",
        "tokens": [
          51274,
          1669,
          309,
          257,
          707,
          22842,
          281,
          574,
          412,
          13,
          51347
        ]
      },
      {
        "avg_logprob": -0.21589074545348716,
        "compression_ratio": 1.6586206896551725,
        "end": 194.64,
        "id": 71,
        "no_speech_prob": 0.000008013486876734532,
        "seek": 17004,
        "start": 189.7,
        "temperature": 0,
        "text": " I don't even really need this onBegin and onEnd,",
        "tokens": [
          51347,
          286,
          500,
          380,
          754,
          534,
          643,
          341,
          322,
          33,
          1146,
          259,
          293,
          322,
          36952,
          11,
          51594
        ]
      },
      {
        "avg_logprob": -0.21589074545348716,
        "compression_ratio": 1.6586206896551725,
        "end": 197.07999999999998,
        "id": 72,
        "no_speech_prob": 0.000008013486876734532,
        "seek": 17004,
        "start": 194.64,
        "temperature": 0,
        "text": " but I'll leave those in there just so you see them.",
        "tokens": [
          51594,
          457,
          286,
          603,
          1856,
          729,
          294,
          456,
          445,
          370,
          291,
          536,
          552,
          13,
          51716
        ]
      },
      {
        "avg_logprob": -0.21589074545348716,
        "compression_ratio": 1.6586206896551725,
        "end": 199.51999999999998,
        "id": 73,
        "no_speech_prob": 0.000008013486876734532,
        "seek": 17004,
        "start": 197.07999999999998,
        "temperature": 0,
        "text": " Okay, so now I'm ready for what is the purpose",
        "tokens": [
          51716,
          1033,
          11,
          370,
          586,
          286,
          478,
          1919,
          337,
          437,
          307,
          264,
          4334,
          51838
        ]
      },
      {
        "avg_logprob": -0.21125429955081662,
        "compression_ratio": 1.868,
        "end": 200.82000000000002,
        "id": 74,
        "no_speech_prob": 0.00004908654227619991,
        "seek": 19952,
        "start": 200,
        "temperature": 0,
        "text": " of this video.",
        "tokens": [
          50388,
          295,
          341,
          960,
          13,
          50429
        ]
      },
      {
        "avg_logprob": -0.21125429955081662,
        "compression_ratio": 1.868,
        "end": 204.28,
        "id": 75,
        "no_speech_prob": 0.00004908654227619991,
        "seek": 19952,
        "start": 200.82000000000002,
        "temperature": 0,
        "text": " The purpose of this video is while I'm training the model,",
        "tokens": [
          50429,
          440,
          4334,
          295,
          341,
          960,
          307,
          1339,
          286,
          478,
          3097,
          264,
          2316,
          11,
          50602
        ]
      },
      {
        "avg_logprob": -0.21125429955081662,
        "compression_ratio": 1.868,
        "end": 206.08,
        "id": 76,
        "no_speech_prob": 0.00004908654227619991,
        "seek": 19952,
        "start": 204.28,
        "temperature": 0,
        "text": " I could wait until I finish training the model,",
        "tokens": [
          50602,
          286,
          727,
          1699,
          1826,
          286,
          2413,
          3097,
          264,
          2316,
          11,
          50692
        ]
      },
      {
        "avg_logprob": -0.21125429955081662,
        "compression_ratio": 1.868,
        "end": 207.4,
        "id": 77,
        "no_speech_prob": 0.00004908654227619991,
        "seek": 19952,
        "start": 206.08,
        "temperature": 0,
        "text": " but I'm actually going to allow this to happen",
        "tokens": [
          50692,
          457,
          286,
          478,
          767,
          516,
          281,
          2089,
          341,
          281,
          1051,
          50758
        ]
      },
      {
        "avg_logprob": -0.21125429955081662,
        "compression_ratio": 1.868,
        "end": 209.16000000000003,
        "id": 78,
        "no_speech_prob": 0.00004908654227619991,
        "seek": 19952,
        "start": 207.4,
        "temperature": 0,
        "text": " while I'm training the model.",
        "tokens": [
          50758,
          1339,
          286,
          478,
          3097,
          264,
          2316,
          13,
          50846
        ]
      },
      {
        "avg_logprob": -0.21125429955081662,
        "compression_ratio": 1.868,
        "end": 212.48000000000002,
        "id": 79,
        "no_speech_prob": 0.00004908654227619991,
        "seek": 19952,
        "start": 209.16000000000003,
        "temperature": 0,
        "text": " I want to be able to specify a color and see",
        "tokens": [
          50846,
          286,
          528,
          281,
          312,
          1075,
          281,
          16500,
          257,
          2017,
          293,
          536,
          51012
        ]
      },
      {
        "avg_logprob": -0.21125429955081662,
        "compression_ratio": 1.868,
        "end": 214.84,
        "id": 80,
        "no_speech_prob": 0.00004908654227619991,
        "seek": 19952,
        "start": 212.48000000000002,
        "temperature": 0,
        "text": " what the neural network thinks that color is.",
        "tokens": [
          51012,
          437,
          264,
          18161,
          3209,
          7309,
          300,
          2017,
          307,
          13,
          51130
        ]
      },
      {
        "avg_logprob": -0.21125429955081662,
        "compression_ratio": 1.868,
        "end": 217.74,
        "id": 81,
        "no_speech_prob": 0.00004908654227619991,
        "seek": 19952,
        "start": 214.84,
        "temperature": 0,
        "text": " So very quickly to do this, what I'm going to do",
        "tokens": [
          51130,
          407,
          588,
          2661,
          281,
          360,
          341,
          11,
          437,
          286,
          478,
          516,
          281,
          360,
          51275
        ]
      },
      {
        "avg_logprob": -0.21125429955081662,
        "compression_ratio": 1.868,
        "end": 222.74,
        "id": 82,
        "no_speech_prob": 0.00004908654227619991,
        "seek": 19952,
        "start": 217.74,
        "temperature": 0,
        "text": " is I'm going to create a rslider, gslider, bslider.",
        "tokens": [
          51275,
          307,
          286,
          478,
          516,
          281,
          1884,
          257,
          367,
          10418,
          1438,
          11,
          290,
          10418,
          1438,
          11,
          272,
          10418,
          1438,
          13,
          51525
        ]
      },
      {
        "avg_logprob": -0.21125429955081662,
        "compression_ratio": 1.868,
        "end": 224.4,
        "id": 83,
        "no_speech_prob": 0.00004908654227619991,
        "seek": 19952,
        "start": 222.76000000000002,
        "temperature": 0,
        "text": " I'm going to make three sliders.",
        "tokens": [
          51526,
          286,
          478,
          516,
          281,
          652,
          1045,
          1061,
          6936,
          13,
          51608
        ]
      },
      {
        "avg_logprob": -0.21125429955081662,
        "compression_ratio": 1.868,
        "end": 227.16000000000003,
        "id": 84,
        "no_speech_prob": 0.00004908654227619991,
        "seek": 19952,
        "start": 224.4,
        "temperature": 0,
        "text": " Again, this could use a lot of improvements",
        "tokens": [
          51608,
          3764,
          11,
          341,
          727,
          764,
          257,
          688,
          295,
          13797,
          51746
        ]
      },
      {
        "avg_logprob": -0.3166912833412925,
        "compression_ratio": 1.547872340425532,
        "end": 229.88,
        "id": 85,
        "no_speech_prob": 0.000002769401362456847,
        "seek": 22716,
        "start": 227.16,
        "temperature": 0,
        "text": " and I'm going to use the p5 dom library,",
        "tokens": [
          50364,
          293,
          286,
          478,
          516,
          281,
          764,
          264,
          280,
          20,
          3285,
          6405,
          11,
          50500
        ]
      },
      {
        "avg_logprob": -0.3166912833412925,
        "compression_ratio": 1.547872340425532,
        "end": 231.4,
        "id": 86,
        "no_speech_prob": 0.000002769401362456847,
        "seek": 22716,
        "start": 229.88,
        "temperature": 0,
        "text": " create slider function.",
        "tokens": [
          50500,
          1884,
          26046,
          2445,
          13,
          50576
        ]
      },
      {
        "avg_logprob": -0.3166912833412925,
        "compression_ratio": 1.547872340425532,
        "end": 234.35999999999999,
        "id": 87,
        "no_speech_prob": 0.000002769401362456847,
        "seek": 22716,
        "start": 231.4,
        "temperature": 0,
        "text": " So the slider is arranged between zero and 255.",
        "tokens": [
          50576,
          407,
          264,
          26046,
          307,
          18721,
          1296,
          4018,
          293,
          3552,
          20,
          13,
          50724
        ]
      },
      {
        "avg_logprob": -0.3166912833412925,
        "compression_ratio": 1.547872340425532,
        "end": 237.44,
        "id": 88,
        "no_speech_prob": 0.000002769401362456847,
        "seek": 22716,
        "start": 234.35999999999999,
        "temperature": 0,
        "text": " And let's start with like, what's,",
        "tokens": [
          50724,
          400,
          718,
          311,
          722,
          365,
          411,
          11,
          437,
          311,
          11,
          50878
        ]
      },
      {
        "avg_logprob": -0.3166912833412925,
        "compression_ratio": 1.547872340425532,
        "end": 239.4,
        "id": 89,
        "no_speech_prob": 0.000002769401362456847,
        "seek": 22716,
        "start": 237.44,
        "temperature": 0,
        "text": " does red and green make yellow?",
        "tokens": [
          50878,
          775,
          2182,
          293,
          3092,
          652,
          5566,
          30,
          50976
        ]
      },
      {
        "avg_logprob": -0.3166912833412925,
        "compression_ratio": 1.547872340425532,
        "end": 241.84,
        "id": 90,
        "no_speech_prob": 0.000002769401362456847,
        "seek": 22716,
        "start": 240.28,
        "temperature": 0,
        "text": " Let's start with a yellow.",
        "tokens": [
          51020,
          961,
          311,
          722,
          365,
          257,
          5566,
          13,
          51098
        ]
      },
      {
        "avg_logprob": -0.3166912833412925,
        "compression_ratio": 1.547872340425532,
        "end": 249.76,
        "id": 91,
        "no_speech_prob": 0.000002769401362456847,
        "seek": 22716,
        "start": 244.76,
        "temperature": 0,
        "text": " And so the g, the bslider should be on zero",
        "tokens": [
          51244,
          400,
          370,
          264,
          290,
          11,
          264,
          272,
          10418,
          1438,
          820,
          312,
          322,
          4018,
          51494
        ]
      },
      {
        "avg_logprob": -0.3166912833412925,
        "compression_ratio": 1.547872340425532,
        "end": 255.92,
        "id": 92,
        "no_speech_prob": 0.000002769401362456847,
        "seek": 22716,
        "start": 251.7,
        "temperature": 0,
        "text": " and then I want the background color to,",
        "tokens": [
          51591,
          293,
          550,
          286,
          528,
          264,
          3678,
          2017,
          281,
          11,
          51802
        ]
      },
      {
        "avg_logprob": -0.21761273626071304,
        "compression_ratio": 1.702127659574468,
        "end": 257.47999999999996,
        "id": 93,
        "no_speech_prob": 0.000003237761347918422,
        "seek": 25592,
        "start": 256.03999999999996,
        "temperature": 0,
        "text": " I don't know what that's doing there.",
        "tokens": [
          50370,
          286,
          500,
          380,
          458,
          437,
          300,
          311,
          884,
          456,
          13,
          50442
        ]
      },
      {
        "avg_logprob": -0.21761273626071304,
        "compression_ratio": 1.702127659574468,
        "end": 259.91999999999996,
        "id": 94,
        "no_speech_prob": 0.000003237761347918422,
        "seek": 25592,
        "start": 257.47999999999996,
        "temperature": 0,
        "text": " I don't need this line anymore, it's distracting.",
        "tokens": [
          50442,
          286,
          500,
          380,
          643,
          341,
          1622,
          3602,
          11,
          309,
          311,
          36689,
          13,
          50564
        ]
      },
      {
        "avg_logprob": -0.21761273626071304,
        "compression_ratio": 1.702127659574468,
        "end": 264.03999999999996,
        "id": 95,
        "no_speech_prob": 0.000003237761347918422,
        "seek": 25592,
        "start": 259.91999999999996,
        "temperature": 0,
        "text": " I want to say rslider, well let's actually,",
        "tokens": [
          50564,
          286,
          528,
          281,
          584,
          367,
          10418,
          1438,
          11,
          731,
          718,
          311,
          767,
          11,
          50770
        ]
      },
      {
        "avg_logprob": -0.21761273626071304,
        "compression_ratio": 1.702127659574468,
        "end": 267.64,
        "id": 96,
        "no_speech_prob": 0.000003237761347918422,
        "seek": 25592,
        "start": 264.03999999999996,
        "temperature": 0,
        "text": " so I want to say let r equal rslider.value.",
        "tokens": [
          50770,
          370,
          286,
          528,
          281,
          584,
          718,
          367,
          2681,
          367,
          10418,
          1438,
          13,
          29155,
          13,
          50950
        ]
      },
      {
        "avg_logprob": -0.21761273626071304,
        "compression_ratio": 1.702127659574468,
        "end": 272.15999999999997,
        "id": 97,
        "no_speech_prob": 0.000003237761347918422,
        "seek": 25592,
        "start": 269.44,
        "temperature": 0,
        "text": " So I want to get the values from the sliders.",
        "tokens": [
          51040,
          407,
          286,
          528,
          281,
          483,
          264,
          4190,
          490,
          264,
          1061,
          6936,
          13,
          51176
        ]
      },
      {
        "avg_logprob": -0.21761273626071304,
        "compression_ratio": 1.702127659574468,
        "end": 274.68,
        "id": 98,
        "no_speech_prob": 0.000003237761347918422,
        "seek": 25592,
        "start": 272.15999999999997,
        "temperature": 0,
        "text": " I want g and I want b.",
        "tokens": [
          51176,
          286,
          528,
          290,
          293,
          286,
          528,
          272,
          13,
          51302
        ]
      },
      {
        "avg_logprob": -0.21761273626071304,
        "compression_ratio": 1.702127659574468,
        "end": 277.47999999999996,
        "id": 99,
        "no_speech_prob": 0.000003237761347918422,
        "seek": 25592,
        "start": 275.68,
        "temperature": 0,
        "text": " Eventually I'm going to send these as inputs",
        "tokens": [
          51352,
          17586,
          286,
          478,
          516,
          281,
          2845,
          613,
          382,
          15743,
          51442
        ]
      },
      {
        "avg_logprob": -0.21761273626071304,
        "compression_ratio": 1.702127659574468,
        "end": 278.71999999999997,
        "id": 100,
        "no_speech_prob": 0.000003237761347918422,
        "seek": 25592,
        "start": 277.47999999999996,
        "temperature": 0,
        "text": " into the neural network, but right now",
        "tokens": [
          51442,
          666,
          264,
          18161,
          3209,
          11,
          457,
          558,
          586,
          51504
        ]
      },
      {
        "avg_logprob": -0.21761273626071304,
        "compression_ratio": 1.702127659574468,
        "end": 280.84,
        "id": 101,
        "no_speech_prob": 0.000003237761347918422,
        "seek": 25592,
        "start": 278.71999999999997,
        "temperature": 0,
        "text": " I just want to be able to see that color.",
        "tokens": [
          51504,
          286,
          445,
          528,
          281,
          312,
          1075,
          281,
          536,
          300,
          2017,
          13,
          51610
        ]
      },
      {
        "avg_logprob": -0.21761273626071304,
        "compression_ratio": 1.702127659574468,
        "end": 283,
        "id": 102,
        "no_speech_prob": 0.000003237761347918422,
        "seek": 25592,
        "start": 280.84,
        "temperature": 0,
        "text": " R, g, b, okay?",
        "tokens": [
          51610,
          497,
          11,
          290,
          11,
          272,
          11,
          1392,
          30,
          51718
        ]
      },
      {
        "avg_logprob": -0.21761273626071304,
        "compression_ratio": 1.702127659574468,
        "end": 284.44,
        "id": 103,
        "no_speech_prob": 0.000003237761347918422,
        "seek": 25592,
        "start": 283,
        "temperature": 0,
        "text": " So here we go.",
        "tokens": [
          51718,
          407,
          510,
          321,
          352,
          13,
          51790
        ]
      },
      {
        "avg_logprob": -0.2334364898933852,
        "compression_ratio": 1.6431718061674008,
        "end": 286.84,
        "id": 104,
        "no_speech_prob": 9.57082647801144e-7,
        "seek": 28444,
        "start": 284.44,
        "temperature": 0,
        "text": " So now we should see there are three sliders",
        "tokens": [
          50364,
          407,
          586,
          321,
          820,
          536,
          456,
          366,
          1045,
          1061,
          6936,
          50484
        ]
      },
      {
        "avg_logprob": -0.2334364898933852,
        "compression_ratio": 1.6431718061674008,
        "end": 290.26,
        "id": 105,
        "no_speech_prob": 9.57082647801144e-7,
        "seek": 28444,
        "start": 286.84,
        "temperature": 0,
        "text": " and as I adjust these sliders, I can change the color.",
        "tokens": [
          50484,
          293,
          382,
          286,
          4369,
          613,
          1061,
          6936,
          11,
          286,
          393,
          1319,
          264,
          2017,
          13,
          50655
        ]
      },
      {
        "avg_logprob": -0.2334364898933852,
        "compression_ratio": 1.6431718061674008,
        "end": 292.16,
        "id": 106,
        "no_speech_prob": 9.57082647801144e-7,
        "seek": 28444,
        "start": 290.26,
        "temperature": 0,
        "text": " And so what I want, whoops,",
        "tokens": [
          50655,
          400,
          370,
          437,
          286,
          528,
          11,
          567,
          3370,
          11,
          50750
        ]
      },
      {
        "avg_logprob": -0.2334364898933852,
        "compression_ratio": 1.6431718061674008,
        "end": 294.88,
        "id": 107,
        "no_speech_prob": 9.57082647801144e-7,
        "seek": 28444,
        "start": 293.28,
        "temperature": 0,
        "text": " what I want is to be able to,",
        "tokens": [
          50806,
          437,
          286,
          528,
          307,
          281,
          312,
          1075,
          281,
          11,
          50886
        ]
      },
      {
        "avg_logprob": -0.2334364898933852,
        "compression_ratio": 1.6431718061674008,
        "end": 296.4,
        "id": 108,
        "no_speech_prob": 9.57082647801144e-7,
        "seek": 28444,
        "start": 294.88,
        "temperature": 0,
        "text": " and I see the, what I want is now",
        "tokens": [
          50886,
          293,
          286,
          536,
          264,
          11,
          437,
          286,
          528,
          307,
          586,
          50962
        ]
      },
      {
        "avg_logprob": -0.2334364898933852,
        "compression_ratio": 1.6431718061674008,
        "end": 299.24,
        "id": 109,
        "no_speech_prob": 9.57082647801144e-7,
        "seek": 28444,
        "start": 296.4,
        "temperature": 0,
        "text": " to see the neural network's prediction down here.",
        "tokens": [
          50962,
          281,
          536,
          264,
          18161,
          3209,
          311,
          17630,
          760,
          510,
          13,
          51104
        ]
      },
      {
        "avg_logprob": -0.2334364898933852,
        "compression_ratio": 1.6431718061674008,
        "end": 301.32,
        "id": 110,
        "no_speech_prob": 9.57082647801144e-7,
        "seek": 28444,
        "start": 299.24,
        "temperature": 0,
        "text": " So how do I do that?",
        "tokens": [
          51104,
          407,
          577,
          360,
          286,
          360,
          300,
          30,
          51208
        ]
      },
      {
        "avg_logprob": -0.2334364898933852,
        "compression_ratio": 1.6431718061674008,
        "end": 306.28,
        "id": 111,
        "no_speech_prob": 9.57082647801144e-7,
        "seek": 28444,
        "start": 301.32,
        "temperature": 0,
        "text": " Okay, time to use tensorflow.js again, woohoo!",
        "tokens": [
          51208,
          1033,
          11,
          565,
          281,
          764,
          40863,
          10565,
          13,
          25530,
          797,
          11,
          21657,
          19069,
          0,
          51456
        ]
      },
      {
        "avg_logprob": -0.2334364898933852,
        "compression_ratio": 1.6431718061674008,
        "end": 308.92,
        "id": 112,
        "no_speech_prob": 9.57082647801144e-7,
        "seek": 28444,
        "start": 306.28,
        "temperature": 0,
        "text": " So I need to make some input data.",
        "tokens": [
          51456,
          407,
          286,
          643,
          281,
          652,
          512,
          4846,
          1412,
          13,
          51588
        ]
      },
      {
        "avg_logprob": -0.2334364898933852,
        "compression_ratio": 1.6431718061674008,
        "end": 313.76,
        "id": 113,
        "no_speech_prob": 9.57082647801144e-7,
        "seek": 28444,
        "start": 308.92,
        "temperature": 0,
        "text": " So the input x's are tensor,",
        "tokens": [
          51588,
          407,
          264,
          4846,
          2031,
          311,
          366,
          40863,
          11,
          51830
        ]
      },
      {
        "avg_logprob": -0.23106257750256226,
        "compression_ratio": 1.4835164835164836,
        "end": 319.16,
        "id": 114,
        "no_speech_prob": 2.9649055477420916e-7,
        "seek": 31444,
        "start": 314.64,
        "temperature": 0,
        "text": " tf.tensor2d,",
        "tokens": [
          50374,
          256,
          69,
          13,
          83,
          23153,
          17,
          67,
          11,
          50600
        ]
      },
      {
        "avg_logprob": -0.23106257750256226,
        "compression_ratio": 1.4835164835164836,
        "end": 322,
        "id": 115,
        "no_speech_prob": 2.9649055477420916e-7,
        "seek": 31444,
        "start": 319.16,
        "temperature": 0,
        "text": " and an array with r, g, b in it.",
        "tokens": [
          50600,
          293,
          364,
          10225,
          365,
          367,
          11,
          290,
          11,
          272,
          294,
          309,
          13,
          50742
        ]
      },
      {
        "avg_logprob": -0.23106257750256226,
        "compression_ratio": 1.4835164835164836,
        "end": 325.71999999999997,
        "id": 116,
        "no_speech_prob": 2.9649055477420916e-7,
        "seek": 31444,
        "start": 322,
        "temperature": 0,
        "text": " Now in theory, I could be running prediction",
        "tokens": [
          50742,
          823,
          294,
          5261,
          11,
          286,
          727,
          312,
          2614,
          17630,
          50928
        ]
      },
      {
        "avg_logprob": -0.23106257750256226,
        "compression_ratio": 1.4835164835164836,
        "end": 328.8,
        "id": 117,
        "no_speech_prob": 2.9649055477420916e-7,
        "seek": 31444,
        "start": 325.71999999999997,
        "temperature": 0,
        "text": " with multiple r, g, b's, right?",
        "tokens": [
          50928,
          365,
          3866,
          367,
          11,
          290,
          11,
          272,
          311,
          11,
          558,
          30,
          51082
        ]
      },
      {
        "avg_logprob": -0.23106257750256226,
        "compression_ratio": 1.4835164835164836,
        "end": 332.58,
        "id": 118,
        "no_speech_prob": 2.9649055477420916e-7,
        "seek": 31444,
        "start": 328.8,
        "temperature": 0,
        "text": " But I'm not, so I need an array of arrays in here.",
        "tokens": [
          51082,
          583,
          286,
          478,
          406,
          11,
          370,
          286,
          643,
          364,
          10225,
          295,
          41011,
          294,
          510,
          13,
          51271
        ]
      },
      {
        "avg_logprob": -0.23106257750256226,
        "compression_ratio": 1.4835164835164836,
        "end": 336.8,
        "id": 119,
        "no_speech_prob": 2.9649055477420916e-7,
        "seek": 31444,
        "start": 334.28,
        "temperature": 0,
        "text": " So this is my input data.",
        "tokens": [
          51356,
          407,
          341,
          307,
          452,
          4846,
          1412,
          13,
          51482
        ]
      },
      {
        "avg_logprob": -0.23106257750256226,
        "compression_ratio": 1.4835164835164836,
        "end": 338.46,
        "id": 120,
        "no_speech_prob": 2.9649055477420916e-7,
        "seek": 31444,
        "start": 336.8,
        "temperature": 0,
        "text": " Then what do I want to do?",
        "tokens": [
          51482,
          1396,
          437,
          360,
          286,
          528,
          281,
          360,
          30,
          51565
        ]
      },
      {
        "avg_logprob": -0.23106257750256226,
        "compression_ratio": 1.4835164835164836,
        "end": 343.4,
        "id": 121,
        "no_speech_prob": 2.9649055477420916e-7,
        "seek": 31444,
        "start": 338.46,
        "temperature": 0,
        "text": " I want to say model.predict with those x's.",
        "tokens": [
          51565,
          286,
          528,
          281,
          584,
          2316,
          13,
          79,
          24945,
          365,
          729,
          2031,
          311,
          13,
          51812
        ]
      },
      {
        "avg_logprob": -0.24046417077382407,
        "compression_ratio": 1.5436893203883495,
        "end": 345.03999999999996,
        "id": 122,
        "no_speech_prob": 1.2359566881059436e-7,
        "seek": 34340,
        "start": 343.4,
        "temperature": 0,
        "text": " Feel like there's, oh, you know what?",
        "tokens": [
          50364,
          14113,
          411,
          456,
          311,
          11,
          1954,
          11,
          291,
          458,
          437,
          30,
          50446
        ]
      },
      {
        "avg_logprob": -0.24046417077382407,
        "compression_ratio": 1.5436893203883495,
        "end": 346.67999999999995,
        "id": 123,
        "no_speech_prob": 1.2359566881059436e-7,
        "seek": 34340,
        "start": 345.03999999999996,
        "temperature": 0,
        "text": " I need to normalize those.",
        "tokens": [
          50446,
          286,
          643,
          281,
          2710,
          1125,
          729,
          13,
          50528
        ]
      },
      {
        "avg_logprob": -0.24046417077382407,
        "compression_ratio": 1.5436893203883495,
        "end": 352.44,
        "id": 124,
        "no_speech_prob": 1.2359566881059436e-7,
        "seek": 34340,
        "start": 349.4,
        "temperature": 0,
        "text": " Right, because it expects to have normalized values",
        "tokens": [
          50664,
          1779,
          11,
          570,
          309,
          33280,
          281,
          362,
          48704,
          4190,
          50816
        ]
      },
      {
        "avg_logprob": -0.24046417077382407,
        "compression_ratio": 1.5436893203883495,
        "end": 353.88,
        "id": 125,
        "no_speech_prob": 1.2359566881059436e-7,
        "seek": 34340,
        "start": 352.44,
        "temperature": 0,
        "text": " between zero and one, so I need to divide",
        "tokens": [
          50816,
          1296,
          4018,
          293,
          472,
          11,
          370,
          286,
          643,
          281,
          9845,
          50888
        ]
      },
      {
        "avg_logprob": -0.24046417077382407,
        "compression_ratio": 1.5436893203883495,
        "end": 355.9,
        "id": 126,
        "no_speech_prob": 1.2359566881059436e-7,
        "seek": 34340,
        "start": 353.88,
        "temperature": 0,
        "text": " each of those by 255.",
        "tokens": [
          50888,
          1184,
          295,
          729,
          538,
          3552,
          20,
          13,
          50989
        ]
      },
      {
        "avg_logprob": -0.24046417077382407,
        "compression_ratio": 1.5436893203883495,
        "end": 358.23999999999995,
        "id": 127,
        "no_speech_prob": 1.2359566881059436e-7,
        "seek": 34340,
        "start": 355.9,
        "temperature": 0,
        "text": " Then I need to call model.predict",
        "tokens": [
          50989,
          1396,
          286,
          643,
          281,
          818,
          2316,
          13,
          79,
          24945,
          51106
        ]
      },
      {
        "avg_logprob": -0.24046417077382407,
        "compression_ratio": 1.5436893203883495,
        "end": 362.15999999999997,
        "id": 128,
        "no_speech_prob": 1.2359566881059436e-7,
        "seek": 34340,
        "start": 358.23999999999995,
        "temperature": 0,
        "text": " and then look at the results.",
        "tokens": [
          51106,
          293,
          550,
          574,
          412,
          264,
          3542,
          13,
          51302
        ]
      },
      {
        "avg_logprob": -0.24046417077382407,
        "compression_ratio": 1.5436893203883495,
        "end": 368.44,
        "id": 129,
        "no_speech_prob": 1.2359566881059436e-7,
        "seek": 34340,
        "start": 366.03999999999996,
        "temperature": 0,
        "text": " And that, oh, you know what?",
        "tokens": [
          51496,
          400,
          300,
          11,
          1954,
          11,
          291,
          458,
          437,
          30,
          51616
        ]
      },
      {
        "avg_logprob": -0.24046417077382407,
        "compression_ratio": 1.5436893203883495,
        "end": 370.64,
        "id": 130,
        "no_speech_prob": 1.2359566881059436e-7,
        "seek": 34340,
        "start": 368.44,
        "temperature": 0,
        "text": " This doesn't actually happen asynchronously.",
        "tokens": [
          51616,
          639,
          1177,
          380,
          767,
          1051,
          42642,
          5098,
          13,
          51726
        ]
      },
      {
        "avg_logprob": -0.2343912422657013,
        "compression_ratio": 1.7302904564315353,
        "end": 376.67999999999995,
        "id": 131,
        "no_speech_prob": 8.446225479019631e-7,
        "seek": 37340,
        "start": 373.91999999999996,
        "temperature": 0,
        "text": " It's because the data is still in the GPU.",
        "tokens": [
          50390,
          467,
          311,
          570,
          264,
          1412,
          307,
          920,
          294,
          264,
          18407,
          13,
          50528
        ]
      },
      {
        "avg_logprob": -0.2343912422657013,
        "compression_ratio": 1.7302904564315353,
        "end": 377.71999999999997,
        "id": 132,
        "no_speech_prob": 8.446225479019631e-7,
        "seek": 37340,
        "start": 376.67999999999995,
        "temperature": 0,
        "text": " This is a confusing thing.",
        "tokens": [
          50528,
          639,
          307,
          257,
          13181,
          551,
          13,
          50580
        ]
      },
      {
        "avg_logprob": -0.2343912422657013,
        "compression_ratio": 1.7302904564315353,
        "end": 379.88,
        "id": 133,
        "no_speech_prob": 8.446225479019631e-7,
        "seek": 37340,
        "start": 377.71999999999997,
        "temperature": 0,
        "text": " I have to pull, I'm going to use that data.",
        "tokens": [
          50580,
          286,
          362,
          281,
          2235,
          11,
          286,
          478,
          516,
          281,
          764,
          300,
          1412,
          13,
          50688
        ]
      },
      {
        "avg_logprob": -0.2343912422657013,
        "compression_ratio": 1.7302904564315353,
        "end": 382.12,
        "id": 134,
        "no_speech_prob": 8.446225479019631e-7,
        "seek": 37340,
        "start": 379.88,
        "temperature": 0,
        "text": " I have to pull it out, but let's just look at the results,",
        "tokens": [
          50688,
          286,
          362,
          281,
          2235,
          309,
          484,
          11,
          457,
          718,
          311,
          445,
          574,
          412,
          264,
          3542,
          11,
          50800
        ]
      },
      {
        "avg_logprob": -0.2343912422657013,
        "compression_ratio": 1.7302904564315353,
        "end": 383.59999999999997,
        "id": 135,
        "no_speech_prob": 8.446225479019631e-7,
        "seek": 37340,
        "start": 382.12,
        "temperature": 0,
        "text": " the pure results.",
        "tokens": [
          50800,
          264,
          6075,
          3542,
          13,
          50874
        ]
      },
      {
        "avg_logprob": -0.2343912422657013,
        "compression_ratio": 1.7302904564315353,
        "end": 388.59999999999997,
        "id": 136,
        "no_speech_prob": 8.446225479019631e-7,
        "seek": 37340,
        "start": 383.59999999999997,
        "temperature": 0,
        "text": " So I should then be able to say results.print, okay?",
        "tokens": [
          50874,
          407,
          286,
          820,
          550,
          312,
          1075,
          281,
          584,
          3542,
          13,
          14030,
          11,
          1392,
          30,
          51124
        ]
      },
      {
        "avg_logprob": -0.2343912422657013,
        "compression_ratio": 1.7302904564315353,
        "end": 392.76,
        "id": 137,
        "no_speech_prob": 8.446225479019631e-7,
        "seek": 37340,
        "start": 389.08,
        "temperature": 0,
        "text": " So I think this is me just creating the inputs,",
        "tokens": [
          51148,
          407,
          286,
          519,
          341,
          307,
          385,
          445,
          4084,
          264,
          15743,
          11,
          51332
        ]
      },
      {
        "avg_logprob": -0.2343912422657013,
        "compression_ratio": 1.7302904564315353,
        "end": 394.12,
        "id": 138,
        "no_speech_prob": 8.446225479019631e-7,
        "seek": 37340,
        "start": 392.76,
        "temperature": 0,
        "text": " getting the prediction, and then I should be able",
        "tokens": [
          51332,
          1242,
          264,
          17630,
          11,
          293,
          550,
          286,
          820,
          312,
          1075,
          51400
        ]
      },
      {
        "avg_logprob": -0.2343912422657013,
        "compression_ratio": 1.7302904564315353,
        "end": 395.47999999999996,
        "id": 139,
        "no_speech_prob": 8.446225479019631e-7,
        "seek": 37340,
        "start": 394.12,
        "temperature": 0,
        "text": " to see that in the console.",
        "tokens": [
          51400,
          281,
          536,
          300,
          294,
          264,
          11076,
          13,
          51468
        ]
      },
      {
        "avg_logprob": -0.2343912422657013,
        "compression_ratio": 1.7302904564315353,
        "end": 399.2,
        "id": 140,
        "no_speech_prob": 8.446225479019631e-7,
        "seek": 37340,
        "start": 398.2,
        "temperature": 0,
        "text": " Syntax error.",
        "tokens": [
          51604,
          3902,
          580,
          2797,
          6713,
          13,
          51654
        ]
      },
      {
        "avg_logprob": -0.2343912422657013,
        "compression_ratio": 1.7302904564315353,
        "end": 402.52,
        "id": 141,
        "no_speech_prob": 8.446225479019631e-7,
        "seek": 37340,
        "start": 399.2,
        "temperature": 0,
        "text": " Do I have an extra curly bracket?",
        "tokens": [
          51654,
          1144,
          286,
          362,
          364,
          2857,
          32066,
          16904,
          30,
          51820
        ]
      },
      {
        "avg_logprob": -0.17378575226356244,
        "compression_ratio": 1.5748031496062993,
        "end": 406.23999999999995,
        "id": 142,
        "no_speech_prob": 0.0000036688625186798163,
        "seek": 40340,
        "start": 404,
        "temperature": 0,
        "text": " All right, okay, so we can see this,",
        "tokens": [
          50394,
          1057,
          558,
          11,
          1392,
          11,
          370,
          321,
          393,
          536,
          341,
          11,
          50506
        ]
      },
      {
        "avg_logprob": -0.17378575226356244,
        "compression_ratio": 1.5748031496062993,
        "end": 408.64,
        "id": 143,
        "no_speech_prob": 0.0000036688625186798163,
        "seek": 40340,
        "start": 406.23999999999995,
        "temperature": 0,
        "text": " and this is exactly what I should be getting, right?",
        "tokens": [
          50506,
          293,
          341,
          307,
          2293,
          437,
          286,
          820,
          312,
          1242,
          11,
          558,
          30,
          50626
        ]
      },
      {
        "avg_logprob": -0.17378575226356244,
        "compression_ratio": 1.5748031496062993,
        "end": 413.08,
        "id": 144,
        "no_speech_prob": 0.0000036688625186798163,
        "seek": 40340,
        "start": 408.64,
        "temperature": 0,
        "text": " It is a probability distribution over nine labels.",
        "tokens": [
          50626,
          467,
          307,
          257,
          8482,
          7316,
          670,
          4949,
          16949,
          13,
          50848
        ]
      },
      {
        "avg_logprob": -0.17378575226356244,
        "compression_ratio": 1.5748031496062993,
        "end": 415.41999999999996,
        "id": 145,
        "no_speech_prob": 0.0000036688625186798163,
        "seek": 40340,
        "start": 413.08,
        "temperature": 0,
        "text": " Now, whether it's giving me correct ones, who knows?",
        "tokens": [
          50848,
          823,
          11,
          1968,
          309,
          311,
          2902,
          385,
          3006,
          2306,
          11,
          567,
          3255,
          30,
          50965
        ]
      },
      {
        "avg_logprob": -0.17378575226356244,
        "compression_ratio": 1.5748031496062993,
        "end": 416.26,
        "id": 146,
        "no_speech_prob": 0.0000036688625186798163,
        "seek": 40340,
        "start": 415.41999999999996,
        "temperature": 0,
        "text": " But look at that.",
        "tokens": [
          50965,
          583,
          574,
          412,
          300,
          13,
          51007
        ]
      },
      {
        "avg_logprob": -0.17378575226356244,
        "compression_ratio": 1.5748031496062993,
        "end": 420.47999999999996,
        "id": 147,
        "no_speech_prob": 0.0000036688625186798163,
        "seek": 40340,
        "start": 416.26,
        "temperature": 0,
        "text": " So now, how do I get the label out of there?",
        "tokens": [
          51007,
          407,
          586,
          11,
          577,
          360,
          286,
          483,
          264,
          7645,
          484,
          295,
          456,
          30,
          51218
        ]
      },
      {
        "avg_logprob": -0.17378575226356244,
        "compression_ratio": 1.5748031496062993,
        "end": 423.91999999999996,
        "id": 148,
        "no_speech_prob": 0.0000036688625186798163,
        "seek": 40340,
        "start": 420.47999999999996,
        "temperature": 0,
        "text": " Well, remember that what I'm looking for",
        "tokens": [
          51218,
          1042,
          11,
          1604,
          300,
          437,
          286,
          478,
          1237,
          337,
          51390
        ]
      },
      {
        "avg_logprob": -0.17378575226356244,
        "compression_ratio": 1.5748031496062993,
        "end": 428.52,
        "id": 149,
        "no_speech_prob": 0.0000036688625186798163,
        "seek": 40340,
        "start": 423.91999999999996,
        "temperature": 0,
        "text": " is I'm looking for which probability is at the highest level.",
        "tokens": [
          51390,
          307,
          286,
          478,
          1237,
          337,
          597,
          8482,
          307,
          412,
          264,
          6343,
          1496,
          13,
          51620
        ]
      },
      {
        "avg_logprob": -0.17378575226356244,
        "compression_ratio": 1.5748031496062993,
        "end": 431.88,
        "id": 150,
        "no_speech_prob": 0.0000036688625186798163,
        "seek": 40340,
        "start": 428.52,
        "temperature": 0,
        "text": " Is it a 90% chance of it being yellowish",
        "tokens": [
          51620,
          1119,
          309,
          257,
          4289,
          4,
          2931,
          295,
          309,
          885,
          5566,
          742,
          51788
        ]
      },
      {
        "avg_logprob": -0.16547722475869314,
        "compression_ratio": 1.5106382978723405,
        "end": 436.88,
        "id": 151,
        "no_speech_prob": 0.000008139691999531351,
        "seek": 43188,
        "start": 431.88,
        "temperature": 0,
        "text": " and.01,.02,.03, 1%, 2%, 3% of being the other ones?",
        "tokens": [
          50364,
          293,
          2411,
          10607,
          11,
          2411,
          12756,
          11,
          2411,
          11592,
          11,
          502,
          8923,
          568,
          8923,
          805,
          4,
          295,
          885,
          264,
          661,
          2306,
          30,
          50614
        ]
      },
      {
        "avg_logprob": -0.16547722475869314,
        "compression_ratio": 1.5106382978723405,
        "end": 440.4,
        "id": 152,
        "no_speech_prob": 0.000008139691999531351,
        "seek": 43188,
        "start": 437.4,
        "temperature": 0,
        "text": " And there actually is a function in TensorFlow.js",
        "tokens": [
          50640,
          400,
          456,
          767,
          307,
          257,
          2445,
          294,
          37624,
          13,
          25530,
          50790
        ]
      },
      {
        "avg_logprob": -0.16547722475869314,
        "compression_ratio": 1.5106382978723405,
        "end": 443.26,
        "id": 153,
        "no_speech_prob": 0.000008139691999531351,
        "seek": 43188,
        "start": 440.4,
        "temperature": 0,
        "text": " that will pull out the index",
        "tokens": [
          50790,
          300,
          486,
          2235,
          484,
          264,
          8186,
          50933
        ]
      },
      {
        "avg_logprob": -0.16547722475869314,
        "compression_ratio": 1.5106382978723405,
        "end": 445.4,
        "id": 154,
        "no_speech_prob": 0.000008139691999531351,
        "seek": 43188,
        "start": 443.26,
        "temperature": 0,
        "text": " of the highest probability value.",
        "tokens": [
          50933,
          295,
          264,
          6343,
          8482,
          2158,
          13,
          51040
        ]
      },
      {
        "avg_logprob": -0.16547722475869314,
        "compression_ratio": 1.5106382978723405,
        "end": 447.88,
        "id": 155,
        "no_speech_prob": 0.000008139691999531351,
        "seek": 43188,
        "start": 445.4,
        "temperature": 0,
        "text": " That's called argmax, right?",
        "tokens": [
          51040,
          663,
          311,
          1219,
          3882,
          41167,
          11,
          558,
          30,
          51164
        ]
      },
      {
        "avg_logprob": -0.16547722475869314,
        "compression_ratio": 1.5106382978723405,
        "end": 449.2,
        "id": 156,
        "no_speech_prob": 0.000008139691999531351,
        "seek": 43188,
        "start": 447.88,
        "temperature": 0,
        "text": " I could write a little for loop",
        "tokens": [
          51164,
          286,
          727,
          2464,
          257,
          707,
          337,
          6367,
          51230
        ]
      },
      {
        "avg_logprob": -0.16547722475869314,
        "compression_ratio": 1.5106382978723405,
        "end": 451.52,
        "id": 157,
        "no_speech_prob": 0.000008139691999531351,
        "seek": 43188,
        "start": 449.2,
        "temperature": 0,
        "text": " or some kind of function to do that,",
        "tokens": [
          51230,
          420,
          512,
          733,
          295,
          2445,
          281,
          360,
          300,
          11,
          51346
        ]
      },
      {
        "avg_logprob": -0.16547722475869314,
        "compression_ratio": 1.5106382978723405,
        "end": 456.52,
        "id": 158,
        "no_speech_prob": 0.000008139691999531351,
        "seek": 43188,
        "start": 451.52,
        "temperature": 0,
        "text": " but if I look for argmax, tf.argmax returns the indices",
        "tokens": [
          51346,
          457,
          498,
          286,
          574,
          337,
          3882,
          41167,
          11,
          256,
          69,
          13,
          33544,
          41167,
          11247,
          264,
          43840,
          51596
        ]
      },
      {
        "avg_logprob": -0.16547722475869314,
        "compression_ratio": 1.5106382978723405,
        "end": 460.08,
        "id": 159,
        "no_speech_prob": 0.000008139691999531351,
        "seek": 43188,
        "start": 457.6,
        "temperature": 0,
        "text": " of the maximum values along an axis.",
        "tokens": [
          51650,
          295,
          264,
          6674,
          4190,
          2051,
          364,
          10298,
          13,
          51774
        ]
      },
      {
        "avg_logprob": -0.30523494879404706,
        "compression_ratio": 1.6108108108108108,
        "end": 462.15999999999997,
        "id": 160,
        "no_speech_prob": 2.238028713463791e-7,
        "seek": 46008,
        "start": 460.08,
        "temperature": 0,
        "text": " So this can be quite more complex",
        "tokens": [
          50364,
          407,
          341,
          393,
          312,
          1596,
          544,
          3997,
          50468
        ]
      },
      {
        "avg_logprob": -0.30523494879404706,
        "compression_ratio": 1.6108108108108108,
        "end": 464,
        "id": 161,
        "no_speech_prob": 2.238028713463791e-7,
        "seek": 46008,
        "start": 462.15999999999997,
        "temperature": 0,
        "text": " because I can have multi-dimensional data,",
        "tokens": [
          50468,
          570,
          286,
          393,
          362,
          4825,
          12,
          18759,
          1412,
          11,
          50560
        ]
      },
      {
        "avg_logprob": -0.30523494879404706,
        "compression_ratio": 1.6108108108108108,
        "end": 466.71999999999997,
        "id": 162,
        "no_speech_prob": 2.238028713463791e-7,
        "seek": 46008,
        "start": 464,
        "temperature": 0,
        "text": " but I actually get to do this in a really simple way.",
        "tokens": [
          50560,
          457,
          286,
          767,
          483,
          281,
          360,
          341,
          294,
          257,
          534,
          2199,
          636,
          13,
          50696
        ]
      },
      {
        "avg_logprob": -0.30523494879404706,
        "compression_ratio": 1.6108108108108108,
        "end": 468.24,
        "id": 163,
        "no_speech_prob": 2.238028713463791e-7,
        "seek": 46008,
        "start": 466.71999999999997,
        "temperature": 0,
        "text": " I just want to say,",
        "tokens": [
          50696,
          286,
          445,
          528,
          281,
          584,
          11,
          50772
        ]
      },
      {
        "avg_logprob": -0.30523494879404706,
        "compression_ratio": 1.6108108108108108,
        "end": 474.88,
        "id": 164,
        "no_speech_prob": 2.238028713463791e-7,
        "seek": 46008,
        "start": 470.47999999999996,
        "temperature": 0,
        "text": " let index equal results.argmax,",
        "tokens": [
          50884,
          718,
          8186,
          2681,
          3542,
          13,
          33544,
          41167,
          11,
          51104
        ]
      },
      {
        "avg_logprob": -0.30523494879404706,
        "compression_ratio": 1.6108108108108108,
        "end": 479.32,
        "id": 165,
        "no_speech_prob": 2.238028713463791e-7,
        "seek": 46008,
        "start": 475.91999999999996,
        "temperature": 0,
        "text": " and there's an axis of one,",
        "tokens": [
          51156,
          293,
          456,
          311,
          364,
          10298,
          295,
          472,
          11,
          51326
        ]
      },
      {
        "avg_logprob": -0.30523494879404706,
        "compression_ratio": 1.6108108108108108,
        "end": 480.86,
        "id": 166,
        "no_speech_prob": 2.238028713463791e-7,
        "seek": 46008,
        "start": 479.32,
        "temperature": 0,
        "text": " there's a one-dimensional here.",
        "tokens": [
          51326,
          456,
          311,
          257,
          472,
          12,
          18759,
          510,
          13,
          51403
        ]
      },
      {
        "avg_logprob": -0.30523494879404706,
        "compression_ratio": 1.6108108108108108,
        "end": 483.24,
        "id": 167,
        "no_speech_prob": 2.238028713463791e-7,
        "seek": 46008,
        "start": 480.86,
        "temperature": 0,
        "text": " So now, let me say index.print,",
        "tokens": [
          51403,
          407,
          586,
          11,
          718,
          385,
          584,
          8186,
          13,
          14030,
          11,
          51522
        ]
      },
      {
        "avg_logprob": -0.30523494879404706,
        "compression_ratio": 1.6108108108108108,
        "end": 487.32,
        "id": 168,
        "no_speech_prob": 2.238028713463791e-7,
        "seek": 46008,
        "start": 485.12,
        "temperature": 0,
        "text": " and so let me run this,",
        "tokens": [
          51616,
          293,
          370,
          718,
          385,
          1190,
          341,
          11,
          51726
        ]
      },
      {
        "avg_logprob": -0.21483184151027512,
        "compression_ratio": 1.587719298245614,
        "end": 490.71999999999997,
        "id": 169,
        "no_speech_prob": 0.000002295919102834887,
        "seek": 48732,
        "start": 488.15999999999997,
        "temperature": 0,
        "text": " and we can see it's just giving me,",
        "tokens": [
          50406,
          293,
          321,
          393,
          536,
          309,
          311,
          445,
          2902,
          385,
          11,
          50534
        ]
      },
      {
        "avg_logprob": -0.21483184151027512,
        "compression_ratio": 1.587719298245614,
        "end": 492.12,
        "id": 170,
        "no_speech_prob": 0.000002295919102834887,
        "seek": 48732,
        "start": 490.71999999999997,
        "temperature": 0,
        "text": " hmm, is that right?",
        "tokens": [
          50534,
          16478,
          11,
          307,
          300,
          558,
          30,
          50604
        ]
      },
      {
        "avg_logprob": -0.21483184151027512,
        "compression_ratio": 1.587719298245614,
        "end": 493.96,
        "id": 171,
        "no_speech_prob": 0.000002295919102834887,
        "seek": 48732,
        "start": 492.12,
        "temperature": 0,
        "text": " Is that a coincidence?",
        "tokens": [
          50604,
          1119,
          300,
          257,
          22137,
          30,
          50696
        ]
      },
      {
        "avg_logprob": -0.21483184151027512,
        "compression_ratio": 1.587719298245614,
        "end": 497.2,
        "id": 172,
        "no_speech_prob": 0.000002295919102834887,
        "seek": 48732,
        "start": 493.96,
        "temperature": 0,
        "text": " So I should get some different values.",
        "tokens": [
          50696,
          407,
          286,
          820,
          483,
          512,
          819,
          4190,
          13,
          50858
        ]
      },
      {
        "avg_logprob": -0.21483184151027512,
        "compression_ratio": 1.587719298245614,
        "end": 499.64,
        "id": 173,
        "no_speech_prob": 0.000002295919102834887,
        "seek": 48732,
        "start": 497.2,
        "temperature": 0,
        "text": " Yes, okay, so it's actually changing.",
        "tokens": [
          50858,
          1079,
          11,
          1392,
          11,
          370,
          309,
          311,
          767,
          4473,
          13,
          50980
        ]
      },
      {
        "avg_logprob": -0.21483184151027512,
        "compression_ratio": 1.587719298245614,
        "end": 502.68,
        "id": 174,
        "no_speech_prob": 0.000002295919102834887,
        "seek": 48732,
        "start": 499.64,
        "temperature": 0,
        "text": " So that's giving me that maximum index.",
        "tokens": [
          50980,
          407,
          300,
          311,
          2902,
          385,
          300,
          6674,
          8186,
          13,
          51132
        ]
      },
      {
        "avg_logprob": -0.21483184151027512,
        "compression_ratio": 1.587719298245614,
        "end": 506.53999999999996,
        "id": 175,
        "no_speech_prob": 0.000002295919102834887,
        "seek": 48732,
        "start": 502.68,
        "temperature": 0,
        "text": " So as I change, so this is my label.",
        "tokens": [
          51132,
          407,
          382,
          286,
          1319,
          11,
          370,
          341,
          307,
          452,
          7645,
          13,
          51325
        ]
      },
      {
        "avg_logprob": -0.21483184151027512,
        "compression_ratio": 1.587719298245614,
        "end": 507.38,
        "id": 176,
        "no_speech_prob": 0.000002295919102834887,
        "seek": 48732,
        "start": 506.53999999999996,
        "temperature": 0,
        "text": " Here's the thing, though.",
        "tokens": [
          51325,
          1692,
          311,
          264,
          551,
          11,
          1673,
          13,
          51367
        ]
      },
      {
        "avg_logprob": -0.21483184151027512,
        "compression_ratio": 1.587719298245614,
        "end": 512.38,
        "id": 177,
        "no_speech_prob": 0.000002295919102834887,
        "seek": 48732,
        "start": 507.38,
        "temperature": 0,
        "text": " That's my label, but I need to convert that to one of these.",
        "tokens": [
          51367,
          663,
          311,
          452,
          7645,
          11,
          457,
          286,
          643,
          281,
          7620,
          300,
          281,
          472,
          295,
          613,
          13,
          51617
        ]
      },
      {
        "avg_logprob": -0.21483184151027512,
        "compression_ratio": 1.587719298245614,
        "end": 515.24,
        "id": 178,
        "no_speech_prob": 0.000002295919102834887,
        "seek": 48732,
        "start": 512.64,
        "temperature": 0,
        "text": " So zero means reddish, one means greenish,",
        "tokens": [
          51630,
          407,
          4018,
          1355,
          2182,
          40974,
          11,
          472,
          1355,
          3092,
          742,
          11,
          51760
        ]
      },
      {
        "avg_logprob": -0.24182897143893772,
        "compression_ratio": 1.5857142857142856,
        "end": 517.92,
        "id": 179,
        "no_speech_prob": 0.000012219073141750414,
        "seek": 51524,
        "start": 515.24,
        "temperature": 0,
        "text": " two means bluish, three means orangish.",
        "tokens": [
          50364,
          732,
          1355,
          888,
          33786,
          11,
          1045,
          1355,
          17481,
          742,
          13,
          50498
        ]
      },
      {
        "avg_logprob": -0.24182897143893772,
        "compression_ratio": 1.5857142857142856,
        "end": 520.74,
        "id": 180,
        "no_speech_prob": 0.000012219073141750414,
        "seek": 51524,
        "start": 517.92,
        "temperature": 0,
        "text": " So I have this label list already.",
        "tokens": [
          50498,
          407,
          286,
          362,
          341,
          7645,
          1329,
          1217,
          13,
          50639
        ]
      },
      {
        "avg_logprob": -0.24182897143893772,
        "compression_ratio": 1.5857142857142856,
        "end": 524.44,
        "id": 181,
        "no_speech_prob": 0.000012219073141750414,
        "seek": 51524,
        "start": 522.26,
        "temperature": 0,
        "text": " I should be able to just say,",
        "tokens": [
          50715,
          286,
          820,
          312,
          1075,
          281,
          445,
          584,
          11,
          50824
        ]
      },
      {
        "avg_logprob": -0.24182897143893772,
        "compression_ratio": 1.5857142857142856,
        "end": 528.44,
        "id": 182,
        "no_speech_prob": 0.000012219073141750414,
        "seek": 51524,
        "start": 524.44,
        "temperature": 0,
        "text": " let label equal labelListIndex.",
        "tokens": [
          50824,
          718,
          7645,
          2681,
          7645,
          43,
          468,
          21790,
          3121,
          13,
          51024
        ]
      },
      {
        "avg_logprob": -0.24182897143893772,
        "compression_ratio": 1.5857142857142856,
        "end": 533.86,
        "id": 183,
        "no_speech_prob": 0.000012219073141750414,
        "seek": 51524,
        "start": 531.6800000000001,
        "temperature": 0,
        "text": " The only thing is I can't do that",
        "tokens": [
          51186,
          440,
          787,
          551,
          307,
          286,
          393,
          380,
          360,
          300,
          51295
        ]
      },
      {
        "avg_logprob": -0.24182897143893772,
        "compression_ratio": 1.5857142857142856,
        "end": 535.84,
        "id": 184,
        "no_speech_prob": 0.000012219073141750414,
        "seek": 51524,
        "start": 533.86,
        "temperature": 0,
        "text": " because this is a tensor.",
        "tokens": [
          51295,
          570,
          341,
          307,
          257,
          40863,
          13,
          51394
        ]
      },
      {
        "avg_logprob": -0.24182897143893772,
        "compression_ratio": 1.5857142857142856,
        "end": 538.24,
        "id": 185,
        "no_speech_prob": 0.000012219073141750414,
        "seek": 51524,
        "start": 535.84,
        "temperature": 0,
        "text": " That's a tensor, and what I want,",
        "tokens": [
          51394,
          663,
          311,
          257,
          40863,
          11,
          293,
          437,
          286,
          528,
          11,
          51514
        ]
      },
      {
        "avg_logprob": -0.24182897143893772,
        "compression_ratio": 1.5857142857142856,
        "end": 539.48,
        "id": 186,
        "no_speech_prob": 0.000012219073141750414,
        "seek": 51524,
        "start": 538.24,
        "temperature": 0,
        "text": " I need to pull that.",
        "tokens": [
          51514,
          286,
          643,
          281,
          2235,
          300,
          13,
          51576
        ]
      },
      {
        "avg_logprob": -0.24182897143893772,
        "compression_ratio": 1.5857142857142856,
        "end": 542.7,
        "id": 187,
        "no_speech_prob": 0.000012219073141750414,
        "seek": 51524,
        "start": 539.48,
        "temperature": 0,
        "text": " The tensor is the numbers, the data that lives on the GPU,",
        "tokens": [
          51576,
          440,
          40863,
          307,
          264,
          3547,
          11,
          264,
          1412,
          300,
          2909,
          322,
          264,
          18407,
          11,
          51737
        ]
      },
      {
        "avg_logprob": -0.24182897143893772,
        "compression_ratio": 1.5857142857142856,
        "end": 544.16,
        "id": 188,
        "no_speech_prob": 0.000012219073141750414,
        "seek": 51524,
        "start": 542.7,
        "temperature": 0,
        "text": " the WebGL fancy thing,",
        "tokens": [
          51737,
          264,
          9573,
          19440,
          10247,
          551,
          11,
          51810
        ]
      },
      {
        "avg_logprob": -0.26374951171875,
        "compression_ratio": 1.7160493827160495,
        "end": 545.92,
        "id": 189,
        "no_speech_prob": 0.00003647848279797472,
        "seek": 54416,
        "start": 544.16,
        "temperature": 0,
        "text": " that TensorFlow.js has implemented.",
        "tokens": [
          50364,
          300,
          37624,
          13,
          25530,
          575,
          12270,
          13,
          50452
        ]
      },
      {
        "avg_logprob": -0.26374951171875,
        "compression_ratio": 1.7160493827160495,
        "end": 547.6,
        "id": 190,
        "no_speech_prob": 0.00003647848279797472,
        "seek": 54416,
        "start": 545.92,
        "temperature": 0,
        "text": " I need to pull that off.",
        "tokens": [
          50452,
          286,
          643,
          281,
          2235,
          300,
          766,
          13,
          50536
        ]
      },
      {
        "avg_logprob": -0.26374951171875,
        "compression_ratio": 1.7160493827160495,
        "end": 549.36,
        "id": 191,
        "no_speech_prob": 0.00003647848279797472,
        "seek": 54416,
        "start": 547.6,
        "temperature": 0,
        "text": " And normally I would pull that off",
        "tokens": [
          50536,
          400,
          5646,
          286,
          576,
          2235,
          300,
          766,
          50624
        ]
      },
      {
        "avg_logprob": -0.26374951171875,
        "compression_ratio": 1.7160493827160495,
        "end": 551.4,
        "id": 192,
        "no_speech_prob": 0.00003647848279797472,
        "seek": 54416,
        "start": 549.36,
        "temperature": 0,
        "text": " with an asynchronous function,",
        "tokens": [
          50624,
          365,
          364,
          49174,
          2445,
          11,
          50726
        ]
      },
      {
        "avg_logprob": -0.26374951171875,
        "compression_ratio": 1.7160493827160495,
        "end": 553.9599999999999,
        "id": 193,
        "no_speech_prob": 0.00003647848279797472,
        "seek": 54416,
        "start": 551.4,
        "temperature": 0,
        "text": " but the thing is here,",
        "tokens": [
          50726,
          457,
          264,
          551,
          307,
          510,
          11,
          50854
        ]
      },
      {
        "avg_logprob": -0.26374951171875,
        "compression_ratio": 1.7160493827160495,
        "end": 556,
        "id": 194,
        "no_speech_prob": 0.00003647848279797472,
        "seek": 54416,
        "start": 553.9599999999999,
        "temperature": 0,
        "text": " it's such a little tiny bit of data,",
        "tokens": [
          50854,
          309,
          311,
          1270,
          257,
          707,
          5870,
          857,
          295,
          1412,
          11,
          50956
        ]
      },
      {
        "avg_logprob": -0.26374951171875,
        "compression_ratio": 1.7160493827160495,
        "end": 558,
        "id": 195,
        "no_speech_prob": 0.00003647848279797472,
        "seek": 54416,
        "start": 556,
        "temperature": 0,
        "text": " I think I can pull it off synchronously",
        "tokens": [
          50956,
          286,
          519,
          286,
          393,
          2235,
          309,
          766,
          19331,
          5098,
          51056
        ]
      },
      {
        "avg_logprob": -0.26374951171875,
        "compression_ratio": 1.7160493827160495,
        "end": 561.16,
        "id": 196,
        "no_speech_prob": 0.00003647848279797472,
        "seek": 54416,
        "start": 558,
        "temperature": 0,
        "text": " and not slow down my program from running.",
        "tokens": [
          51056,
          293,
          406,
          2964,
          760,
          452,
          1461,
          490,
          2614,
          13,
          51214
        ]
      },
      {
        "avg_logprob": -0.26374951171875,
        "compression_ratio": 1.7160493827160495,
        "end": 564.24,
        "id": 197,
        "no_speech_prob": 0.00003647848279797472,
        "seek": 54416,
        "start": 561.16,
        "temperature": 0,
        "text": " So actually what I want to say here is data sync,",
        "tokens": [
          51214,
          407,
          767,
          437,
          286,
          528,
          281,
          584,
          510,
          307,
          1412,
          20271,
          11,
          51368
        ]
      },
      {
        "avg_logprob": -0.26374951171875,
        "compression_ratio": 1.7160493827160495,
        "end": 567.0799999999999,
        "id": 198,
        "no_speech_prob": 0.00003647848279797472,
        "seek": 54416,
        "start": 565.52,
        "temperature": 0,
        "text": " and then, which is a,",
        "tokens": [
          51432,
          293,
          550,
          11,
          597,
          307,
          257,
          11,
          51510
        ]
      },
      {
        "avg_logprob": -0.26374951171875,
        "compression_ratio": 1.7160493827160495,
        "end": 569.9599999999999,
        "id": 199,
        "no_speech_prob": 0.00003647848279797472,
        "seek": 54416,
        "start": 567.0799999999999,
        "temperature": 0,
        "text": " dot data would pull it off asynchronously.",
        "tokens": [
          51510,
          5893,
          1412,
          576,
          2235,
          309,
          766,
          42642,
          5098,
          13,
          51654
        ]
      },
      {
        "avg_logprob": -0.26374951171875,
        "compression_ratio": 1.7160493827160495,
        "end": 573.24,
        "id": 200,
        "no_speech_prob": 0.00003647848279797472,
        "seek": 54416,
        "start": 569.9599999999999,
        "temperature": 0,
        "text": " So let's look at, and let's say,",
        "tokens": [
          51654,
          407,
          718,
          311,
          574,
          412,
          11,
          293,
          718,
          311,
          584,
          11,
          51818
        ]
      },
      {
        "avg_logprob": -0.22927259742666822,
        "compression_ratio": 1.5951219512195123,
        "end": 576.44,
        "id": 201,
        "no_speech_prob": 0.000002813014816638315,
        "seek": 57324,
        "start": 573.24,
        "temperature": 0,
        "text": " I'm going to console.log index,",
        "tokens": [
          50364,
          286,
          478,
          516,
          281,
          11076,
          13,
          4987,
          8186,
          11,
          50524
        ]
      },
      {
        "avg_logprob": -0.22927259742666822,
        "compression_ratio": 1.5951219512195123,
        "end": 578.2,
        "id": 202,
        "no_speech_prob": 0.000002813014816638315,
        "seek": 57324,
        "start": 576.44,
        "temperature": 0,
        "text": " and let me get rid of my other console logs",
        "tokens": [
          50524,
          293,
          718,
          385,
          483,
          3973,
          295,
          452,
          661,
          11076,
          20820,
          50612
        ]
      },
      {
        "avg_logprob": -0.22927259742666822,
        "compression_ratio": 1.5951219512195123,
        "end": 581.84,
        "id": 203,
        "no_speech_prob": 0.000002813014816638315,
        "seek": 57324,
        "start": 578.2,
        "temperature": 0,
        "text": " that I don't really want to look at right now.",
        "tokens": [
          50612,
          300,
          286,
          500,
          380,
          534,
          528,
          281,
          574,
          412,
          558,
          586,
          13,
          50794
        ]
      },
      {
        "avg_logprob": -0.22927259742666822,
        "compression_ratio": 1.5951219512195123,
        "end": 586.62,
        "id": 204,
        "no_speech_prob": 0.000002813014816638315,
        "seek": 57324,
        "start": 585.6,
        "temperature": 0,
        "text": " So, oh, okay.",
        "tokens": [
          50982,
          407,
          11,
          1954,
          11,
          1392,
          13,
          51033
        ]
      },
      {
        "avg_logprob": -0.22927259742666822,
        "compression_ratio": 1.5951219512195123,
        "end": 590,
        "id": 205,
        "no_speech_prob": 0.000002813014816638315,
        "seek": 57324,
        "start": 586.62,
        "temperature": 0,
        "text": " So I got an array with the number in it.",
        "tokens": [
          51033,
          407,
          286,
          658,
          364,
          10225,
          365,
          264,
          1230,
          294,
          309,
          13,
          51202
        ]
      },
      {
        "avg_logprob": -0.22927259742666822,
        "compression_ratio": 1.5951219512195123,
        "end": 591.44,
        "id": 206,
        "no_speech_prob": 0.000002813014816638315,
        "seek": 57324,
        "start": 590,
        "temperature": 0,
        "text": " I pulled it off,",
        "tokens": [
          51202,
          286,
          7373,
          309,
          766,
          11,
          51274
        ]
      },
      {
        "avg_logprob": -0.22927259742666822,
        "compression_ratio": 1.5951219512195123,
        "end": 595.8,
        "id": 207,
        "no_speech_prob": 0.000002813014816638315,
        "seek": 57324,
        "start": 591.44,
        "temperature": 0,
        "text": " and so then I just want to say index zero.",
        "tokens": [
          51274,
          293,
          370,
          550,
          286,
          445,
          528,
          281,
          584,
          8186,
          4018,
          13,
          51492
        ]
      },
      {
        "avg_logprob": -0.22927259742666822,
        "compression_ratio": 1.5951219512195123,
        "end": 598.92,
        "id": 208,
        "no_speech_prob": 0.000002813014816638315,
        "seek": 57324,
        "start": 595.8,
        "temperature": 0,
        "text": " So I only need that first value, index zero.",
        "tokens": [
          51492,
          407,
          286,
          787,
          643,
          300,
          700,
          2158,
          11,
          8186,
          4018,
          13,
          51648
        ]
      },
      {
        "avg_logprob": -0.22927259742666822,
        "compression_ratio": 1.5951219512195123,
        "end": 601.84,
        "id": 209,
        "no_speech_prob": 0.000002813014816638315,
        "seek": 57324,
        "start": 600.88,
        "temperature": 0,
        "text": " And so there we go.",
        "tokens": [
          51746,
          400,
          370,
          456,
          321,
          352,
          13,
          51794
        ]
      },
      {
        "avg_logprob": -0.22927259742666822,
        "compression_ratio": 1.5951219512195123,
        "end": 603.04,
        "id": 210,
        "no_speech_prob": 0.000002813014816638315,
        "seek": 57324,
        "start": 601.84,
        "temperature": 0,
        "text": " That's the label number.",
        "tokens": [
          51794,
          663,
          311,
          264,
          7645,
          1230,
          13,
          51854
        ]
      },
      {
        "avg_logprob": -0.26874817952071084,
        "compression_ratio": 1.660919540229885,
        "end": 604.68,
        "id": 211,
        "no_speech_prob": 0.0000016536891962459777,
        "seek": 60304,
        "start": 603.8399999999999,
        "temperature": 0,
        "text": " We now have the label number,",
        "tokens": [
          50404,
          492,
          586,
          362,
          264,
          7645,
          1230,
          11,
          50446
        ]
      },
      {
        "avg_logprob": -0.26874817952071084,
        "compression_ratio": 1.660919540229885,
        "end": 606.9599999999999,
        "id": 212,
        "no_speech_prob": 0.0000016536891962459777,
        "seek": 60304,
        "start": 604.68,
        "temperature": 0,
        "text": " and so now I can say this,",
        "tokens": [
          50446,
          293,
          370,
          586,
          286,
          393,
          584,
          341,
          11,
          50560
        ]
      },
      {
        "avg_logprob": -0.26874817952071084,
        "compression_ratio": 1.660919540229885,
        "end": 609.56,
        "id": 213,
        "no_speech_prob": 0.0000016536891962459777,
        "seek": 60304,
        "start": 608.5999999999999,
        "temperature": 0,
        "text": " and I can say,",
        "tokens": [
          50642,
          293,
          286,
          393,
          584,
          11,
          50690
        ]
      },
      {
        "avg_logprob": -0.26874817952071084,
        "compression_ratio": 1.660919540229885,
        "end": 611.56,
        "id": 214,
        "no_speech_prob": 0.0000016536891962459777,
        "seek": 60304,
        "start": 609.56,
        "temperature": 0,
        "text": " and let's put it on a paragraph element.",
        "tokens": [
          50690,
          293,
          718,
          311,
          829,
          309,
          322,
          257,
          18865,
          4478,
          13,
          50790
        ]
      },
      {
        "avg_logprob": -0.26874817952071084,
        "compression_ratio": 1.660919540229885,
        "end": 614.68,
        "id": 215,
        "no_speech_prob": 0.0000016536891962459777,
        "seek": 60304,
        "start": 611.56,
        "temperature": 0,
        "text": " So let's say let label p.",
        "tokens": [
          50790,
          407,
          718,
          311,
          584,
          718,
          7645,
          280,
          13,
          50946
        ]
      },
      {
        "avg_logprob": -0.26874817952071084,
        "compression_ratio": 1.660919540229885,
        "end": 617.56,
        "id": 216,
        "no_speech_prob": 0.0000016536891962459777,
        "seek": 60304,
        "start": 615.9599999999999,
        "temperature": 0,
        "text": " Let's have that be first.",
        "tokens": [
          51010,
          961,
          311,
          362,
          300,
          312,
          700,
          13,
          51090
        ]
      },
      {
        "avg_logprob": -0.26874817952071084,
        "compression_ratio": 1.660919540229885,
        "end": 622.9399999999999,
        "id": 217,
        "no_speech_prob": 0.0000016536891962459777,
        "seek": 60304,
        "start": 618.4399999999999,
        "temperature": 0,
        "text": " And so now I want to say label p equals create p.",
        "tokens": [
          51134,
          400,
          370,
          586,
          286,
          528,
          281,
          584,
          7645,
          280,
          6915,
          1884,
          280,
          13,
          51359
        ]
      },
      {
        "avg_logprob": -0.26874817952071084,
        "compression_ratio": 1.660919540229885,
        "end": 626.9599999999999,
        "id": 218,
        "no_speech_prob": 0.0000016536891962459777,
        "seek": 60304,
        "start": 624.38,
        "temperature": 0,
        "text": " And then I should say,",
        "tokens": [
          51431,
          400,
          550,
          286,
          820,
          584,
          11,
          51560
        ]
      },
      {
        "avg_logprob": -0.26874817952071084,
        "compression_ratio": 1.660919540229885,
        "end": 628.88,
        "id": 219,
        "no_speech_prob": 0.0000016536891962459777,
        "seek": 60304,
        "start": 626.9599999999999,
        "temperature": 0,
        "text": " all the way back down here,",
        "tokens": [
          51560,
          439,
          264,
          636,
          646,
          760,
          510,
          11,
          51656
        ]
      },
      {
        "avg_logprob": -0.26874817952071084,
        "compression_ratio": 1.660919540229885,
        "end": 631.8,
        "id": 220,
        "no_speech_prob": 0.0000016536891962459777,
        "seek": 60304,
        "start": 628.88,
        "temperature": 0,
        "text": " label p dot html label.",
        "tokens": [
          51656,
          7645,
          280,
          5893,
          276,
          83,
          15480,
          7645,
          13,
          51802
        ]
      },
      {
        "avg_logprob": -0.1954405042860243,
        "compression_ratio": 1.5843621399176955,
        "end": 633.4399999999999,
        "id": 221,
        "no_speech_prob": 0.00001750290539348498,
        "seek": 63180,
        "start": 631.8,
        "temperature": 0,
        "text": " Okay, ready for this?",
        "tokens": [
          50364,
          1033,
          11,
          1919,
          337,
          341,
          30,
          50446
        ]
      },
      {
        "avg_logprob": -0.1954405042860243,
        "compression_ratio": 1.5843621399176955,
        "end": 634.68,
        "id": 222,
        "no_speech_prob": 0.00001750290539348498,
        "seek": 63180,
        "start": 633.4399999999999,
        "temperature": 0,
        "text": " Here we go.",
        "tokens": [
          50446,
          1692,
          321,
          352,
          13,
          50508
        ]
      },
      {
        "avg_logprob": -0.1954405042860243,
        "compression_ratio": 1.5843621399176955,
        "end": 636.28,
        "id": 223,
        "no_speech_prob": 0.00001750290539348498,
        "seek": 63180,
        "start": 634.68,
        "temperature": 0,
        "text": " I've started my training.",
        "tokens": [
          50508,
          286,
          600,
          1409,
          452,
          3097,
          13,
          50588
        ]
      },
      {
        "avg_logprob": -0.1954405042860243,
        "compression_ratio": 1.5843621399176955,
        "end": 637.56,
        "id": 224,
        "no_speech_prob": 0.00001750290539348498,
        "seek": 63180,
        "start": 636.28,
        "temperature": 0,
        "text": " Oh, wait, why?",
        "tokens": [
          50588,
          876,
          11,
          1699,
          11,
          983,
          30,
          50652
        ]
      },
      {
        "avg_logprob": -0.1954405042860243,
        "compression_ratio": 1.5843621399176955,
        "end": 638.4,
        "id": 225,
        "no_speech_prob": 0.00001750290539348498,
        "seek": 63180,
        "start": 637.56,
        "temperature": 0,
        "text": " This is so silly,",
        "tokens": [
          50652,
          639,
          307,
          370,
          11774,
          11,
          50694
        ]
      },
      {
        "avg_logprob": -0.1954405042860243,
        "compression_ratio": 1.5843621399176955,
        "end": 641.16,
        "id": 226,
        "no_speech_prob": 0.00001750290539348498,
        "seek": 63180,
        "start": 638.4,
        "temperature": 0,
        "text": " but I want the label to be above it.",
        "tokens": [
          50694,
          457,
          286,
          528,
          264,
          7645,
          281,
          312,
          3673,
          309,
          13,
          50832
        ]
      },
      {
        "avg_logprob": -0.1954405042860243,
        "compression_ratio": 1.5843621399176955,
        "end": 643.52,
        "id": 227,
        "no_speech_prob": 0.00001750290539348498,
        "seek": 63180,
        "start": 641.16,
        "temperature": 0,
        "text": " I really should not be changing this right now.",
        "tokens": [
          50832,
          286,
          534,
          820,
          406,
          312,
          4473,
          341,
          558,
          586,
          13,
          50950
        ]
      },
      {
        "avg_logprob": -0.1954405042860243,
        "compression_ratio": 1.5843621399176955,
        "end": 646.68,
        "id": 228,
        "no_speech_prob": 0.00001750290539348498,
        "seek": 63180,
        "start": 645.3199999999999,
        "temperature": 0,
        "text": " So let me just put it here.",
        "tokens": [
          51040,
          407,
          718,
          385,
          445,
          829,
          309,
          510,
          13,
          51108
        ]
      },
      {
        "avg_logprob": -0.1954405042860243,
        "compression_ratio": 1.5843621399176955,
        "end": 651.9799999999999,
        "id": 229,
        "no_speech_prob": 0.00001750290539348498,
        "seek": 63180,
        "start": 649.8399999999999,
        "temperature": 0,
        "text": " Okay, so here we go.",
        "tokens": [
          51266,
          1033,
          11,
          370,
          510,
          321,
          352,
          13,
          51373
        ]
      },
      {
        "avg_logprob": -0.1954405042860243,
        "compression_ratio": 1.5843621399176955,
        "end": 653.8399999999999,
        "id": 230,
        "no_speech_prob": 0.00001750290539348498,
        "seek": 63180,
        "start": 651.9799999999999,
        "temperature": 0,
        "text": " It thinks that's greenish, right?",
        "tokens": [
          51373,
          467,
          7309,
          300,
          311,
          3092,
          742,
          11,
          558,
          30,
          51466
        ]
      },
      {
        "avg_logprob": -0.1954405042860243,
        "compression_ratio": 1.5843621399176955,
        "end": 656.12,
        "id": 231,
        "no_speech_prob": 0.00001750290539348498,
        "seek": 63180,
        "start": 653.8399999999999,
        "temperature": 0,
        "text": " Well, it hasn't gotten very far with the training.",
        "tokens": [
          51466,
          1042,
          11,
          309,
          6132,
          380,
          5768,
          588,
          1400,
          365,
          264,
          3097,
          13,
          51580
        ]
      },
      {
        "avg_logprob": -0.1954405042860243,
        "compression_ratio": 1.5843621399176955,
        "end": 658.3599999999999,
        "id": 232,
        "no_speech_prob": 0.00001750290539348498,
        "seek": 63180,
        "start": 656.12,
        "temperature": 0,
        "text": " I would imagine that once we train further",
        "tokens": [
          51580,
          286,
          576,
          3811,
          300,
          1564,
          321,
          3847,
          3052,
          51692
        ]
      },
      {
        "avg_logprob": -0.1954405042860243,
        "compression_ratio": 1.5843621399176955,
        "end": 660.0799999999999,
        "id": 233,
        "no_speech_prob": 0.00001750290539348498,
        "seek": 63180,
        "start": 658.3599999999999,
        "temperature": 0,
        "text": " and the law starts going down,",
        "tokens": [
          51692,
          293,
          264,
          2101,
          3719,
          516,
          760,
          11,
          51778
        ]
      },
      {
        "avg_logprob": -0.22175007240445005,
        "compression_ratio": 1.6744186046511629,
        "end": 662.5400000000001,
        "id": 234,
        "no_speech_prob": 0.0000023320735635934398,
        "seek": 66008,
        "start": 660.08,
        "temperature": 0,
        "text": " it's going to recognize that as yellowish.",
        "tokens": [
          50364,
          309,
          311,
          516,
          281,
          5521,
          300,
          382,
          5566,
          742,
          13,
          50487
        ]
      },
      {
        "avg_logprob": -0.22175007240445005,
        "compression_ratio": 1.6744186046511629,
        "end": 664.76,
        "id": 235,
        "no_speech_prob": 0.0000023320735635934398,
        "seek": 66008,
        "start": 662.5400000000001,
        "temperature": 0,
        "text": " So here, I'm going to just wait a little bit,",
        "tokens": [
          50487,
          407,
          510,
          11,
          286,
          478,
          516,
          281,
          445,
          1699,
          257,
          707,
          857,
          11,
          50598
        ]
      },
      {
        "avg_logprob": -0.22175007240445005,
        "compression_ratio": 1.6744186046511629,
        "end": 666.5600000000001,
        "id": 236,
        "no_speech_prob": 0.0000023320735635934398,
        "seek": 66008,
        "start": 664.76,
        "temperature": 0,
        "text": " and I'll be back in a minute.",
        "tokens": [
          50598,
          293,
          286,
          603,
          312,
          646,
          294,
          257,
          3456,
          13,
          50688
        ]
      },
      {
        "avg_logprob": -0.22175007240445005,
        "compression_ratio": 1.6744186046511629,
        "end": 668.5200000000001,
        "id": 237,
        "no_speech_prob": 0.0000023320735635934398,
        "seek": 66008,
        "start": 666.5600000000001,
        "temperature": 0,
        "text": " All right, I'm back.",
        "tokens": [
          50688,
          220,
          7868,
          558,
          11,
          286,
          478,
          646,
          13,
          50786
        ]
      },
      {
        "avg_logprob": -0.22175007240445005,
        "compression_ratio": 1.6744186046511629,
        "end": 674.08,
        "id": 238,
        "no_speech_prob": 0.0000023320735635934398,
        "seek": 66008,
        "start": 672.1,
        "temperature": 0,
        "text": " So it trained over 10 epochs,",
        "tokens": [
          50965,
          407,
          309,
          8895,
          670,
          1266,
          30992,
          28346,
          11,
          51064
        ]
      },
      {
        "avg_logprob": -0.22175007240445005,
        "compression_ratio": 1.6744186046511629,
        "end": 676.58,
        "id": 239,
        "no_speech_prob": 0.0000023320735635934398,
        "seek": 66008,
        "start": 674.08,
        "temperature": 0,
        "text": " and you can see now it's saying this is yellowish.",
        "tokens": [
          51064,
          293,
          291,
          393,
          536,
          586,
          309,
          311,
          1566,
          341,
          307,
          5566,
          742,
          13,
          51189
        ]
      },
      {
        "avg_logprob": -0.22175007240445005,
        "compression_ratio": 1.6744186046511629,
        "end": 677.72,
        "id": 240,
        "no_speech_prob": 0.0000023320735635934398,
        "seek": 66008,
        "start": 676.58,
        "temperature": 0,
        "text": " Let me tune this down.",
        "tokens": [
          51189,
          961,
          385,
          10864,
          341,
          760,
          13,
          51246
        ]
      },
      {
        "avg_logprob": -0.22175007240445005,
        "compression_ratio": 1.6744186046511629,
        "end": 678.9200000000001,
        "id": 241,
        "no_speech_prob": 0.0000023320735635934398,
        "seek": 66008,
        "start": 677.72,
        "temperature": 0,
        "text": " That's greenish.",
        "tokens": [
          51246,
          663,
          311,
          3092,
          742,
          13,
          51306
        ]
      },
      {
        "avg_logprob": -0.22175007240445005,
        "compression_ratio": 1.6744186046511629,
        "end": 679.88,
        "id": 242,
        "no_speech_prob": 0.0000023320735635934398,
        "seek": 66008,
        "start": 678.9200000000001,
        "temperature": 0,
        "text": " Turn this up.",
        "tokens": [
          51306,
          7956,
          341,
          493,
          13,
          51354
        ]
      },
      {
        "avg_logprob": -0.22175007240445005,
        "compression_ratio": 1.6744186046511629,
        "end": 681.0600000000001,
        "id": 243,
        "no_speech_prob": 0.0000023320735635934398,
        "seek": 66008,
        "start": 679.88,
        "temperature": 0,
        "text": " That's bluish.",
        "tokens": [
          51354,
          663,
          311,
          888,
          33786,
          13,
          51413
        ]
      },
      {
        "avg_logprob": -0.22175007240445005,
        "compression_ratio": 1.6744186046511629,
        "end": 682.36,
        "id": 244,
        "no_speech_prob": 0.0000023320735635934398,
        "seek": 66008,
        "start": 681.0600000000001,
        "temperature": 0,
        "text": " We still got bluish.",
        "tokens": [
          51413,
          492,
          920,
          658,
          888,
          33786,
          13,
          51478
        ]
      },
      {
        "avg_logprob": -0.22175007240445005,
        "compression_ratio": 1.6744186046511629,
        "end": 684.0400000000001,
        "id": 245,
        "no_speech_prob": 0.0000023320735635934398,
        "seek": 66008,
        "start": 682.36,
        "temperature": 0,
        "text": " Can we get some purple?",
        "tokens": [
          51478,
          1664,
          321,
          483,
          512,
          9656,
          30,
          51562
        ]
      },
      {
        "avg_logprob": -0.22175007240445005,
        "compression_ratio": 1.6744186046511629,
        "end": 685.0400000000001,
        "id": 246,
        "no_speech_prob": 0.0000023320735635934398,
        "seek": 66008,
        "start": 684.0400000000001,
        "temperature": 0,
        "text": " Purpleish.",
        "tokens": [
          51562,
          28483,
          742,
          13,
          51612
        ]
      },
      {
        "avg_logprob": -0.22175007240445005,
        "compression_ratio": 1.6744186046511629,
        "end": 686.2800000000001,
        "id": 247,
        "no_speech_prob": 0.0000023320735635934398,
        "seek": 66008,
        "start": 685.0400000000001,
        "temperature": 0,
        "text": " Can we get some pink?",
        "tokens": [
          51612,
          1664,
          321,
          483,
          512,
          7022,
          30,
          51674
        ]
      },
      {
        "avg_logprob": -0.22175007240445005,
        "compression_ratio": 1.6744186046511629,
        "end": 687.7800000000001,
        "id": 248,
        "no_speech_prob": 0.0000023320735635934398,
        "seek": 66008,
        "start": 686.2800000000001,
        "temperature": 0,
        "text": " Oh, it didn't get pink.",
        "tokens": [
          51674,
          876,
          11,
          309,
          994,
          380,
          483,
          7022,
          13,
          51749
        ]
      },
      {
        "avg_logprob": -0.22175007240445005,
        "compression_ratio": 1.6744186046511629,
        "end": 689.5400000000001,
        "id": 249,
        "no_speech_prob": 0.0000023320735635934398,
        "seek": 66008,
        "start": 687.7800000000001,
        "temperature": 0,
        "text": " Maybe if I add a little more brightness,",
        "tokens": [
          51749,
          2704,
          498,
          286,
          909,
          257,
          707,
          544,
          21367,
          11,
          51837
        ]
      },
      {
        "avg_logprob": -0.2088520681257728,
        "compression_ratio": 1.6937269372693726,
        "end": 691.5799999999999,
        "id": 250,
        "no_speech_prob": 0.000020462917746044695,
        "seek": 68954,
        "start": 689.9399999999999,
        "temperature": 0,
        "text": " ah, now it thinks that's pink.",
        "tokens": [
          50384,
          3716,
          11,
          586,
          309,
          7309,
          300,
          311,
          7022,
          13,
          50466
        ]
      },
      {
        "avg_logprob": -0.2088520681257728,
        "compression_ratio": 1.6937269372693726,
        "end": 695.14,
        "id": 251,
        "no_speech_prob": 0.000020462917746044695,
        "seek": 68954,
        "start": 691.5799999999999,
        "temperature": 0,
        "text": " So I have now trained the neural network to recognize,",
        "tokens": [
          50466,
          407,
          286,
          362,
          586,
          8895,
          264,
          18161,
          3209,
          281,
          5521,
          11,
          50644
        ]
      },
      {
        "avg_logprob": -0.2088520681257728,
        "compression_ratio": 1.6937269372693726,
        "end": 698.06,
        "id": 252,
        "no_speech_prob": 0.000020462917746044695,
        "seek": 68954,
        "start": 695.14,
        "temperature": 0,
        "text": " and let's see if it can get red, reddish.",
        "tokens": [
          50644,
          293,
          718,
          311,
          536,
          498,
          309,
          393,
          483,
          2182,
          11,
          2182,
          40974,
          13,
          50790
        ]
      },
      {
        "avg_logprob": -0.2088520681257728,
        "compression_ratio": 1.6937269372693726,
        "end": 700.62,
        "id": 253,
        "no_speech_prob": 0.000020462917746044695,
        "seek": 68954,
        "start": 698.06,
        "temperature": 0,
        "text": " So we can see I can play with this all day long.",
        "tokens": [
          50790,
          407,
          321,
          393,
          536,
          286,
          393,
          862,
          365,
          341,
          439,
          786,
          938,
          13,
          50918
        ]
      },
      {
        "avg_logprob": -0.2088520681257728,
        "compression_ratio": 1.6937269372693726,
        "end": 703.14,
        "id": 254,
        "no_speech_prob": 0.000020462917746044695,
        "seek": 68954,
        "start": 700.62,
        "temperature": 0,
        "text": " This is now going to classify the color",
        "tokens": [
          50918,
          639,
          307,
          586,
          516,
          281,
          33872,
          264,
          2017,
          51044
        ]
      },
      {
        "avg_logprob": -0.2088520681257728,
        "compression_ratio": 1.6937269372693726,
        "end": 704.6999999999999,
        "id": 255,
        "no_speech_prob": 0.000020462917746044695,
        "seek": 68954,
        "start": 703.14,
        "temperature": 0,
        "text": " based on that particular model.",
        "tokens": [
          51044,
          2361,
          322,
          300,
          1729,
          2316,
          13,
          51122
        ]
      },
      {
        "avg_logprob": -0.2088520681257728,
        "compression_ratio": 1.6937269372693726,
        "end": 706.74,
        "id": 256,
        "no_speech_prob": 0.000020462917746044695,
        "seek": 68954,
        "start": 704.6999999999999,
        "temperature": 0,
        "text": " So in a way, I'm done.",
        "tokens": [
          51122,
          407,
          294,
          257,
          636,
          11,
          286,
          478,
          1096,
          13,
          51224
        ]
      },
      {
        "avg_logprob": -0.2088520681257728,
        "compression_ratio": 1.6937269372693726,
        "end": 710.06,
        "id": 257,
        "no_speech_prob": 0.000020462917746044695,
        "seek": 68954,
        "start": 706.74,
        "temperature": 0,
        "text": " I probably want to train it for more epochs.",
        "tokens": [
          51224,
          286,
          1391,
          528,
          281,
          3847,
          309,
          337,
          544,
          30992,
          28346,
          13,
          51390
        ]
      },
      {
        "avg_logprob": -0.2088520681257728,
        "compression_ratio": 1.6937269372693726,
        "end": 711.38,
        "id": 258,
        "no_speech_prob": 0.000020462917746044695,
        "seek": 68954,
        "start": 710.06,
        "temperature": 0,
        "text": " What are some things that I want to do?",
        "tokens": [
          51390,
          708,
          366,
          512,
          721,
          300,
          286,
          528,
          281,
          360,
          30,
          51456
        ]
      },
      {
        "avg_logprob": -0.2088520681257728,
        "compression_ratio": 1.6937269372693726,
        "end": 715.78,
        "id": 259,
        "no_speech_prob": 0.000020462917746044695,
        "seek": 68954,
        "start": 711.38,
        "temperature": 0,
        "text": " So one is I would want to get more data.",
        "tokens": [
          51456,
          407,
          472,
          307,
          286,
          576,
          528,
          281,
          483,
          544,
          1412,
          13,
          51676
        ]
      },
      {
        "avg_logprob": -0.2088520681257728,
        "compression_ratio": 1.6937269372693726,
        "end": 717.0999999999999,
        "id": 260,
        "no_speech_prob": 0.000020462917746044695,
        "seek": 68954,
        "start": 715.78,
        "temperature": 0,
        "text": " I would want to be more thoughtful",
        "tokens": [
          51676,
          286,
          576,
          528,
          281,
          312,
          544,
          21566,
          51742
        ]
      },
      {
        "avg_logprob": -0.2088520681257728,
        "compression_ratio": 1.6937269372693726,
        "end": 718.62,
        "id": 261,
        "no_speech_prob": 0.000020462917746044695,
        "seek": 68954,
        "start": 717.0999999999999,
        "temperature": 0,
        "text": " about the validation data.",
        "tokens": [
          51742,
          466,
          264,
          24071,
          1412,
          13,
          51818
        ]
      },
      {
        "avg_logprob": -0.2132502071193007,
        "compression_ratio": 1.8508064516129032,
        "end": 720.34,
        "id": 262,
        "no_speech_prob": 0.000002726470256675384,
        "seek": 71862,
        "start": 718.62,
        "temperature": 0,
        "text": " And then the other thing I would want to start doing",
        "tokens": [
          50364,
          400,
          550,
          264,
          661,
          551,
          286,
          576,
          528,
          281,
          722,
          884,
          50450
        ]
      },
      {
        "avg_logprob": -0.2132502071193007,
        "compression_ratio": 1.8508064516129032,
        "end": 722.58,
        "id": 263,
        "no_speech_prob": 0.000002726470256675384,
        "seek": 71862,
        "start": 720.34,
        "temperature": 0,
        "text": " is thinking about, well, does it actually work better?",
        "tokens": [
          50450,
          307,
          1953,
          466,
          11,
          731,
          11,
          775,
          309,
          767,
          589,
          1101,
          30,
          50562
        ]
      },
      {
        "avg_logprob": -0.2132502071193007,
        "compression_ratio": 1.8508064516129032,
        "end": 725.7,
        "id": 264,
        "no_speech_prob": 0.000002726470256675384,
        "seek": 71862,
        "start": 722.58,
        "temperature": 0,
        "text": " What are the hyperparameters that I can play with?",
        "tokens": [
          50562,
          708,
          366,
          264,
          9848,
          2181,
          335,
          6202,
          300,
          286,
          393,
          862,
          365,
          30,
          50718
        ]
      },
      {
        "avg_logprob": -0.2132502071193007,
        "compression_ratio": 1.8508064516129032,
        "end": 730.78,
        "id": 265,
        "no_speech_prob": 0.000002726470256675384,
        "seek": 71862,
        "start": 725.7,
        "temperature": 0,
        "text": " For example, the hidden layer, I put 16 units in it.",
        "tokens": [
          50718,
          1171,
          1365,
          11,
          264,
          7633,
          4583,
          11,
          286,
          829,
          3165,
          6815,
          294,
          309,
          13,
          50972
        ]
      },
      {
        "avg_logprob": -0.2132502071193007,
        "compression_ratio": 1.8508064516129032,
        "end": 733.0600000000001,
        "id": 266,
        "no_speech_prob": 0.000002726470256675384,
        "seek": 71862,
        "start": 730.78,
        "temperature": 0,
        "text": " Or what happens if I use a different activation",
        "tokens": [
          50972,
          1610,
          437,
          2314,
          498,
          286,
          764,
          257,
          819,
          24433,
          51086
        ]
      },
      {
        "avg_logprob": -0.2132502071193007,
        "compression_ratio": 1.8508064516129032,
        "end": 734.38,
        "id": 267,
        "no_speech_prob": 0.000002726470256675384,
        "seek": 71862,
        "start": 733.0600000000001,
        "temperature": 0,
        "text": " function for the hidden layer?",
        "tokens": [
          51086,
          2445,
          337,
          264,
          7633,
          4583,
          30,
          51152
        ]
      },
      {
        "avg_logprob": -0.2132502071193007,
        "compression_ratio": 1.8508064516129032,
        "end": 736.7,
        "id": 268,
        "no_speech_prob": 0.000002726470256675384,
        "seek": 71862,
        "start": 734.38,
        "temperature": 0,
        "text": " What happens if I use more nodes or less nodes?",
        "tokens": [
          51152,
          708,
          2314,
          498,
          286,
          764,
          544,
          13891,
          420,
          1570,
          13891,
          30,
          51268
        ]
      },
      {
        "avg_logprob": -0.2132502071193007,
        "compression_ratio": 1.8508064516129032,
        "end": 738.5,
        "id": 269,
        "no_speech_prob": 0.000002726470256675384,
        "seek": 71862,
        "start": 736.7,
        "temperature": 0,
        "text": " What if I change the learning rate?",
        "tokens": [
          51268,
          708,
          498,
          286,
          1319,
          264,
          2539,
          3314,
          30,
          51358
        ]
      },
      {
        "avg_logprob": -0.2132502071193007,
        "compression_ratio": 1.8508064516129032,
        "end": 741.34,
        "id": 270,
        "no_speech_prob": 0.000002726470256675384,
        "seek": 71862,
        "start": 738.5,
        "temperature": 0,
        "text": " What if I change the optimization function,",
        "tokens": [
          51358,
          708,
          498,
          286,
          1319,
          264,
          19618,
          2445,
          11,
          51500
        ]
      },
      {
        "avg_logprob": -0.2132502071193007,
        "compression_ratio": 1.8508064516129032,
        "end": 744.54,
        "id": 271,
        "no_speech_prob": 0.000002726470256675384,
        "seek": 71862,
        "start": 741.34,
        "temperature": 0,
        "text": " if I use the atom optimization function?",
        "tokens": [
          51500,
          498,
          286,
          764,
          264,
          12018,
          19618,
          2445,
          30,
          51660
        ]
      },
      {
        "avg_logprob": -0.2343914337158203,
        "compression_ratio": 1.7586206896551724,
        "end": 750.42,
        "id": 272,
        "no_speech_prob": 0.000008398068530368619,
        "seek": 74454,
        "start": 744.54,
        "temperature": 0,
        "text": " So all these things are things that I could play with",
        "tokens": [
          50364,
          407,
          439,
          613,
          721,
          366,
          721,
          300,
          286,
          727,
          862,
          365,
          50658
        ]
      },
      {
        "avg_logprob": -0.2343914337158203,
        "compression_ratio": 1.7586206896551724,
        "end": 753.66,
        "id": 273,
        "no_speech_prob": 0.000008398068530368619,
        "seek": 74454,
        "start": 750.42,
        "temperature": 0,
        "text": " and research and think about and experiment with to try",
        "tokens": [
          50658,
          293,
          2132,
          293,
          519,
          466,
          293,
          5120,
          365,
          281,
          853,
          50820
        ]
      },
      {
        "avg_logprob": -0.2343914337158203,
        "compression_ratio": 1.7586206896551724,
        "end": 755.6999999999999,
        "id": 274,
        "no_speech_prob": 0.000008398068530368619,
        "seek": 74454,
        "start": 753.66,
        "temperature": 0,
        "text": " to tune the model really well.",
        "tokens": [
          50820,
          281,
          10864,
          264,
          2316,
          534,
          731,
          13,
          50922
        ]
      },
      {
        "avg_logprob": -0.2343914337158203,
        "compression_ratio": 1.7586206896551724,
        "end": 759.3399999999999,
        "id": 275,
        "no_speech_prob": 0.000008398068530368619,
        "seek": 74454,
        "start": 755.6999999999999,
        "temperature": 0,
        "text": " Then at some point, I also would want to save that model,",
        "tokens": [
          50922,
          1396,
          412,
          512,
          935,
          11,
          286,
          611,
          576,
          528,
          281,
          3155,
          300,
          2316,
          11,
          51104
        ]
      },
      {
        "avg_logprob": -0.2343914337158203,
        "compression_ratio": 1.7586206896551724,
        "end": 763.5799999999999,
        "id": 276,
        "no_speech_prob": 0.000008398068530368619,
        "seek": 74454,
        "start": 759.3399999999999,
        "temperature": 0,
        "text": " save it to a JSON file, so the trained model somehow,",
        "tokens": [
          51104,
          3155,
          309,
          281,
          257,
          31828,
          3991,
          11,
          370,
          264,
          8895,
          2316,
          6063,
          11,
          51316
        ]
      },
      {
        "avg_logprob": -0.2343914337158203,
        "compression_ratio": 1.7586206896551724,
        "end": 765.74,
        "id": 277,
        "no_speech_prob": 0.000008398068530368619,
        "seek": 74454,
        "start": 763.5799999999999,
        "temperature": 0,
        "text": " so that I could load it back in without having",
        "tokens": [
          51316,
          370,
          300,
          286,
          727,
          3677,
          309,
          646,
          294,
          1553,
          1419,
          51424
        ]
      },
      {
        "avg_logprob": -0.2343914337158203,
        "compression_ratio": 1.7586206896551724,
        "end": 767.54,
        "id": 278,
        "no_speech_prob": 0.000008398068530368619,
        "seek": 74454,
        "start": 765.74,
        "temperature": 0,
        "text": " to run through the training process again.",
        "tokens": [
          51424,
          281,
          1190,
          807,
          264,
          3097,
          1399,
          797,
          13,
          51514
        ]
      },
      {
        "avg_logprob": -0.2343914337158203,
        "compression_ratio": 1.7586206896551724,
        "end": 769.14,
        "id": 279,
        "no_speech_prob": 0.000008398068530368619,
        "seek": 74454,
        "start": 767.54,
        "temperature": 0,
        "text": " Maybe I'd even want a larger data set.",
        "tokens": [
          51514,
          2704,
          286,
          1116,
          754,
          528,
          257,
          4833,
          1412,
          992,
          13,
          51594
        ]
      },
      {
        "avg_logprob": -0.2343914337158203,
        "compression_ratio": 1.7586206896551724,
        "end": 770.74,
        "id": 280,
        "no_speech_prob": 0.000008398068530368619,
        "seek": 74454,
        "start": 769.14,
        "temperature": 0,
        "text": " I'd want to train it over a long time.",
        "tokens": [
          51594,
          286,
          1116,
          528,
          281,
          3847,
          309,
          670,
          257,
          938,
          565,
          13,
          51674
        ]
      },
      {
        "avg_logprob": -0.2343914337158203,
        "compression_ratio": 1.7586206896551724,
        "end": 772.8199999999999,
        "id": 281,
        "no_speech_prob": 0.000008398068530368619,
        "seek": 74454,
        "start": 770.74,
        "temperature": 0,
        "text": " Maybe I want to port this code to Node",
        "tokens": [
          51674,
          2704,
          286,
          528,
          281,
          2436,
          341,
          3089,
          281,
          38640,
          51778
        ]
      },
      {
        "avg_logprob": -0.27241279397692,
        "compression_ratio": 1.5365853658536586,
        "end": 775.0600000000001,
        "id": 282,
        "no_speech_prob": 0.00008888076263247058,
        "seek": 77282,
        "start": 772.82,
        "temperature": 0,
        "text": " so I could let it train server-side without having",
        "tokens": [
          50364,
          370,
          286,
          727,
          718,
          309,
          3847,
          7154,
          12,
          1812,
          1553,
          1419,
          50476
        ]
      },
      {
        "avg_logprob": -0.27241279397692,
        "compression_ratio": 1.5365853658536586,
        "end": 776.0600000000001,
        "id": 283,
        "no_speech_prob": 0.00008888076263247058,
        "seek": 77282,
        "start": 775.0600000000001,
        "temperature": 0,
        "text": " to train in the client.",
        "tokens": [
          50476,
          281,
          3847,
          294,
          264,
          6423,
          13,
          50526
        ]
      },
      {
        "avg_logprob": -0.27241279397692,
        "compression_ratio": 1.5365853658536586,
        "end": 777.6600000000001,
        "id": 284,
        "no_speech_prob": 0.00008888076263247058,
        "seek": 77282,
        "start": 776.0600000000001,
        "temperature": 0,
        "text": " There's so many possibilities.",
        "tokens": [
          50526,
          821,
          311,
          370,
          867,
          12178,
          13,
          50606
        ]
      },
      {
        "avg_logprob": -0.27241279397692,
        "compression_ratio": 1.5365853658536586,
        "end": 782.5,
        "id": 285,
        "no_speech_prob": 0.00008888076263247058,
        "seek": 77282,
        "start": 777.6600000000001,
        "temperature": 0,
        "text": " But I have now built a machine learning model",
        "tokens": [
          50606,
          583,
          286,
          362,
          586,
          3094,
          257,
          3479,
          2539,
          2316,
          50848
        ]
      },
      {
        "avg_logprob": -0.27241279397692,
        "compression_ratio": 1.5365853658536586,
        "end": 784.1400000000001,
        "id": 286,
        "no_speech_prob": 0.00008888076263247058,
        "seek": 77282,
        "start": 782.5,
        "temperature": 0,
        "text": " with TensorFlow.js.",
        "tokens": [
          50848,
          365,
          37624,
          13,
          25530,
          13,
          50930
        ]
      },
      {
        "avg_logprob": -0.27241279397692,
        "compression_ratio": 1.5365853658536586,
        "end": 786.22,
        "id": 287,
        "no_speech_prob": 0.00008888076263247058,
        "seek": 77282,
        "start": 784.1400000000001,
        "temperature": 0,
        "text": " I'm going to cry.",
        "tokens": [
          50930,
          286,
          478,
          516,
          281,
          3305,
          13,
          51034
        ]
      },
      {
        "avg_logprob": -0.27241279397692,
        "compression_ratio": 1.5365853658536586,
        "end": 788.4200000000001,
        "id": 288,
        "no_speech_prob": 0.00008888076263247058,
        "seek": 77282,
        "start": 786.22,
        "temperature": 0,
        "text": " I don't know how many videos this took.",
        "tokens": [
          51034,
          286,
          500,
          380,
          458,
          577,
          867,
          2145,
          341,
          1890,
          13,
          51144
        ]
      },
      {
        "avg_logprob": -0.27241279397692,
        "compression_ratio": 1.5365853658536586,
        "end": 792.46,
        "id": 289,
        "no_speech_prob": 0.00008888076263247058,
        "seek": 77282,
        "start": 788.4200000000001,
        "temperature": 0,
        "text": " That trains a model based on crowdsource color data.",
        "tokens": [
          51144,
          663,
          16329,
          257,
          2316,
          2361,
          322,
          26070,
          2948,
          2017,
          1412,
          13,
          51346
        ]
      },
      {
        "avg_logprob": -0.27241279397692,
        "compression_ratio": 1.5365853658536586,
        "end": 795.6600000000001,
        "id": 290,
        "no_speech_prob": 0.00008888076263247058,
        "seek": 77282,
        "start": 792.46,
        "temperature": 0,
        "text": " And if you want, just humor me for a second.",
        "tokens": [
          51346,
          400,
          498,
          291,
          528,
          11,
          445,
          14318,
          385,
          337,
          257,
          1150,
          13,
          51506
        ]
      },
      {
        "avg_logprob": -0.27241279397692,
        "compression_ratio": 1.5365853658536586,
        "end": 800.98,
        "id": 291,
        "no_speech_prob": 0.00008888076263247058,
        "seek": 77282,
        "start": 795.6600000000001,
        "temperature": 0,
        "text": " If you remember, if I go here, this is the system.",
        "tokens": [
          51506,
          759,
          291,
          1604,
          11,
          498,
          286,
          352,
          510,
          11,
          341,
          307,
          264,
          1185,
          13,
          51772
        ]
      },
      {
        "avg_logprob": -0.1933201491230666,
        "compression_ratio": 1.7107843137254901,
        "end": 805.3000000000001,
        "id": 292,
        "no_speech_prob": 0.00005144216993357986,
        "seek": 80098,
        "start": 800.98,
        "temperature": 0,
        "text": " This system was used to allow people from the internet",
        "tokens": [
          50364,
          639,
          1185,
          390,
          1143,
          281,
          2089,
          561,
          490,
          264,
          4705,
          50580
        ]
      },
      {
        "avg_logprob": -0.1933201491230666,
        "compression_ratio": 1.7107843137254901,
        "end": 810.1800000000001,
        "id": 293,
        "no_speech_prob": 0.00005144216993357986,
        "seek": 80098,
        "start": 805.3000000000001,
        "temperature": 0,
        "text": " to click on and say, that's pinkish, that's greenish,",
        "tokens": [
          50580,
          281,
          2052,
          322,
          293,
          584,
          11,
          300,
          311,
          7022,
          742,
          11,
          300,
          311,
          3092,
          742,
          11,
          50824
        ]
      },
      {
        "avg_logprob": -0.1933201491230666,
        "compression_ratio": 1.7107843137254901,
        "end": 813.1800000000001,
        "id": 294,
        "no_speech_prob": 0.00005144216993357986,
        "seek": 80098,
        "start": 810.1800000000001,
        "temperature": 0,
        "text": " that's bluish, tag a whole bunch of colors,",
        "tokens": [
          50824,
          300,
          311,
          888,
          33786,
          11,
          6162,
          257,
          1379,
          3840,
          295,
          4577,
          11,
          50974
        ]
      },
      {
        "avg_logprob": -0.1933201491230666,
        "compression_ratio": 1.7107843137254901,
        "end": 815.98,
        "id": 295,
        "no_speech_prob": 0.00005144216993357986,
        "seek": 80098,
        "start": 813.1800000000001,
        "temperature": 0,
        "text": " save all that data in a Firebase database,",
        "tokens": [
          50974,
          3155,
          439,
          300,
          1412,
          294,
          257,
          35173,
          8149,
          11,
          51114
        ]
      },
      {
        "avg_logprob": -0.1933201491230666,
        "compression_ratio": 1.7107843137254901,
        "end": 817.94,
        "id": 296,
        "no_speech_prob": 0.00005144216993357986,
        "seek": 80098,
        "start": 815.98,
        "temperature": 0,
        "text": " retrieve all that data, clean that data,",
        "tokens": [
          51114,
          30254,
          439,
          300,
          1412,
          11,
          2541,
          300,
          1412,
          11,
          51212
        ]
      },
      {
        "avg_logprob": -0.1933201491230666,
        "compression_ratio": 1.7107843137254901,
        "end": 825.22,
        "id": 297,
        "no_speech_prob": 0.00005144216993357986,
        "seek": 80098,
        "start": 817.94,
        "temperature": 0,
        "text": " put it in a JSON file, load that JSON file here into this sketch,",
        "tokens": [
          51212,
          829,
          309,
          294,
          257,
          31828,
          3991,
          11,
          3677,
          300,
          31828,
          3991,
          510,
          666,
          341,
          12325,
          11,
          51576
        ]
      },
      {
        "avg_logprob": -0.1933201491230666,
        "compression_ratio": 1.7107843137254901,
        "end": 828.98,
        "id": 298,
        "no_speech_prob": 0.00005144216993357986,
        "seek": 80098,
        "start": 825.22,
        "temperature": 0,
        "text": " build a model, train the model with that data,",
        "tokens": [
          51576,
          1322,
          257,
          2316,
          11,
          3847,
          264,
          2316,
          365,
          300,
          1412,
          11,
          51764
        ]
      },
      {
        "avg_logprob": -0.29820416898143537,
        "compression_ratio": 1.5555555555555556,
        "end": 831.86,
        "id": 299,
        "no_speech_prob": 7.571149467366922e-7,
        "seek": 82898,
        "start": 828.98,
        "temperature": 0,
        "text": " and then pull a new color from a slider.",
        "tokens": [
          50364,
          293,
          550,
          2235,
          257,
          777,
          2017,
          490,
          257,
          26046,
          13,
          50508
        ]
      },
      {
        "avg_logprob": -0.29820416898143537,
        "compression_ratio": 1.5555555555555556,
        "end": 834.4200000000001,
        "id": 300,
        "no_speech_prob": 7.571149467366922e-7,
        "seek": 82898,
        "start": 831.86,
        "temperature": 0,
        "text": " Oh, and I've forgotten something.",
        "tokens": [
          50508,
          876,
          11,
          293,
          286,
          600,
          11832,
          746,
          13,
          50636
        ]
      },
      {
        "avg_logprob": -0.29820416898143537,
        "compression_ratio": 1.5555555555555556,
        "end": 835.7,
        "id": 301,
        "no_speech_prob": 7.571149467366922e-7,
        "seek": 82898,
        "start": 834.4200000000001,
        "temperature": 0,
        "text": " Memory management.",
        "tokens": [
          50636,
          38203,
          4592,
          13,
          50700
        ]
      },
      {
        "avg_logprob": -0.29820416898143537,
        "compression_ratio": 1.5555555555555556,
        "end": 838.86,
        "id": 302,
        "no_speech_prob": 7.571149467366922e-7,
        "seek": 82898,
        "start": 835.7,
        "temperature": 0,
        "text": " Oh, I knew there was a step that I'm missing.",
        "tokens": [
          50700,
          876,
          11,
          286,
          2586,
          456,
          390,
          257,
          1823,
          300,
          286,
          478,
          5361,
          13,
          50858
        ]
      },
      {
        "avg_logprob": -0.29820416898143537,
        "compression_ratio": 1.5555555555555556,
        "end": 842.7,
        "id": 303,
        "no_speech_prob": 7.571149467366922e-7,
        "seek": 82898,
        "start": 838.86,
        "temperature": 0,
        "text": " Estimating what category out of the fixed set of labels",
        "tokens": [
          50858,
          4410,
          332,
          990,
          437,
          7719,
          484,
          295,
          264,
          6806,
          992,
          295,
          16949,
          51050
        ]
      },
      {
        "avg_logprob": -0.29820416898143537,
        "compression_ratio": 1.5555555555555556,
        "end": 844.7,
        "id": 304,
        "no_speech_prob": 7.571149467366922e-7,
        "seek": 82898,
        "start": 842.7,
        "temperature": 0,
        "text": " that color is.",
        "tokens": [
          51050,
          300,
          2017,
          307,
          13,
          51150
        ]
      },
      {
        "avg_logprob": -0.29820416898143537,
        "compression_ratio": 1.5555555555555556,
        "end": 848.86,
        "id": 305,
        "no_speech_prob": 7.571149467366922e-7,
        "seek": 82898,
        "start": 844.7,
        "temperature": 0,
        "text": " But I did forget something really quite important,",
        "tokens": [
          51150,
          583,
          286,
          630,
          2870,
          746,
          534,
          1596,
          1021,
          11,
          51358
        ]
      },
      {
        "avg_logprob": -0.29820416898143537,
        "compression_ratio": 1.5555555555555556,
        "end": 850.14,
        "id": 306,
        "no_speech_prob": 7.571149467366922e-7,
        "seek": 82898,
        "start": 848.86,
        "temperature": 0,
        "text": " which is memory management.",
        "tokens": [
          51358,
          597,
          307,
          4675,
          4592,
          13,
          51422
        ]
      },
      {
        "avg_logprob": -0.29820416898143537,
        "compression_ratio": 1.5555555555555556,
        "end": 851.26,
        "id": 307,
        "no_speech_prob": 7.571149467366922e-7,
        "seek": 82898,
        "start": 850.14,
        "temperature": 0,
        "text": " Let's look at this.",
        "tokens": [
          51422,
          961,
          311,
          574,
          412,
          341,
          13,
          51478
        ]
      },
      {
        "avg_logprob": -0.29820416898143537,
        "compression_ratio": 1.5555555555555556,
        "end": 851.76,
        "id": 308,
        "no_speech_prob": 7.571149467366922e-7,
        "seek": 82898,
        "start": 851.26,
        "temperature": 0,
        "text": " Num.",
        "tokens": [
          51478,
          22592,
          13,
          51503
        ]
      },
      {
        "avg_logprob": -0.29820416898143537,
        "compression_ratio": 1.5555555555555556,
        "end": 855.62,
        "id": 309,
        "no_speech_prob": 7.571149467366922e-7,
        "seek": 82898,
        "start": 853.86,
        "temperature": 0,
        "text": " Memory.",
        "tokens": [
          51608,
          38203,
          13,
          51696
        ]
      },
      {
        "avg_logprob": -0.2214694489213757,
        "compression_ratio": 1.5805243445692885,
        "end": 861.26,
        "id": 310,
        "no_speech_prob": 0.000002726470256675384,
        "seek": 85562,
        "start": 855.86,
        "temperature": 0,
        "text": " tf.memory.numTensors.",
        "tokens": [
          50376,
          256,
          69,
          13,
          17886,
          827,
          13,
          77,
          449,
          51,
          694,
          830,
          13,
          50646
        ]
      },
      {
        "avg_logprob": -0.2214694489213757,
        "compression_ratio": 1.5805243445692885,
        "end": 864.1,
        "id": 311,
        "no_speech_prob": 0.000002726470256675384,
        "seek": 85562,
        "start": 861.26,
        "temperature": 0,
        "text": " So again, when I create tensors that",
        "tokens": [
          50646,
          407,
          797,
          11,
          562,
          286,
          1884,
          10688,
          830,
          300,
          50788
        ]
      },
      {
        "avg_logprob": -0.2214694489213757,
        "compression_ratio": 1.5805243445692885,
        "end": 867.46,
        "id": 312,
        "no_speech_prob": 0.000002726470256675384,
        "seek": 85562,
        "start": 864.1,
        "temperature": 0,
        "text": " are allocated to memory on the GPU to store numbers,",
        "tokens": [
          50788,
          366,
          29772,
          281,
          4675,
          322,
          264,
          18407,
          281,
          3531,
          3547,
          11,
          50956
        ]
      },
      {
        "avg_logprob": -0.2214694489213757,
        "compression_ratio": 1.5805243445692885,
        "end": 869.18,
        "id": 313,
        "no_speech_prob": 0.000002726470256675384,
        "seek": 85562,
        "start": 867.46,
        "temperature": 0,
        "text": " those don't get cleaned up automatically.",
        "tokens": [
          50956,
          729,
          500,
          380,
          483,
          16146,
          493,
          6772,
          13,
          51042
        ]
      },
      {
        "avg_logprob": -0.2214694489213757,
        "compression_ratio": 1.5805243445692885,
        "end": 872.2,
        "id": 314,
        "no_speech_prob": 0.000002726470256675384,
        "seek": 85562,
        "start": 869.18,
        "temperature": 0,
        "text": " There's no garbage collector like in regular JavaScript",
        "tokens": [
          51042,
          821,
          311,
          572,
          14150,
          23960,
          411,
          294,
          3890,
          15778,
          51193
        ]
      },
      {
        "avg_logprob": -0.2214694489213757,
        "compression_ratio": 1.5805243445692885,
        "end": 873.3,
        "id": 315,
        "no_speech_prob": 0.000002726470256675384,
        "seek": 85562,
        "start": 872.2,
        "temperature": 0,
        "text": " programming.",
        "tokens": [
          51193,
          9410,
          13,
          51248
        ]
      },
      {
        "avg_logprob": -0.2214694489213757,
        "compression_ratio": 1.5805243445692885,
        "end": 875.66,
        "id": 316,
        "no_speech_prob": 0.000002726470256675384,
        "seek": 85562,
        "start": 873.3,
        "temperature": 0,
        "text": " So there are 15,485 tensors.",
        "tokens": [
          51248,
          407,
          456,
          366,
          2119,
          11,
          13318,
          20,
          10688,
          830,
          13,
          51366
        ]
      },
      {
        "avg_logprob": -0.2214694489213757,
        "compression_ratio": 1.5805243445692885,
        "end": 877.94,
        "id": 317,
        "no_speech_prob": 0.000002726470256675384,
        "seek": 85562,
        "start": 875.66,
        "temperature": 0,
        "text": " Now, one thing, and there's still",
        "tokens": [
          51366,
          823,
          11,
          472,
          551,
          11,
          293,
          456,
          311,
          920,
          51480
        ]
      },
      {
        "avg_logprob": -0.2214694489213757,
        "compression_ratio": 1.5805243445692885,
        "end": 879.58,
        "id": 318,
        "no_speech_prob": 0.000002726470256675384,
        "seek": 85562,
        "start": 877.94,
        "temperature": 0,
        "text": " even more and more and more.",
        "tokens": [
          51480,
          754,
          544,
          293,
          544,
          293,
          544,
          13,
          51562
        ]
      },
      {
        "avg_logprob": -0.2214694489213757,
        "compression_ratio": 1.5805243445692885,
        "end": 880.24,
        "id": 319,
        "no_speech_prob": 0.000002726470256675384,
        "seek": 85562,
        "start": 879.58,
        "temperature": 0,
        "text": " It's growing.",
        "tokens": [
          51562,
          467,
          311,
          4194,
          13,
          51595
        ]
      },
      {
        "avg_logprob": -0.2214694489213757,
        "compression_ratio": 1.5805243445692885,
        "end": 881.7,
        "id": 320,
        "no_speech_prob": 0.000002726470256675384,
        "seek": 85562,
        "start": 880.24,
        "temperature": 0,
        "text": " This is a memory leak.",
        "tokens": [
          51595,
          639,
          307,
          257,
          4675,
          17143,
          13,
          51668
        ]
      },
      {
        "avg_logprob": -0.2214694489213757,
        "compression_ratio": 1.5805243445692885,
        "end": 884.54,
        "id": 321,
        "no_speech_prob": 0.000002726470256675384,
        "seek": 85562,
        "start": 881.7,
        "temperature": 0,
        "text": " So one place where I didn't clean up any of the tensors",
        "tokens": [
          51668,
          407,
          472,
          1081,
          689,
          286,
          994,
          380,
          2541,
          493,
          604,
          295,
          264,
          10688,
          830,
          51810
        ]
      },
      {
        "avg_logprob": -0.2214694489213757,
        "compression_ratio": 1.5805243445692885,
        "end": 885.3,
        "id": 322,
        "no_speech_prob": 0.000002726470256675384,
        "seek": 85562,
        "start": 884.54,
        "temperature": 0,
        "text": " is right here.",
        "tokens": [
          51810,
          307,
          558,
          510,
          13,
          51848
        ]
      },
      {
        "avg_logprob": -0.22947136561075845,
        "compression_ratio": 1.6106194690265487,
        "end": 889.14,
        "id": 323,
        "no_speech_prob": 0.0000030894937026459957,
        "seek": 88530,
        "start": 885.4599999999999,
        "temperature": 0,
        "text": " And there's an easy way I can clean this up",
        "tokens": [
          50372,
          400,
          456,
          311,
          364,
          1858,
          636,
          286,
          393,
          2541,
          341,
          493,
          50556
        ]
      },
      {
        "avg_logprob": -0.22947136561075845,
        "compression_ratio": 1.6106194690265487,
        "end": 893.6999999999999,
        "id": 324,
        "no_speech_prob": 0.0000030894937026459957,
        "seek": 88530,
        "start": 889.14,
        "temperature": 0,
        "text": " by adding in the tf.tidy function.",
        "tokens": [
          50556,
          538,
          5127,
          294,
          264,
          256,
          69,
          13,
          83,
          38836,
          2445,
          13,
          50784
        ]
      },
      {
        "avg_logprob": -0.22947136561075845,
        "compression_ratio": 1.6106194690265487,
        "end": 898.42,
        "id": 325,
        "no_speech_prob": 0.0000030894937026459957,
        "seek": 88530,
        "start": 893.6999999999999,
        "temperature": 0,
        "text": " So what tf.tidy does is it says, just put all of this code that's",
        "tokens": [
          50784,
          407,
          437,
          256,
          69,
          13,
          83,
          38836,
          775,
          307,
          309,
          1619,
          11,
          445,
          829,
          439,
          295,
          341,
          3089,
          300,
          311,
          51020
        ]
      },
      {
        "avg_logprob": -0.22947136561075845,
        "compression_ratio": 1.6106194690265487,
        "end": 902.42,
        "id": 326,
        "no_speech_prob": 0.0000030894937026459957,
        "seek": 88530,
        "start": 898.42,
        "temperature": 0,
        "text": " inside of this function passed into tf.tidy.",
        "tokens": [
          51020,
          1854,
          295,
          341,
          2445,
          4678,
          666,
          256,
          69,
          13,
          83,
          38836,
          13,
          51220
        ]
      },
      {
        "avg_logprob": -0.22947136561075845,
        "compression_ratio": 1.6106194690265487,
        "end": 904.9,
        "id": 327,
        "no_speech_prob": 0.0000030894937026459957,
        "seek": 88530,
        "start": 902.42,
        "temperature": 0,
        "text": " Clean up any tensors that are made there.",
        "tokens": [
          51220,
          18463,
          493,
          604,
          10688,
          830,
          300,
          366,
          1027,
          456,
          13,
          51344
        ]
      },
      {
        "avg_logprob": -0.22947136561075845,
        "compression_ratio": 1.6106194690265487,
        "end": 908.4599999999999,
        "id": 328,
        "no_speech_prob": 0.0000030894937026459957,
        "seek": 88530,
        "start": 904.9,
        "temperature": 0,
        "text": " So this will clean up everything for me.",
        "tokens": [
          51344,
          407,
          341,
          486,
          2541,
          493,
          1203,
          337,
          385,
          13,
          51522
        ]
      },
      {
        "avg_logprob": -0.22947136561075845,
        "compression_ratio": 1.6106194690265487,
        "end": 911.06,
        "id": 329,
        "no_speech_prob": 0.0000030894937026459957,
        "seek": 88530,
        "start": 908.4599999999999,
        "temperature": 0,
        "text": " So now let's run this again.",
        "tokens": [
          51522,
          407,
          586,
          718,
          311,
          1190,
          341,
          797,
          13,
          51652
        ]
      },
      {
        "avg_logprob": -0.22947136561075845,
        "compression_ratio": 1.6106194690265487,
        "end": 913.02,
        "id": 330,
        "no_speech_prob": 0.0000030894937026459957,
        "seek": 88530,
        "start": 911.06,
        "temperature": 0,
        "text": " And we're going to take a look at the tensors.",
        "tokens": [
          51652,
          400,
          321,
          434,
          516,
          281,
          747,
          257,
          574,
          412,
          264,
          10688,
          830,
          13,
          51750
        ]
      },
      {
        "avg_logprob": -0.22947136561075845,
        "compression_ratio": 1.6106194690265487,
        "end": 914.9799999999999,
        "id": 331,
        "no_speech_prob": 0.0000030894937026459957,
        "seek": 88530,
        "start": 913.02,
        "temperature": 0,
        "text": " There's 31, 73.",
        "tokens": [
          51750,
          821,
          311,
          10353,
          11,
          28387,
          13,
          51848
        ]
      },
      {
        "avg_logprob": -0.2394576072692871,
        "compression_ratio": 1.470873786407767,
        "end": 917.02,
        "id": 332,
        "no_speech_prob": 0.000002642582103362656,
        "seek": 91498,
        "start": 915.14,
        "temperature": 0,
        "text": " It's kind of leaking, right?",
        "tokens": [
          50372,
          467,
          311,
          733,
          295,
          32856,
          11,
          558,
          30,
          50466
        ]
      },
      {
        "avg_logprob": -0.2394576072692871,
        "compression_ratio": 1.470873786407767,
        "end": 920.02,
        "id": 333,
        "no_speech_prob": 0.000002642582103362656,
        "seek": 91498,
        "start": 917.02,
        "temperature": 0,
        "text": " Well, let's let it get all the way through 10 epochs.",
        "tokens": [
          50466,
          1042,
          11,
          718,
          311,
          718,
          309,
          483,
          439,
          264,
          636,
          807,
          1266,
          30992,
          28346,
          13,
          50616
        ]
      },
      {
        "avg_logprob": -0.2394576072692871,
        "compression_ratio": 1.470873786407767,
        "end": 921.86,
        "id": 334,
        "no_speech_prob": 0.000002642582103362656,
        "seek": 91498,
        "start": 920.02,
        "temperature": 0,
        "text": " I'll be back in a minute when that finishes.",
        "tokens": [
          50616,
          286,
          603,
          312,
          646,
          294,
          257,
          3456,
          562,
          300,
          23615,
          13,
          50708
        ]
      },
      {
        "avg_logprob": -0.2394576072692871,
        "compression_ratio": 1.470873786407767,
        "end": 931.02,
        "id": 335,
        "no_speech_prob": 0.000002642582103362656,
        "seek": 91498,
        "start": 921.86,
        "temperature": 0,
        "text": " So the training is complete.",
        "tokens": [
          50708,
          220,
          6455,
          264,
          3097,
          307,
          3566,
          13,
          51166
        ]
      },
      {
        "avg_logprob": -0.2394576072692871,
        "compression_ratio": 1.470873786407767,
        "end": 933.26,
        "id": 336,
        "no_speech_prob": 0.000002642582103362656,
        "seek": 91498,
        "start": 931.02,
        "temperature": 0,
        "text": " And we can see now, ah, there we go.",
        "tokens": [
          51166,
          400,
          321,
          393,
          536,
          586,
          11,
          3716,
          11,
          456,
          321,
          352,
          13,
          51278
        ]
      },
      {
        "avg_logprob": -0.2394576072692871,
        "compression_ratio": 1.470873786407767,
        "end": 936.4200000000001,
        "id": 337,
        "no_speech_prob": 0.000002642582103362656,
        "seek": 91498,
        "start": 933.26,
        "temperature": 0,
        "text": " I am no longer leaking tensors.",
        "tokens": [
          51278,
          286,
          669,
          572,
          2854,
          32856,
          10688,
          830,
          13,
          51436
        ]
      },
      {
        "avg_logprob": -0.2394576072692871,
        "compression_ratio": 1.470873786407767,
        "end": 941.4200000000001,
        "id": 338,
        "no_speech_prob": 0.000002642582103362656,
        "seek": 91498,
        "start": 936.4200000000001,
        "temperature": 0,
        "text": " Now, the thing is, did I really need 1,628 tensors?",
        "tokens": [
          51436,
          823,
          11,
          264,
          551,
          307,
          11,
          630,
          286,
          534,
          643,
          502,
          11,
          21,
          11205,
          10688,
          830,
          30,
          51686
        ]
      },
      {
        "avg_logprob": -0.2394576072692871,
        "compression_ratio": 1.470873786407767,
        "end": 942.46,
        "id": 339,
        "no_speech_prob": 0.000002642582103362656,
        "seek": 91498,
        "start": 941.4200000000001,
        "temperature": 0,
        "text": " I don't think that I did.",
        "tokens": [
          51686,
          286,
          500,
          380,
          519,
          300,
          286,
          630,
          13,
          51738
        ]
      },
      {
        "avg_logprob": -0.28182181068088696,
        "compression_ratio": 1.4842105263157894,
        "end": 946.34,
        "id": 340,
        "no_speech_prob": 0.0000045659689931198955,
        "seek": 94246,
        "start": 942.46,
        "temperature": 0,
        "text": " I think there is also a leak going on",
        "tokens": [
          50364,
          286,
          519,
          456,
          307,
          611,
          257,
          17143,
          516,
          322,
          50558
        ]
      },
      {
        "avg_logprob": -0.28182181068088696,
        "compression_ratio": 1.4842105263157894,
        "end": 949.82,
        "id": 341,
        "no_speech_prob": 0.0000045659689931198955,
        "seek": 94246,
        "start": 946.34,
        "temperature": 0,
        "text": " inside of this train function.",
        "tokens": [
          50558,
          1854,
          295,
          341,
          3847,
          2445,
          13,
          50732
        ]
      },
      {
        "avg_logprob": -0.28182181068088696,
        "compression_ratio": 1.4842105263157894,
        "end": 953.58,
        "id": 342,
        "no_speech_prob": 0.0000045659689931198955,
        "seek": 94246,
        "start": 949.82,
        "temperature": 0,
        "text": " And I think there's an issue with this.",
        "tokens": [
          50732,
          400,
          286,
          519,
          456,
          311,
          364,
          2734,
          365,
          341,
          13,
          50920
        ]
      },
      {
        "avg_logprob": -0.28182181068088696,
        "compression_ratio": 1.4842105263157894,
        "end": 956.26,
        "id": 343,
        "no_speech_prob": 0.0000045659689931198955,
        "seek": 94246,
        "start": 953.58,
        "temperature": 0,
        "text": " And so I might have to do a follow-up video about this.",
        "tokens": [
          50920,
          400,
          370,
          286,
          1062,
          362,
          281,
          360,
          257,
          1524,
          12,
          1010,
          960,
          466,
          341,
          13,
          51054
        ]
      },
      {
        "avg_logprob": -0.28182181068088696,
        "compression_ratio": 1.4842105263157894,
        "end": 963.22,
        "id": 344,
        "no_speech_prob": 0.0000045659689931198955,
        "seek": 94246,
        "start": 956.26,
        "temperature": 0,
        "text": " Because at the moment, if I go to github.com tf.",
        "tokens": [
          51054,
          1436,
          412,
          264,
          1623,
          11,
          498,
          286,
          352,
          281,
          290,
          355,
          836,
          13,
          1112,
          256,
          69,
          13,
          51402
        ]
      },
      {
        "avg_logprob": -0.28182181068088696,
        "compression_ratio": 1.4842105263157894,
        "end": 965.46,
        "id": 345,
        "no_speech_prob": 0.0000045659689931198955,
        "seek": 94246,
        "start": 963.22,
        "temperature": 0,
        "text": " Oh, hold on.",
        "tokens": [
          51402,
          876,
          11,
          1797,
          322,
          13,
          51514
        ]
      },
      {
        "avg_logprob": -0.28182181068088696,
        "compression_ratio": 1.4842105263157894,
        "end": 971.58,
        "id": 346,
        "no_speech_prob": 0.0000045659689931198955,
        "seek": 94246,
        "start": 965.46,
        "temperature": 0,
        "text": " Let's go from here to, I should have had this prepared.",
        "tokens": [
          51514,
          961,
          311,
          352,
          490,
          510,
          281,
          11,
          286,
          820,
          362,
          632,
          341,
          4927,
          13,
          51820
        ]
      },
      {
        "avg_logprob": -0.2797344391604504,
        "compression_ratio": 1.5180722891566265,
        "end": 972.3000000000001,
        "id": 347,
        "no_speech_prob": 0.0000074112749643973075,
        "seek": 97158,
        "start": 971.62,
        "temperature": 0,
        "text": " Where do I go?",
        "tokens": [
          50366,
          2305,
          360,
          286,
          352,
          30,
          50400
        ]
      },
      {
        "avg_logprob": -0.2797344391604504,
        "compression_ratio": 1.5180722891566265,
        "end": 976.38,
        "id": 348,
        "no_speech_prob": 0.0000074112749643973075,
        "seek": 97158,
        "start": 972.3000000000001,
        "temperature": 0,
        "text": " GitHub and Issues.",
        "tokens": [
          50400,
          23331,
          293,
          38195,
          1247,
          13,
          50604
        ]
      },
      {
        "avg_logprob": -0.2797344391604504,
        "compression_ratio": 1.5180722891566265,
        "end": 985.22,
        "id": 349,
        "no_speech_prob": 0.0000074112749643973075,
        "seek": 97158,
        "start": 976.38,
        "temperature": 0,
        "text": " And I'm going to look for a fit memory leak, this one.",
        "tokens": [
          50604,
          400,
          286,
          478,
          516,
          281,
          574,
          337,
          257,
          3318,
          4675,
          17143,
          11,
          341,
          472,
          13,
          51046
        ]
      },
      {
        "avg_logprob": -0.2797344391604504,
        "compression_ratio": 1.5180722891566265,
        "end": 989.5400000000001,
        "id": 350,
        "no_speech_prob": 0.0000074112749643973075,
        "seek": 97158,
        "start": 985.22,
        "temperature": 0,
        "text": " So I believe there is, at present, a memory leak",
        "tokens": [
          51046,
          407,
          286,
          1697,
          456,
          307,
          11,
          412,
          1974,
          11,
          257,
          4675,
          17143,
          51262
        ]
      },
      {
        "avg_logprob": -0.2797344391604504,
        "compression_ratio": 1.5180722891566265,
        "end": 991.98,
        "id": 351,
        "no_speech_prob": 0.0000074112749643973075,
        "seek": 97158,
        "start": 989.5400000000001,
        "temperature": 0,
        "text": " in model.fit with callbacks.",
        "tokens": [
          51262,
          294,
          2316,
          13,
          6845,
          365,
          818,
          17758,
          13,
          51384
        ]
      },
      {
        "avg_logprob": -0.2797344391604504,
        "compression_ratio": 1.5180722891566265,
        "end": 995.62,
        "id": 352,
        "no_speech_prob": 0.0000074112749643973075,
        "seek": 97158,
        "start": 991.98,
        "temperature": 0,
        "text": " And you can see that's exactly what I'm doing, right?",
        "tokens": [
          51384,
          400,
          291,
          393,
          536,
          300,
          311,
          2293,
          437,
          286,
          478,
          884,
          11,
          558,
          30,
          51566
        ]
      },
      {
        "avg_logprob": -0.2797344391604504,
        "compression_ratio": 1.5180722891566265,
        "end": 999.9000000000001,
        "id": 353,
        "no_speech_prob": 0.0000074112749643973075,
        "seek": 97158,
        "start": 995.62,
        "temperature": 0,
        "text": " Where model.fit with callbacks.",
        "tokens": [
          51566,
          2305,
          2316,
          13,
          6845,
          365,
          818,
          17758,
          13,
          51780
        ]
      },
      {
        "avg_logprob": -0.22616561363483298,
        "compression_ratio": 1.7281553398058251,
        "end": 1003.22,
        "id": 354,
        "no_speech_prob": 0.00001834288013924379,
        "seek": 99990,
        "start": 999.9,
        "temperature": 0,
        "text": " So I'm going to not worry about that particular memory leak",
        "tokens": [
          50364,
          407,
          286,
          478,
          516,
          281,
          406,
          3292,
          466,
          300,
          1729,
          4675,
          17143,
          50530
        ]
      },
      {
        "avg_logprob": -0.22616561363483298,
        "compression_ratio": 1.7281553398058251,
        "end": 1004.14,
        "id": 355,
        "no_speech_prob": 0.00001834288013924379,
        "seek": 99990,
        "start": 1003.22,
        "temperature": 0,
        "text": " right now.",
        "tokens": [
          50530,
          558,
          586,
          13,
          50576
        ]
      },
      {
        "avg_logprob": -0.22616561363483298,
        "compression_ratio": 1.7281553398058251,
        "end": 1006.66,
        "id": 356,
        "no_speech_prob": 0.00001834288013924379,
        "seek": 99990,
        "start": 1004.14,
        "temperature": 0,
        "text": " I'm going to wait for us to see if that gets corrected.",
        "tokens": [
          50576,
          286,
          478,
          516,
          281,
          1699,
          337,
          505,
          281,
          536,
          498,
          300,
          2170,
          31687,
          13,
          50702
        ]
      },
      {
        "avg_logprob": -0.22616561363483298,
        "compression_ratio": 1.7281553398058251,
        "end": 1008.02,
        "id": 357,
        "no_speech_prob": 0.00001834288013924379,
        "seek": 99990,
        "start": 1006.66,
        "temperature": 0,
        "text": " By the time you're watching this,",
        "tokens": [
          50702,
          3146,
          264,
          565,
          291,
          434,
          1976,
          341,
          11,
          50770
        ]
      },
      {
        "avg_logprob": -0.22616561363483298,
        "compression_ratio": 1.7281553398058251,
        "end": 1009.36,
        "id": 358,
        "no_speech_prob": 0.00001834288013924379,
        "seek": 99990,
        "start": 1008.02,
        "temperature": 0,
        "text": " that might already be corrected.",
        "tokens": [
          50770,
          300,
          1062,
          1217,
          312,
          31687,
          13,
          50837
        ]
      },
      {
        "avg_logprob": -0.22616561363483298,
        "compression_ratio": 1.7281553398058251,
        "end": 1011.9399999999999,
        "id": 359,
        "no_speech_prob": 0.00001834288013924379,
        "seek": 99990,
        "start": 1009.36,
        "temperature": 0,
        "text": " And this code might have no more memory leaks in it",
        "tokens": [
          50837,
          400,
          341,
          3089,
          1062,
          362,
          572,
          544,
          4675,
          28885,
          294,
          309,
          50966
        ]
      },
      {
        "avg_logprob": -0.22616561363483298,
        "compression_ratio": 1.7281553398058251,
        "end": 1014.54,
        "id": 360,
        "no_speech_prob": 0.00001834288013924379,
        "seek": 99990,
        "start": 1011.9399999999999,
        "temperature": 0,
        "text": " just by updating the version of TensorFlow.js.",
        "tokens": [
          50966,
          445,
          538,
          25113,
          264,
          3037,
          295,
          37624,
          13,
          25530,
          13,
          51096
        ]
      },
      {
        "avg_logprob": -0.22616561363483298,
        "compression_ratio": 1.7281553398058251,
        "end": 1017.06,
        "id": 361,
        "no_speech_prob": 0.00001834288013924379,
        "seek": 99990,
        "start": 1014.54,
        "temperature": 0,
        "text": " Or I might still be missing something in here",
        "tokens": [
          51096,
          1610,
          286,
          1062,
          920,
          312,
          5361,
          746,
          294,
          510,
          51222
        ]
      },
      {
        "avg_logprob": -0.22616561363483298,
        "compression_ratio": 1.7281553398058251,
        "end": 1017.9399999999999,
        "id": 362,
        "no_speech_prob": 0.00001834288013924379,
        "seek": 99990,
        "start": 1017.06,
        "temperature": 0,
        "text": " to do a memory leak.",
        "tokens": [
          51222,
          281,
          360,
          257,
          4675,
          17143,
          13,
          51266
        ]
      },
      {
        "avg_logprob": -0.22616561363483298,
        "compression_ratio": 1.7281553398058251,
        "end": 1021.42,
        "id": 363,
        "no_speech_prob": 0.00001834288013924379,
        "seek": 99990,
        "start": 1017.9399999999999,
        "temperature": 0,
        "text": " So if you don't want any spoilers,",
        "tokens": [
          51266,
          407,
          498,
          291,
          500,
          380,
          528,
          604,
          32237,
          11,
          51440
        ]
      },
      {
        "avg_logprob": -0.22616561363483298,
        "compression_ratio": 1.7281553398058251,
        "end": 1024.74,
        "id": 364,
        "no_speech_prob": 0.00001834288013924379,
        "seek": 99990,
        "start": 1021.42,
        "temperature": 0,
        "text": " the following videos have not been published yet.",
        "tokens": [
          51440,
          264,
          3480,
          2145,
          362,
          406,
          668,
          6572,
          1939,
          13,
          51606
        ]
      },
      {
        "avg_logprob": -0.22616561363483298,
        "compression_ratio": 1.7281553398058251,
        "end": 1026.66,
        "id": 365,
        "no_speech_prob": 0.00001834288013924379,
        "seek": 99990,
        "start": 1024.74,
        "temperature": 0,
        "text": " You could sort that out yourself.",
        "tokens": [
          51606,
          509,
          727,
          1333,
          300,
          484,
          1803,
          13,
          51702
        ]
      },
      {
        "avg_logprob": -0.22616561363483298,
        "compression_ratio": 1.7281553398058251,
        "end": 1029.58,
        "id": 366,
        "no_speech_prob": 0.00001834288013924379,
        "seek": 99990,
        "start": 1026.66,
        "temperature": 0,
        "text": " But I will come back at some point and talk about that.",
        "tokens": [
          51702,
          583,
          286,
          486,
          808,
          646,
          412,
          512,
          935,
          293,
          751,
          466,
          300,
          13,
          51848
        ]
      },
      {
        "avg_logprob": -0.24222771448033456,
        "compression_ratio": 1.6436363636363636,
        "end": 1032.4199999999998,
        "id": 367,
        "no_speech_prob": 0.00010720803402364254,
        "seek": 102958,
        "start": 1030.26,
        "temperature": 0,
        "text": " Thank you for watching.",
        "tokens": [
          50398,
          1044,
          291,
          337,
          1976,
          13,
          50506
        ]
      },
      {
        "avg_logprob": -0.24222771448033456,
        "compression_ratio": 1.6436363636363636,
        "end": 1038.1799999999998,
        "id": 368,
        "no_speech_prob": 0.00010720803402364254,
        "seek": 102958,
        "start": 1032.4199999999998,
        "temperature": 0,
        "text": " I wish you many purplish and pinkish and bluish",
        "tokens": [
          50506,
          286,
          3172,
          291,
          867,
          1864,
          564,
          742,
          293,
          7022,
          742,
          293,
          888,
          33786,
          50794
        ]
      },
      {
        "avg_logprob": -0.24222771448033456,
        "compression_ratio": 1.6436363636363636,
        "end": 1041.46,
        "id": 369,
        "no_speech_prob": 0.00010720803402364254,
        "seek": 102958,
        "start": 1038.1799999999998,
        "temperature": 0,
        "text": " and greenish days.",
        "tokens": [
          50794,
          293,
          3092,
          742,
          1708,
          13,
          50958
        ]
      },
      {
        "avg_logprob": -0.24222771448033456,
        "compression_ratio": 1.6436363636363636,
        "end": 1044.22,
        "id": 370,
        "no_speech_prob": 0.00010720803402364254,
        "seek": 102958,
        "start": 1041.46,
        "temperature": 0,
        "text": " All the colors of the rainbow, may they fill your days",
        "tokens": [
          50958,
          1057,
          264,
          4577,
          295,
          264,
          18526,
          11,
          815,
          436,
          2836,
          428,
          1708,
          51096
        ]
      },
      {
        "avg_logprob": -0.24222771448033456,
        "compression_ratio": 1.6436363636363636,
        "end": 1045.1799999999998,
        "id": 371,
        "no_speech_prob": 0.00010720803402364254,
        "seek": 102958,
        "start": 1044.22,
        "temperature": 0,
        "text": " with joy.",
        "tokens": [
          51096,
          365,
          6258,
          13,
          51144
        ]
      },
      {
        "avg_logprob": -0.24222771448033456,
        "compression_ratio": 1.6436363636363636,
        "end": 1048.3799999999999,
        "id": 372,
        "no_speech_prob": 0.00010720803402364254,
        "seek": 102958,
        "start": 1045.1799999999998,
        "temperature": 0,
        "text": " May you make your own classifier with your own data.",
        "tokens": [
          51144,
          1891,
          291,
          652,
          428,
          1065,
          1508,
          9902,
          365,
          428,
          1065,
          1412,
          13,
          51304
        ]
      },
      {
        "avg_logprob": -0.24222771448033456,
        "compression_ratio": 1.6436363636363636,
        "end": 1049.78,
        "id": 373,
        "no_speech_prob": 0.00010720803402364254,
        "seek": 102958,
        "start": 1048.3799999999999,
        "temperature": 0,
        "text": " Please share it with me.",
        "tokens": [
          51304,
          2555,
          2073,
          309,
          365,
          385,
          13,
          51374
        ]
      },
      {
        "avg_logprob": -0.24222771448033456,
        "compression_ratio": 1.6436363636363636,
        "end": 1050.3799999999999,
        "id": 374,
        "no_speech_prob": 0.00010720803402364254,
        "seek": 102958,
        "start": 1049.78,
        "temperature": 0,
        "text": " I don't know.",
        "tokens": [
          51374,
          286,
          500,
          380,
          458,
          13,
          51404
        ]
      },
      {
        "avg_logprob": -0.24222771448033456,
        "compression_ratio": 1.6436363636363636,
        "end": 1052.54,
        "id": 375,
        "no_speech_prob": 0.00010720803402364254,
        "seek": 102958,
        "start": 1050.3799999999999,
        "temperature": 0,
        "text": " Has this helped the world, this tutorial series?",
        "tokens": [
          51404,
          8646,
          341,
          4254,
          264,
          1002,
          11,
          341,
          7073,
          2638,
          30,
          51512
        ]
      },
      {
        "avg_logprob": -0.24222771448033456,
        "compression_ratio": 1.6436363636363636,
        "end": 1055.1,
        "id": 376,
        "no_speech_prob": 0.00010720803402364254,
        "seek": 102958,
        "start": 1052.54,
        "temperature": 0,
        "text": " I've missed so much about data and data collection",
        "tokens": [
          51512,
          286,
          600,
          6721,
          370,
          709,
          466,
          1412,
          293,
          1412,
          5765,
          51640
        ]
      },
      {
        "avg_logprob": -0.24222771448033456,
        "compression_ratio": 1.6436363636363636,
        "end": 1057.06,
        "id": 377,
        "no_speech_prob": 0.00010720803402364254,
        "seek": 102958,
        "start": 1055.1,
        "temperature": 0,
        "text": " and machine learning and models and algorithms.",
        "tokens": [
          51640,
          293,
          3479,
          2539,
          293,
          5245,
          293,
          14642,
          13,
          51738
        ]
      },
      {
        "avg_logprob": -0.24222771448033456,
        "compression_ratio": 1.6436363636363636,
        "end": 1058.48,
        "id": 378,
        "no_speech_prob": 0.00010720803402364254,
        "seek": 102958,
        "start": 1057.06,
        "temperature": 0,
        "text": " But hopefully, I've done something.",
        "tokens": [
          51738,
          583,
          4696,
          11,
          286,
          600,
          1096,
          746,
          13,
          51809
        ]
      },
      {
        "avg_logprob": -0.24222771448033456,
        "compression_ratio": 1.6436363636363636,
        "end": 1059.54,
        "id": 379,
        "no_speech_prob": 0.00010720803402364254,
        "seek": 102958,
        "start": 1058.48,
        "temperature": 0,
        "text": " This is not the end.",
        "tokens": [
          51809,
          639,
          307,
          406,
          264,
          917,
          13,
          51862
        ]
      },
      {
        "avg_logprob": -0.3494821957179478,
        "compression_ratio": 1.180327868852459,
        "end": 1061.34,
        "id": 380,
        "no_speech_prob": 0.0004954967880621552,
        "seek": 105954,
        "start": 1059.54,
        "temperature": 0,
        "text": " It's only the beginning.",
        "tokens": [
          50364,
          467,
          311,
          787,
          264,
          2863,
          13,
          50454
        ]
      },
      {
        "avg_logprob": -0.3494821957179478,
        "compression_ratio": 1.180327868852459,
        "end": 1064.54,
        "id": 381,
        "no_speech_prob": 0.0004954967880621552,
        "seek": 105954,
        "start": 1061.34,
        "temperature": 0,
        "text": " I'll see you soon in future tutorial videos.",
        "tokens": [
          50454,
          286,
          603,
          536,
          291,
          2321,
          294,
          2027,
          7073,
          2145,
          13,
          50614
        ]
      },
      {
        "avg_logprob": -0.3494821957179478,
        "compression_ratio": 1.180327868852459,
        "end": 1067.82,
        "id": 382,
        "no_speech_prob": 0.0004954967880621552,
        "seek": 105954,
        "start": 1064.54,
        "temperature": 0,
        "text": " Because this playlist probably has about 300 more left in it.",
        "tokens": [
          50614,
          1436,
          341,
          16788,
          1391,
          575,
          466,
          6641,
          544,
          1411,
          294,
          309,
          13,
          50778
        ]
      },
      {
        "avg_logprob": -0.3494821957179478,
        "compression_ratio": 1.180327868852459,
        "end": 1068.42,
        "id": 383,
        "no_speech_prob": 0.0004954967880621552,
        "seek": 105954,
        "start": 1067.82,
        "temperature": 0,
        "text": " OK, goodbye.",
        "tokens": [
          50778,
          2264,
          11,
          12084,
          13,
          50808
        ]
      },
      {
        "avg_logprob": -1.4695320129394531,
        "compression_ratio": 0.2727272727272727,
        "end": 1071.3000000000002,
        "id": 384,
        "no_speech_prob": 0.3812182545661926,
        "seek": 106842,
        "start": 1068.42,
        "temperature": 1,
        "text": " You",
        "tokens": [
          50408,
          509,
          50508
        ]
      }
    ],
    "transcription": " All right, this is getting tiring, but I am back and I have yet another video in this building your own custom color classifier with TensorFlow.js series. Now, the thing that I want to add to this video, and by the way, this line moving across is pointless. I just have it there so that I can see that the draw loop is animating, that I haven't blocked it. There's two things that I missed that are kind of important from the previous video. One is, this is actually not the validation data loss. I didn't realize this, but I'm going to change this here. I'm going to console.log the full logs object. So, what I'm putting onto the screen is logs.loss. Let me console.log what's there. So, again, we have to wait a minute for the first epoch to finish. Apologies for that. Da, da, da, da, da, da, da, da. Okay, there are actually two loss values. There's the loss function computed against the training data and there's the loss function computed against the validation data. Now, to do this properly, I really should be using the validation loss because that's data that hasn't been done with the training. That's protecting against overfitting, having my model work really well with the training data only. The thing is, I have a very small data set of 5,000 data points. I'm just using 10% as the validation data and the way TensorFlow.js works, it also takes that 10% from the end and I wasn't careful about shuffling the data around. So, this is something that I should come back to. I don't know, maybe this series will go on to infinity, but if I were doing this properly, I would actually want to show the validation loss here like this, logs.validationLoss. Maybe I want to show both and maybe I want to be more thoughtful about shuffling the data first in advance. But that's not what I said I was going to do in the next video, so I'm again leaving that temporarily as an exercise to the viewer or I'll come back and do it in a future video, I don't know yet. That's item number one. Item number two, thank you to me, I am so me and others in the Coding Train sponsor patron group. I made this way more complicated than it needs by trying to make this an async function in here. Actually, this does not need to be an async function. If I just return tf.nextFrame. So if I just return tf.nextFrame, it's actually returning the promise and unlocking the draw loop, so that makes it simpler. And actually, what I really want to do, what I really want, I couldn't make this so simpler. What am I doing here? At the end of every batch, I want tf.nextFrame to be executed and so I actually don't need to write a wrapper function to execute tf.nextFrame. What I could just do is set that as the callback. Again, if I wanted to do more with onBatchEnd, look at the loss and the logs, but really what I want is at the end of every batch to draw a new frame of animation, I could just put tf.nextFrame as the function, which is the callback there. Okay? So this is still working, that simplifies the code, makes it a little nicer to look at. I don't even really need this onBegin and onEnd, but I'll leave those in there just so you see them. Okay, so now I'm ready for what is the purpose of this video. The purpose of this video is while I'm training the model, I could wait until I finish training the model, but I'm actually going to allow this to happen while I'm training the model. I want to be able to specify a color and see what the neural network thinks that color is. So very quickly to do this, what I'm going to do is I'm going to create a rslider, gslider, bslider. I'm going to make three sliders. Again, this could use a lot of improvements and I'm going to use the p5 dom library, create slider function. So the slider is arranged between zero and 255. And let's start with like, what's, does red and green make yellow? Let's start with a yellow. And so the g, the bslider should be on zero and then I want the background color to, I don't know what that's doing there. I don't need this line anymore, it's distracting. I want to say rslider, well let's actually, so I want to say let r equal rslider.value. So I want to get the values from the sliders. I want g and I want b. Eventually I'm going to send these as inputs into the neural network, but right now I just want to be able to see that color. R, g, b, okay? So here we go. So now we should see there are three sliders and as I adjust these sliders, I can change the color. And so what I want, whoops, what I want is to be able to, and I see the, what I want is now to see the neural network's prediction down here. So how do I do that? Okay, time to use tensorflow.js again, woohoo! So I need to make some input data. So the input x's are tensor, tf.tensor2d, and an array with r, g, b in it. Now in theory, I could be running prediction with multiple r, g, b's, right? But I'm not, so I need an array of arrays in here. So this is my input data. Then what do I want to do? I want to say model.predict with those x's. Feel like there's, oh, you know what? I need to normalize those. Right, because it expects to have normalized values between zero and one, so I need to divide each of those by 255. Then I need to call model.predict and then look at the results. And that, oh, you know what? This doesn't actually happen asynchronously. It's because the data is still in the GPU. This is a confusing thing. I have to pull, I'm going to use that data. I have to pull it out, but let's just look at the results, the pure results. So I should then be able to say results.print, okay? So I think this is me just creating the inputs, getting the prediction, and then I should be able to see that in the console. Syntax error. Do I have an extra curly bracket? All right, okay, so we can see this, and this is exactly what I should be getting, right? It is a probability distribution over nine labels. Now, whether it's giving me correct ones, who knows? But look at that. So now, how do I get the label out of there? Well, remember that what I'm looking for is I'm looking for which probability is at the highest level. Is it a 90% chance of it being yellowish and.01,.02,.03, 1%, 2%, 3% of being the other ones? And there actually is a function in TensorFlow.js that will pull out the index of the highest probability value. That's called argmax, right? I could write a little for loop or some kind of function to do that, but if I look for argmax, tf.argmax returns the indices of the maximum values along an axis. So this can be quite more complex because I can have multi-dimensional data, but I actually get to do this in a really simple way. I just want to say, let index equal results.argmax, and there's an axis of one, there's a one-dimensional here. So now, let me say index.print, and so let me run this, and we can see it's just giving me, hmm, is that right? Is that a coincidence? So I should get some different values. Yes, okay, so it's actually changing. So that's giving me that maximum index. So as I change, so this is my label. Here's the thing, though. That's my label, but I need to convert that to one of these. So zero means reddish, one means greenish, two means bluish, three means orangish. So I have this label list already. I should be able to just say, let label equal labelListIndex. The only thing is I can't do that because this is a tensor. That's a tensor, and what I want, I need to pull that. The tensor is the numbers, the data that lives on the GPU, the WebGL fancy thing, that TensorFlow.js has implemented. I need to pull that off. And normally I would pull that off with an asynchronous function, but the thing is here, it's such a little tiny bit of data, I think I can pull it off synchronously and not slow down my program from running. So actually what I want to say here is data sync, and then, which is a, dot data would pull it off asynchronously. So let's look at, and let's say, I'm going to console.log index, and let me get rid of my other console logs that I don't really want to look at right now. So, oh, okay. So I got an array with the number in it. I pulled it off, and so then I just want to say index zero. So I only need that first value, index zero. And so there we go. That's the label number. We now have the label number, and so now I can say this, and I can say, and let's put it on a paragraph element. So let's say let label p. Let's have that be first. And so now I want to say label p equals create p. And then I should say, all the way back down here, label p dot html label. Okay, ready for this? Here we go. I've started my training. Oh, wait, why? This is so silly, but I want the label to be above it. I really should not be changing this right now. So let me just put it here. Okay, so here we go. It thinks that's greenish, right? Well, it hasn't gotten very far with the training. I would imagine that once we train further and the law starts going down, it's going to recognize that as yellowish. So here, I'm going to just wait a little bit, and I'll be back in a minute. All right, I'm back. So it trained over 10 epochs, and you can see now it's saying this is yellowish. Let me tune this down. That's greenish. Turn this up. That's bluish. We still got bluish. Can we get some purple? Purpleish. Can we get some pink? Oh, it didn't get pink. Maybe if I add a little more brightness, ah, now it thinks that's pink. So I have now trained the neural network to recognize, and let's see if it can get red, reddish. So we can see I can play with this all day long. This is now going to classify the color based on that particular model. So in a way, I'm done. I probably want to train it for more epochs. What are some things that I want to do? So one is I would want to get more data. I would want to be more thoughtful about the validation data. And then the other thing I would want to start doing is thinking about, well, does it actually work better? What are the hyperparameters that I can play with? For example, the hidden layer, I put 16 units in it. Or what happens if I use a different activation function for the hidden layer? What happens if I use more nodes or less nodes? What if I change the learning rate? What if I change the optimization function, if I use the atom optimization function? So all these things are things that I could play with and research and think about and experiment with to try to tune the model really well. Then at some point, I also would want to save that model, save it to a JSON file, so the trained model somehow, so that I could load it back in without having to run through the training process again. Maybe I'd even want a larger data set. I'd want to train it over a long time. Maybe I want to port this code to Node so I could let it train server-side without having to train in the client. There's so many possibilities. But I have now built a machine learning model with TensorFlow.js. I'm going to cry. I don't know how many videos this took. That trains a model based on crowdsource color data. And if you want, just humor me for a second. If you remember, if I go here, this is the system. This system was used to allow people from the internet to click on and say, that's pinkish, that's greenish, that's bluish, tag a whole bunch of colors, save all that data in a Firebase database, retrieve all that data, clean that data, put it in a JSON file, load that JSON file here into this sketch, build a model, train the model with that data, and then pull a new color from a slider. Oh, and I've forgotten something. Memory management. Oh, I knew there was a step that I'm missing. Estimating what category out of the fixed set of labels that color is. But I did forget something really quite important, which is memory management. Let's look at this. Num. Memory. tf.memory.numTensors. So again, when I create tensors that are allocated to memory on the GPU to store numbers, those don't get cleaned up automatically. There's no garbage collector like in regular JavaScript programming. So there are 15,485 tensors. Now, one thing, and there's still even more and more and more. It's growing. This is a memory leak. So one place where I didn't clean up any of the tensors is right here. And there's an easy way I can clean this up by adding in the tf.tidy function. So what tf.tidy does is it says, just put all of this code that's inside of this function passed into tf.tidy. Clean up any tensors that are made there. So this will clean up everything for me. So now let's run this again. And we're going to take a look at the tensors. There's 31, 73. It's kind of leaking, right? Well, let's let it get all the way through 10 epochs. I'll be back in a minute when that finishes. So the training is complete. And we can see now, ah, there we go. I am no longer leaking tensors. Now, the thing is, did I really need 1,628 tensors? I don't think that I did. I think there is also a leak going on inside of this train function. And I think there's an issue with this. And so I might have to do a follow-up video about this. Because at the moment, if I go to github.com tf. Oh, hold on. Let's go from here to, I should have had this prepared. Where do I go? GitHub and Issues. And I'm going to look for a fit memory leak, this one. So I believe there is, at present, a memory leak in model.fit with callbacks. And you can see that's exactly what I'm doing, right? Where model.fit with callbacks. So I'm going to not worry about that particular memory leak right now. I'm going to wait for us to see if that gets corrected. By the time you're watching this, that might already be corrected. And this code might have no more memory leaks in it just by updating the version of TensorFlow.js. Or I might still be missing something in here to do a memory leak. So if you don't want any spoilers, the following videos have not been published yet. You could sort that out yourself. But I will come back at some point and talk about that. Thank you for watching. I wish you many purplish and pinkish and bluish and greenish days. All the colors of the rainbow, may they fill your days with joy. May you make your own classifier with your own data. Please share it with me. I don't know. Has this helped the world, this tutorial series? I've missed so much about data and data collection and machine learning and models and algorithms. But hopefully, I've done something. This is not the end. It's only the beginning. I'll see you soon in future tutorial videos. Because this playlist probably has about 300 more left in it. OK, goodbye. You",
    "translation": null
  },
  "error": null,
  "status": "succeeded",
  "created_at": "2023-09-26T21:03:56.584266Z",
  "started_at": "2023-09-26T21:23:01.024051Z",
  "completed_at": "2023-09-26T21:27:50.25303Z",
  "webhook": "https://83ceaa0b612c.ngrok.app/?video_id=lz2L-sT8bG0",
  "webhook_events_filter": [
    "completed"
  ],
  "metrics": {
    "predict_time": 289.228979
  },
  "urls": {
    "cancel": "https://api.replicate.com/v1/predictions/32wud5zbttncrp7m5yi7uj7cue/cancel",
    "get": "https://api.replicate.com/v1/predictions/32wud5zbttncrp7m5yi7uj7cue"
  }
}