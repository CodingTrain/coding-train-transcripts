{
  "id": "wbgfnerbdvmkldcby5xmonssvu",
  "version": "91ee9c0c3df30478510ff8c8a3a545add1ad0259ad3a9f78fba57fbc05ee64f7",
  "input": {
    "audio": "https://upcdn.io/FW25b4F/raw/coding-train/tE-ZYXU8A8U.m4a"
  },
  "logs": "Transcribe with large-v2 model\nDetected language: English\n  0%|          | 0/81952 [00:00<?, ?frames/s]\n  3%|▎         | 2684/81952 [00:04<02:07, 621.48frames/s]\n  6%|▋         | 5152/81952 [00:09<02:29, 512.06frames/s]\n 10%|▉         | 8132/81952 [00:17<02:45, 445.61frames/s]\n 13%|█▎        | 10824/81952 [00:22<02:32, 467.32frames/s]\n 16%|█▋        | 13360/81952 [00:28<02:29, 459.93frames/s]\n 20%|█▉        | 16168/81952 [00:35<02:29, 439.95frames/s]\n 23%|██▎       | 18952/81952 [00:41<02:25, 434.04frames/s]\n 27%|██▋       | 21776/81952 [00:47<02:13, 449.34frames/s]\n 30%|██▉       | 24560/81952 [00:53<02:04, 461.26frames/s]\n 33%|███▎      | 27040/81952 [00:57<01:53, 481.70frames/s]\n 36%|███▌      | 29568/81952 [01:02<01:43, 504.65frames/s]\n 39%|███▉      | 32344/81952 [01:08<01:42, 486.09frames/s]\n 42%|████▏     | 34696/81952 [01:13<01:37, 484.58frames/s]\n 45%|████▌     | 37000/81952 [01:18<01:33, 478.55frames/s]\n 48%|████▊     | 39008/81952 [01:22<01:29, 480.91frames/s]\n 50%|█████     | 41368/81952 [01:26<01:21, 495.92frames/s]\n 54%|█████▍    | 44064/81952 [01:31<01:10, 535.85frames/s]\n 57%|█████▋    | 46792/81952 [01:36<01:06, 527.24frames/s]\n 60%|█████▉    | 49128/81952 [01:41<01:03, 517.44frames/s]\n 63%|██████▎   | 51632/81952 [01:46<01:01, 489.17frames/s]\n 66%|██████▌   | 54160/81952 [01:51<00:55, 498.60frames/s]\n 69%|██████▉   | 56704/81952 [01:58<00:55, 451.02frames/s]\n 72%|███████▏  | 59272/81952 [02:03<00:47, 476.32frames/s]\n 76%|███████▌  | 62112/81952 [02:09<00:41, 479.48frames/s]\n 79%|███████▉  | 65088/81952 [02:16<00:37, 446.28frames/s]\n 83%|████████▎ | 67832/81952 [02:23<00:32, 439.09frames/s]\n 86%|████████▌ | 70568/81952 [02:28<00:25, 453.39frames/s]\n 89%|████████▉ | 73248/81952 [02:35<00:19, 444.35frames/s]\n 93%|█████████▎| 76216/81952 [02:41<00:12, 449.34frames/s]\n 96%|█████████▋| 79056/81952 [02:48<00:06, 433.68frames/s]\n 99%|█████████▊| 80864/81952 [02:53<00:02, 421.55frames/s]\n99%|█████████▊| 80864/81952 [02:58<00:02, 453.31frames/s]\n",
  "output": {
    "detected_language": "english",
    "segments": [
      {
        "avg_logprob": -0.3889879768277392,
        "compression_ratio": 1.5502392344497609,
        "end": 8.24,
        "id": 0,
        "no_speech_prob": 0.0004511344013735652,
        "seek": 0,
        "start": 0,
        "temperature": 0,
        "text": " Hello, welcome to session 5 of programming from A to Z, which is like a thing that I'm doing on the internet, on YouTube",
        "tokens": [
          50364,
          2425,
          11,
          2928,
          281,
          5481,
          1025,
          295,
          9410,
          490,
          316,
          281,
          1176,
          11,
          597,
          307,
          411,
          257,
          551,
          300,
          286,
          478,
          884,
          322,
          264,
          4705,
          11,
          322,
          3088,
          50776
        ]
      },
      {
        "avg_logprob": -0.3889879768277392,
        "compression_ratio": 1.5502392344497609,
        "end": 10.72,
        "id": 1,
        "no_speech_prob": 0.0004511344013735652,
        "seek": 0,
        "start": 8.8,
        "temperature": 0,
        "text": " some tutorials",
        "tokens": [
          50804,
          512,
          17616,
          50900
        ]
      },
      {
        "avg_logprob": -0.3889879768277392,
        "compression_ratio": 1.5502392344497609,
        "end": 16.76,
        "id": 2,
        "no_speech_prob": 0.0004511344013735652,
        "seek": 0,
        "start": 10.72,
        "temperature": 0,
        "text": " topics around programming with text. Now on the one hand you could divide",
        "tokens": [
          50900,
          8378,
          926,
          9410,
          365,
          2487,
          13,
          823,
          322,
          264,
          472,
          1011,
          291,
          727,
          9845,
          51202
        ]
      },
      {
        "avg_logprob": -0.3889879768277392,
        "compression_ratio": 1.5502392344497609,
        "end": 21.56,
        "id": 3,
        "no_speech_prob": 0.0004511344013735652,
        "seek": 0,
        "start": 17.44,
        "temperature": 0,
        "text": " this course, this set of playlists together into two parts.",
        "tokens": [
          51236,
          341,
          1164,
          11,
          341,
          992,
          295,
          862,
          36693,
          1214,
          666,
          732,
          3166,
          13,
          51442
        ]
      },
      {
        "avg_logprob": -0.3889879768277392,
        "compression_ratio": 1.5502392344497609,
        "end": 23.92,
        "id": 4,
        "no_speech_prob": 0.0004511344013735652,
        "seek": 0,
        "start": 22.2,
        "temperature": 0,
        "text": " There's",
        "tokens": [
          51474,
          821,
          311,
          51560
        ]
      },
      {
        "avg_logprob": -0.3889879768277392,
        "compression_ratio": 1.5502392344497609,
        "end": 26.84,
        "id": 5,
        "no_speech_prob": 0.0004511344013735652,
        "seek": 0,
        "start": 23.92,
        "temperature": 0,
        "text": " analyzing text and there's generating text and",
        "tokens": [
          51560,
          23663,
          2487,
          293,
          456,
          311,
          17746,
          2487,
          293,
          51706
        ]
      },
      {
        "avg_logprob": -0.324549066452753,
        "compression_ratio": 1.669291338582677,
        "end": 28.12,
        "id": 6,
        "no_speech_prob": 0.00005390988371800631,
        "seek": 2684,
        "start": 27.16,
        "temperature": 0,
        "text": " and",
        "tokens": [
          50380,
          293,
          50428
        ]
      },
      {
        "avg_logprob": -0.324549066452753,
        "compression_ratio": 1.669291338582677,
        "end": 32.16,
        "id": 7,
        "no_speech_prob": 0.00005390988371800631,
        "seek": 2684,
        "start": 28.12,
        "temperature": 0,
        "text": " today's session is really about, and you can also think of that as reading text and",
        "tokens": [
          50428,
          965,
          311,
          5481,
          307,
          534,
          466,
          11,
          293,
          291,
          393,
          611,
          519,
          295,
          300,
          382,
          3760,
          2487,
          293,
          50630
        ]
      },
      {
        "avg_logprob": -0.324549066452753,
        "compression_ratio": 1.669291338582677,
        "end": 38.8,
        "id": 8,
        "no_speech_prob": 0.00005390988371800631,
        "seek": 2684,
        "start": 32.64,
        "temperature": 0,
        "text": " writing text, right? And most of the projects that I've been demonstrating or that I will show or talk about do some",
        "tokens": [
          50654,
          3579,
          2487,
          11,
          558,
          30,
          400,
          881,
          295,
          264,
          4455,
          300,
          286,
          600,
          668,
          29889,
          420,
          300,
          286,
          486,
          855,
          420,
          751,
          466,
          360,
          512,
          50962
        ]
      },
      {
        "avg_logprob": -0.324549066452753,
        "compression_ratio": 1.669291338582677,
        "end": 44.4,
        "id": 9,
        "no_speech_prob": 0.00005390988371800631,
        "seek": 2684,
        "start": 38.88,
        "temperature": 0,
        "text": " combination of both. Maybe they read in a source material, mash it up, chop it up and generate something from it.",
        "tokens": [
          50966,
          6562,
          295,
          1293,
          13,
          2704,
          436,
          1401,
          294,
          257,
          4009,
          2527,
          11,
          31344,
          309,
          493,
          11,
          7931,
          309,
          493,
          293,
          8460,
          746,
          490,
          309,
          13,
          51242
        ]
      },
      {
        "avg_logprob": -0.324549066452753,
        "compression_ratio": 1.669291338582677,
        "end": 51.519999999999996,
        "id": 10,
        "no_speech_prob": 0.00005390988371800631,
        "seek": 2684,
        "start": 45.28,
        "temperature": 0,
        "text": " Today though I really want to focus on the reading in of text, the analysis of text. And when I say today",
        "tokens": [
          51286,
          2692,
          1673,
          286,
          534,
          528,
          281,
          1879,
          322,
          264,
          3760,
          294,
          295,
          2487,
          11,
          264,
          5215,
          295,
          2487,
          13,
          400,
          562,
          286,
          584,
          965,
          51598
        ]
      },
      {
        "avg_logprob": -0.2768818342766794,
        "compression_ratio": 1.8167701863354038,
        "end": 53.52,
        "id": 11,
        "no_speech_prob": 0.0038241574075073004,
        "seek": 5152,
        "start": 51.52,
        "temperature": 0,
        "text": " I mean what I'm doing right now.",
        "tokens": [
          50364,
          286,
          914,
          437,
          286,
          478,
          884,
          558,
          586,
          13,
          50464
        ]
      },
      {
        "avg_logprob": -0.2768818342766794,
        "compression_ratio": 1.8167701863354038,
        "end": 59.120000000000005,
        "id": 12,
        "no_speech_prob": 0.0038241574075073004,
        "seek": 5152,
        "start": 53.760000000000005,
        "temperature": 0,
        "text": " But ultimately the set of tutorials that I'm going to make. So if you, I'm going to give you a sort of summary of",
        "tokens": [
          50476,
          583,
          6284,
          264,
          992,
          295,
          17616,
          300,
          286,
          478,
          516,
          281,
          652,
          13,
          407,
          498,
          291,
          11,
          286,
          478,
          516,
          281,
          976,
          291,
          257,
          1333,
          295,
          12691,
          295,
          50744
        ]
      },
      {
        "avg_logprob": -0.2768818342766794,
        "compression_ratio": 1.8167701863354038,
        "end": 63.480000000000004,
        "id": 13,
        "no_speech_prob": 0.0038241574075073004,
        "seek": 5152,
        "start": 59.120000000000005,
        "temperature": 0,
        "text": " the topic. I'm going to show you some relevant projects that might inspire you with ideas.",
        "tokens": [
          50744,
          264,
          4829,
          13,
          286,
          478,
          516,
          281,
          855,
          291,
          512,
          7340,
          4455,
          300,
          1062,
          15638,
          291,
          365,
          3487,
          13,
          50962
        ]
      },
      {
        "avg_logprob": -0.2768818342766794,
        "compression_ratio": 1.8167701863354038,
        "end": 67.88,
        "id": 14,
        "no_speech_prob": 0.0038241574075073004,
        "seek": 5152,
        "start": 63.480000000000004,
        "temperature": 0,
        "text": " But if you want to just get to the coding part you can skip to the next video in this playlist.",
        "tokens": [
          50962,
          583,
          498,
          291,
          528,
          281,
          445,
          483,
          281,
          264,
          17720,
          644,
          291,
          393,
          10023,
          281,
          264,
          958,
          960,
          294,
          341,
          16788,
          13,
          51182
        ]
      },
      {
        "avg_logprob": -0.2768818342766794,
        "compression_ratio": 1.8167701863354038,
        "end": 71.48,
        "id": 15,
        "no_speech_prob": 0.0038241574075073004,
        "seek": 5152,
        "start": 67.88,
        "temperature": 0,
        "text": " You should be in the session 5 or week 5 programming from A to Z playlist.",
        "tokens": [
          51182,
          509,
          820,
          312,
          294,
          264,
          5481,
          1025,
          420,
          1243,
          1025,
          9410,
          490,
          316,
          281,
          1176,
          16788,
          13,
          51362
        ]
      },
      {
        "avg_logprob": -0.2768818342766794,
        "compression_ratio": 1.8167701863354038,
        "end": 75.60000000000001,
        "id": 16,
        "no_speech_prob": 0.0038241574075073004,
        "seek": 5152,
        "start": 71.56,
        "temperature": 0,
        "text": " So the building block, so text analysis, by text analysis I mean",
        "tokens": [
          51366,
          407,
          264,
          2390,
          3461,
          11,
          370,
          2487,
          5215,
          11,
          538,
          2487,
          5215,
          286,
          914,
          51568
        ]
      },
      {
        "avg_logprob": -0.2768818342766794,
        "compression_ratio": 1.8167701863354038,
        "end": 81.32000000000001,
        "id": 17,
        "no_speech_prob": 0.0038241574075073004,
        "seek": 5152,
        "start": 76.24000000000001,
        "temperature": 0,
        "text": " reading in a text, analyzing it and producing some result. That could, that result could be sentiment analysis.",
        "tokens": [
          51600,
          3760,
          294,
          257,
          2487,
          11,
          23663,
          309,
          293,
          10501,
          512,
          1874,
          13,
          663,
          727,
          11,
          300,
          1874,
          727,
          312,
          16149,
          5215,
          13,
          51854
        ]
      },
      {
        "avg_logprob": -0.33530611204869537,
        "compression_ratio": 1.71875,
        "end": 89.52,
        "id": 18,
        "no_speech_prob": 0.00004006336894235574,
        "seek": 8132,
        "start": 81.32,
        "temperature": 0,
        "text": " It's a positive text as a negative text. It could be ah this text is very similar in style to another text.",
        "tokens": [
          50364,
          467,
          311,
          257,
          3353,
          2487,
          382,
          257,
          3671,
          2487,
          13,
          467,
          727,
          312,
          3716,
          341,
          2487,
          307,
          588,
          2531,
          294,
          3758,
          281,
          1071,
          2487,
          13,
          50774
        ]
      },
      {
        "avg_logprob": -0.33530611204869537,
        "compression_ratio": 1.71875,
        "end": 95,
        "id": 19,
        "no_speech_prob": 0.00004006336894235574,
        "seek": 8132,
        "start": 89.52,
        "temperature": 0,
        "text": " There are so many different kinds of ways you can about this. This is a difficult text, an easy text.",
        "tokens": [
          50774,
          821,
          366,
          370,
          867,
          819,
          3685,
          295,
          2098,
          291,
          393,
          466,
          341,
          13,
          639,
          307,
          257,
          2252,
          2487,
          11,
          364,
          1858,
          2487,
          13,
          51048
        ]
      },
      {
        "avg_logprob": -0.33530611204869537,
        "compression_ratio": 1.71875,
        "end": 101.91999999999999,
        "id": 20,
        "no_speech_prob": 0.00004006336894235574,
        "seek": 8132,
        "start": 95,
        "temperature": 0,
        "text": " You can imagine what kinds of outcome, statistical analysis, whatever you could do from reading in text with a computer program.",
        "tokens": [
          51048,
          509,
          393,
          3811,
          437,
          3685,
          295,
          9700,
          11,
          22820,
          5215,
          11,
          2035,
          291,
          727,
          360,
          490,
          3760,
          294,
          2487,
          365,
          257,
          3820,
          1461,
          13,
          51394
        ]
      },
      {
        "avg_logprob": -0.33530611204869537,
        "compression_ratio": 1.71875,
        "end": 108.24,
        "id": 21,
        "no_speech_prob": 0.00004006336894235574,
        "seek": 8132,
        "start": 102.08,
        "temperature": 0,
        "text": " But the, we need a building block. And the building block for every single one of these text analysis",
        "tokens": [
          51402,
          583,
          264,
          11,
          321,
          643,
          257,
          2390,
          3461,
          13,
          400,
          264,
          2390,
          3461,
          337,
          633,
          2167,
          472,
          295,
          613,
          2487,
          5215,
          51710
        ]
      },
      {
        "avg_logprob": -0.31188909357244315,
        "compression_ratio": 1.783132530120482,
        "end": 113.36,
        "id": 22,
        "no_speech_prob": 0.05748741328716278,
        "seek": 10824,
        "start": 108.64,
        "temperature": 0,
        "text": " examples and scenarios that I will present and show you how to code is word counting.",
        "tokens": [
          50384,
          5110,
          293,
          15077,
          300,
          286,
          486,
          1974,
          293,
          855,
          291,
          577,
          281,
          3089,
          307,
          1349,
          13251,
          13,
          50620
        ]
      },
      {
        "avg_logprob": -0.31188909357244315,
        "compression_ratio": 1.783132530120482,
        "end": 116.88,
        "id": 23,
        "no_speech_prob": 0.05748741328716278,
        "seek": 10824,
        "start": 113.36,
        "temperature": 0,
        "text": " Actually word counting is a totally simple thing and this is not a new thing.",
        "tokens": [
          50620,
          5135,
          1349,
          13251,
          307,
          257,
          3879,
          2199,
          551,
          293,
          341,
          307,
          406,
          257,
          777,
          551,
          13,
          50796
        ]
      },
      {
        "avg_logprob": -0.31188909357244315,
        "compression_ratio": 1.783132530120482,
        "end": 122.67999999999999,
        "id": 24,
        "no_speech_prob": 0.05748741328716278,
        "seek": 10824,
        "start": 116.88,
        "temperature": 0,
        "text": " So I'm going to pop over to this Wikipedia page here on this idea of a concordance. So concordance",
        "tokens": [
          50796,
          407,
          286,
          478,
          516,
          281,
          1665,
          670,
          281,
          341,
          28999,
          3028,
          510,
          322,
          341,
          1558,
          295,
          257,
          1588,
          765,
          719,
          13,
          407,
          1588,
          765,
          719,
          51086
        ]
      },
      {
        "avg_logprob": -0.31188909357244315,
        "compression_ratio": 1.783132530120482,
        "end": 131.12,
        "id": 25,
        "no_speech_prob": 0.05748741328716278,
        "seek": 10824,
        "start": 124.56,
        "temperature": 0,
        "text": " Concordance, concordance. A concordance is an alphabetical list of the principal words used in a book or body of work",
        "tokens": [
          51180,
          18200,
          765,
          719,
          11,
          1588,
          765,
          719,
          13,
          316,
          1588,
          765,
          719,
          307,
          364,
          23339,
          804,
          1329,
          295,
          264,
          9716,
          2283,
          1143,
          294,
          257,
          1446,
          420,
          1772,
          295,
          589,
          51508
        ]
      },
      {
        "avg_logprob": -0.31188909357244315,
        "compression_ratio": 1.783132530120482,
        "end": 133.6,
        "id": 26,
        "no_speech_prob": 0.05748741328716278,
        "seek": 10824,
        "start": 131.2,
        "temperature": 0,
        "text": " listing every instance of each word with its immediate context.",
        "tokens": [
          51512,
          22161,
          633,
          5197,
          295,
          1184,
          1349,
          365,
          1080,
          11629,
          4319,
          13,
          51632
        ]
      },
      {
        "avg_logprob": -0.3416550019208123,
        "compression_ratio": 2.03041825095057,
        "end": 140.76,
        "id": 27,
        "no_speech_prob": 0.028434261679649353,
        "seek": 13360,
        "start": 133.76,
        "temperature": 0,
        "text": " Blah blah blah blah blah. What that means is a concordance is hey I want to know all the words that were in this text",
        "tokens": [
          50372,
          2177,
          545,
          12288,
          12288,
          12288,
          12288,
          13,
          708,
          300,
          1355,
          307,
          257,
          1588,
          765,
          719,
          307,
          4177,
          286,
          528,
          281,
          458,
          439,
          264,
          2283,
          300,
          645,
          294,
          341,
          2487,
          50722
        ]
      },
      {
        "avg_logprob": -0.3416550019208123,
        "compression_ratio": 2.03041825095057,
        "end": 143.4,
        "id": 28,
        "no_speech_prob": 0.028434261679649353,
        "seek": 13360,
        "start": 140.76,
        "temperature": 0,
        "text": " and I want to know how many times they appeared and where they appeared.",
        "tokens": [
          50722,
          293,
          286,
          528,
          281,
          458,
          577,
          867,
          1413,
          436,
          8516,
          293,
          689,
          436,
          8516,
          13,
          50854
        ]
      },
      {
        "avg_logprob": -0.3416550019208123,
        "compression_ratio": 2.03041825095057,
        "end": 148.95999999999998,
        "id": 29,
        "no_speech_prob": 0.028434261679649353,
        "seek": 13360,
        "start": 143.4,
        "temperature": 0,
        "text": " So I'm going to do something simpler when I build an actual code example which is just here's a list of all the words in a text",
        "tokens": [
          50854,
          407,
          286,
          478,
          516,
          281,
          360,
          746,
          18587,
          562,
          286,
          1322,
          364,
          3539,
          3089,
          1365,
          597,
          307,
          445,
          510,
          311,
          257,
          1329,
          295,
          439,
          264,
          2283,
          294,
          257,
          2487,
          51132
        ]
      },
      {
        "avg_logprob": -0.3416550019208123,
        "compression_ratio": 2.03041825095057,
        "end": 155.84,
        "id": 30,
        "no_speech_prob": 0.028434261679649353,
        "seek": 13360,
        "start": 148.95999999999998,
        "temperature": 0,
        "text": " and how many times those words appeared. And this is not something that's just sort of a computing thing.",
        "tokens": [
          51132,
          293,
          577,
          867,
          1413,
          729,
          2283,
          8516,
          13,
          400,
          341,
          307,
          406,
          746,
          300,
          311,
          445,
          1333,
          295,
          257,
          15866,
          551,
          13,
          51476
        ]
      },
      {
        "avg_logprob": -0.3416550019208123,
        "compression_ratio": 2.03041825095057,
        "end": 161.68,
        "id": 31,
        "no_speech_prob": 0.028434261679649353,
        "seek": 13360,
        "start": 155.84,
        "temperature": 0,
        "text": " This has been done, you know, this has been done by hand many years with, you know, you know, well, you know,",
        "tokens": [
          51476,
          639,
          575,
          668,
          1096,
          11,
          291,
          458,
          11,
          341,
          575,
          668,
          1096,
          538,
          1011,
          867,
          924,
          365,
          11,
          291,
          458,
          11,
          291,
          458,
          11,
          731,
          11,
          291,
          458,
          11,
          51768
        ]
      },
      {
        "avg_logprob": -0.31089104872483475,
        "compression_ratio": 1.7013422818791946,
        "end": 165.68,
        "id": 32,
        "no_speech_prob": 0.007815096527338028,
        "seek": 16168,
        "start": 161.68,
        "temperature": 0,
        "text": " this used to be done by hand and so there's some information here about biblical concordances, you know,",
        "tokens": [
          50364,
          341,
          1143,
          281,
          312,
          1096,
          538,
          1011,
          293,
          370,
          456,
          311,
          512,
          1589,
          510,
          466,
          26083,
          1588,
          765,
          2676,
          11,
          291,
          458,
          11,
          50564
        ]
      },
      {
        "avg_logprob": -0.31089104872483475,
        "compression_ratio": 1.7013422818791946,
        "end": 171.84,
        "id": 33,
        "no_speech_prob": 0.007815096527338028,
        "seek": 16168,
        "start": 165.68,
        "temperature": 0,
        "text": " reading the text manually, making this big list, making references, you know, an index in a way is very similar to a concordance",
        "tokens": [
          50564,
          3760,
          264,
          2487,
          16945,
          11,
          1455,
          341,
          955,
          1329,
          11,
          1455,
          15400,
          11,
          291,
          458,
          11,
          364,
          8186,
          294,
          257,
          636,
          307,
          588,
          2531,
          281,
          257,
          1588,
          765,
          719,
          50872
        ]
      },
      {
        "avg_logprob": -0.31089104872483475,
        "compression_ratio": 1.7013422818791946,
        "end": 178.16,
        "id": 34,
        "no_speech_prob": 0.007815096527338028,
        "seek": 16168,
        "start": 171.84,
        "temperature": 0,
        "text": " in many ways as well. It's a sort of list of topics and where they appear in a particular book.",
        "tokens": [
          50872,
          294,
          867,
          2098,
          382,
          731,
          13,
          467,
          311,
          257,
          1333,
          295,
          1329,
          295,
          8378,
          293,
          689,
          436,
          4204,
          294,
          257,
          1729,
          1446,
          13,
          51188
        ]
      },
      {
        "avg_logprob": -0.31089104872483475,
        "compression_ratio": 1.7013422818791946,
        "end": 183.60000000000002,
        "id": 35,
        "no_speech_prob": 0.007815096527338028,
        "seek": 16168,
        "start": 178.16,
        "temperature": 0,
        "text": " So I encourage you, but you can see that here again this list I think is useful.",
        "tokens": [
          51188,
          407,
          286,
          5373,
          291,
          11,
          457,
          291,
          393,
          536,
          300,
          510,
          797,
          341,
          1329,
          286,
          519,
          307,
          4420,
          13,
          51460
        ]
      },
      {
        "avg_logprob": -0.31089104872483475,
        "compression_ratio": 1.7013422818791946,
        "end": 189.52,
        "id": 36,
        "no_speech_prob": 0.007815096527338028,
        "seek": 16168,
        "start": 183.60000000000002,
        "temperature": 0,
        "text": " Here I am. This is, hello, this is Daniel Shiffman reading you a Wikipedia page on the internet.",
        "tokens": [
          51460,
          1692,
          286,
          669,
          13,
          639,
          307,
          11,
          7751,
          11,
          341,
          307,
          8033,
          1160,
          3661,
          1601,
          3760,
          291,
          257,
          28999,
          3028,
          322,
          264,
          4705,
          13,
          51756
        ]
      },
      {
        "avg_logprob": -0.3151239146356997,
        "compression_ratio": 1.7706093189964158,
        "end": 197.04000000000002,
        "id": 37,
        "no_speech_prob": 0.0007096395711414516,
        "seek": 18952,
        "start": 189.60000000000002,
        "temperature": 0,
        "text": " But this is kind of useful context and it has some nice, it's kind of, it has some nice reference material for you to like",
        "tokens": [
          50368,
          583,
          341,
          307,
          733,
          295,
          4420,
          4319,
          293,
          309,
          575,
          512,
          1481,
          11,
          309,
          311,
          733,
          295,
          11,
          309,
          575,
          512,
          1481,
          6408,
          2527,
          337,
          291,
          281,
          411,
          50740
        ]
      },
      {
        "avg_logprob": -0.3151239146356997,
        "compression_ratio": 1.7706093189964158,
        "end": 204.16000000000003,
        "id": 38,
        "no_speech_prob": 0.0007096395711414516,
        "seek": 18952,
        "start": 197.04000000000002,
        "temperature": 0,
        "text": " kind of expand further into this territory. But you can see this idea of analyzing, one thing you can do with a concordance",
        "tokens": [
          50740,
          733,
          295,
          5268,
          3052,
          666,
          341,
          11360,
          13,
          583,
          291,
          393,
          536,
          341,
          1558,
          295,
          23663,
          11,
          472,
          551,
          291,
          393,
          360,
          365,
          257,
          1588,
          765,
          719,
          51096
        ]
      },
      {
        "avg_logprob": -0.3151239146356997,
        "compression_ratio": 1.7706093189964158,
        "end": 210.72,
        "id": 39,
        "no_speech_prob": 0.0007096395711414516,
        "seek": 18952,
        "start": 204.16000000000003,
        "temperature": 0,
        "text": " is try to figure out are there key words associated with a piece of text. One of the examples I'll show you is a term frequency",
        "tokens": [
          51096,
          307,
          853,
          281,
          2573,
          484,
          366,
          456,
          2141,
          2283,
          6615,
          365,
          257,
          2522,
          295,
          2487,
          13,
          1485,
          295,
          264,
          5110,
          286,
          603,
          855,
          291,
          307,
          257,
          1433,
          7893,
          51424
        ]
      },
      {
        "avg_logprob": -0.3151239146356997,
        "compression_ratio": 1.7706093189964158,
        "end": 217.76000000000002,
        "id": 40,
        "no_speech_prob": 0.0007096395711414516,
        "seek": 18952,
        "start": 210.72,
        "temperature": 0,
        "text": " inverse document frequency algorithm that pulls out key words from a text and all sorts of other things that you can do",
        "tokens": [
          51424,
          17340,
          4166,
          7893,
          9284,
          300,
          16982,
          484,
          2141,
          2283,
          490,
          257,
          2487,
          293,
          439,
          7527,
          295,
          661,
          721,
          300,
          291,
          393,
          360,
          51776
        ]
      },
      {
        "avg_logprob": -0.27982737566973714,
        "compression_ratio": 1.7423076923076923,
        "end": 224,
        "id": 41,
        "no_speech_prob": 0.08034049719572067,
        "seek": 21776,
        "start": 218,
        "temperature": 0,
        "text": " with a concordance algorithm. So let me show you some examples. So first of all, here's a great example.",
        "tokens": [
          50376,
          365,
          257,
          1588,
          765,
          719,
          9284,
          13,
          407,
          718,
          385,
          855,
          291,
          512,
          5110,
          13,
          407,
          700,
          295,
          439,
          11,
          510,
          311,
          257,
          869,
          1365,
          13,
          50676
        ]
      },
      {
        "avg_logprob": -0.27982737566973714,
        "compression_ratio": 1.7423076923076923,
        "end": 228.16,
        "id": 42,
        "no_speech_prob": 0.08034049719572067,
        "seek": 21776,
        "start": 224,
        "temperature": 0,
        "text": " This is by Rune Madsen and they'll be in this video's description, there'll be links to all these projects.",
        "tokens": [
          50676,
          639,
          307,
          538,
          497,
          2613,
          5326,
          6748,
          293,
          436,
          603,
          312,
          294,
          341,
          960,
          311,
          3855,
          11,
          456,
          603,
          312,
          6123,
          281,
          439,
          613,
          4455,
          13,
          50884
        ]
      },
      {
        "avg_logprob": -0.27982737566973714,
        "compression_ratio": 1.7423076923076923,
        "end": 236.16,
        "id": 43,
        "no_speech_prob": 0.08034049719572067,
        "seek": 21776,
        "start": 228.16,
        "temperature": 0,
        "text": " This is called speech comparison. It's a project created in processing and this is a kind of what you might typically see",
        "tokens": [
          50884,
          639,
          307,
          1219,
          6218,
          9660,
          13,
          467,
          311,
          257,
          1716,
          2942,
          294,
          9007,
          293,
          341,
          307,
          257,
          733,
          295,
          437,
          291,
          1062,
          5850,
          536,
          51284
        ]
      },
      {
        "avg_logprob": -0.27982737566973714,
        "compression_ratio": 1.7423076923076923,
        "end": 245.6,
        "id": 44,
        "no_speech_prob": 0.08034049719572067,
        "seek": 21776,
        "start": 236.16,
        "temperature": 0,
        "text": " as a kind of example project idea with a text concordance. This is visualizing a bunch of speeches by these particular",
        "tokens": [
          51284,
          382,
          257,
          733,
          295,
          1365,
          1716,
          1558,
          365,
          257,
          2487,
          1588,
          765,
          719,
          13,
          639,
          307,
          5056,
          3319,
          257,
          3840,
          295,
          29982,
          538,
          613,
          1729,
          51756
        ]
      },
      {
        "avg_logprob": -0.310580622066151,
        "compression_ratio": 1.7766990291262137,
        "end": 254.64,
        "id": 45,
        "no_speech_prob": 0.005468436516821384,
        "seek": 24560,
        "start": 246.56,
        "temperature": 0,
        "text": " speakers and counting words and you can see here a list of words that appear in the text and then drawing a color coded",
        "tokens": [
          50412,
          9518,
          293,
          13251,
          2283,
          293,
          291,
          393,
          536,
          510,
          257,
          1329,
          295,
          2283,
          300,
          4204,
          294,
          264,
          2487,
          293,
          550,
          6316,
          257,
          2017,
          34874,
          50816
        ]
      },
      {
        "avg_logprob": -0.310580622066151,
        "compression_ratio": 1.7766990291262137,
        "end": 264.32,
        "id": 46,
        "no_speech_prob": 0.005468436516821384,
        "seek": 24560,
        "start": 254.64,
        "temperature": 0,
        "text": " visualization of the frequency of that term. And you can see here climate. Hmm, climate is used a lot by this purple person.",
        "tokens": [
          50816,
          25801,
          295,
          264,
          7893,
          295,
          300,
          1433,
          13,
          400,
          291,
          393,
          536,
          510,
          5659,
          13,
          8239,
          11,
          5659,
          307,
          1143,
          257,
          688,
          538,
          341,
          9656,
          954,
          13,
          51300
        ]
      },
      {
        "avg_logprob": -0.310580622066151,
        "compression_ratio": 1.7766990291262137,
        "end": 270.4,
        "id": 47,
        "no_speech_prob": 0.005468436516821384,
        "seek": 24560,
        "start": 264.32,
        "temperature": 0,
        "text": " I like to think of myself as a purple person. I don't know what color you think of yourself, I think of myself as purple.",
        "tokens": [
          51300,
          286,
          411,
          281,
          519,
          295,
          2059,
          382,
          257,
          9656,
          954,
          13,
          286,
          500,
          380,
          458,
          437,
          2017,
          291,
          519,
          295,
          1803,
          11,
          286,
          519,
          295,
          2059,
          382,
          9656,
          13,
          51604
        ]
      },
      {
        "avg_logprob": -0.32299405052548363,
        "compression_ratio": 1.5739130434782609,
        "end": 279.59999999999997,
        "id": 48,
        "no_speech_prob": 0.03307817131280899,
        "seek": 27040,
        "start": 270.71999999999997,
        "temperature": 0,
        "text": " And you can see that person is Al Gore, well known and famous for his work in climate, I was going to say climate science",
        "tokens": [
          50380,
          400,
          291,
          393,
          536,
          300,
          954,
          307,
          967,
          45450,
          11,
          731,
          2570,
          293,
          4618,
          337,
          702,
          589,
          294,
          5659,
          11,
          286,
          390,
          516,
          281,
          584,
          5659,
          3497,
          50824
        ]
      },
      {
        "avg_logprob": -0.32299405052548363,
        "compression_ratio": 1.5739130434782609,
        "end": 285.12,
        "id": 49,
        "no_speech_prob": 0.03307817131280899,
        "seek": 27040,
        "start": 279.59999999999997,
        "temperature": 0,
        "text": " but I don't know if that's accurate, promotion or getting a message out there about the issue of climate change.",
        "tokens": [
          50824,
          457,
          286,
          500,
          380,
          458,
          498,
          300,
          311,
          8559,
          11,
          15783,
          420,
          1242,
          257,
          3636,
          484,
          456,
          466,
          264,
          2734,
          295,
          5659,
          1319,
          13,
          51100
        ]
      },
      {
        "avg_logprob": -0.32299405052548363,
        "compression_ratio": 1.5739130434782609,
        "end": 295.67999999999995,
        "id": 50,
        "no_speech_prob": 0.03307817131280899,
        "seek": 27040,
        "start": 285.12,
        "temperature": 0,
        "text": " And so this is something you might think about doing. How could you creatively visualize the words that appear in a given text.",
        "tokens": [
          51100,
          400,
          370,
          341,
          307,
          746,
          291,
          1062,
          519,
          466,
          884,
          13,
          1012,
          727,
          291,
          43750,
          23273,
          264,
          2283,
          300,
          4204,
          294,
          257,
          2212,
          2487,
          13,
          51628
        ]
      },
      {
        "avg_logprob": -0.29690898259480797,
        "compression_ratio": 1.7978339350180506,
        "end": 303.28000000000003,
        "id": 51,
        "no_speech_prob": 0.19926847517490387,
        "seek": 29568,
        "start": 296.40000000000003,
        "temperature": 0,
        "text": " Let me show you another project by Sarah Groff Palermo. This project is called BookBook and I encourage you to take a look",
        "tokens": [
          50400,
          961,
          385,
          855,
          291,
          1071,
          1716,
          538,
          9519,
          12981,
          602,
          6116,
          46096,
          13,
          639,
          1716,
          307,
          1219,
          9476,
          19203,
          293,
          286,
          5373,
          291,
          281,
          747,
          257,
          574,
          50744
        ]
      },
      {
        "avg_logprob": -0.29690898259480797,
        "compression_ratio": 1.7978339350180506,
        "end": 309.76,
        "id": 52,
        "no_speech_prob": 0.19926847517490387,
        "seek": 29568,
        "start": 303.28000000000003,
        "temperature": 0,
        "text": " more deeply at this project but I'm going to click here. This is a comparison of The Jungle by Sinclair versus The Jungle Book",
        "tokens": [
          50744,
          544,
          8760,
          412,
          341,
          1716,
          457,
          286,
          478,
          516,
          281,
          2052,
          510,
          13,
          639,
          307,
          257,
          9660,
          295,
          440,
          44021,
          538,
          318,
          4647,
          24319,
          5717,
          440,
          44021,
          9476,
          51068
        ]
      },
      {
        "avg_logprob": -0.29690898259480797,
        "compression_ratio": 1.7978339350180506,
        "end": 316.96000000000004,
        "id": 53,
        "no_speech_prob": 0.19926847517490387,
        "seek": 29568,
        "start": 309.76,
        "temperature": 0,
        "text": " by Kipling and I'm going to click on this and what you're seeing here is what are the words that are unique to one text",
        "tokens": [
          51068,
          538,
          17459,
          11970,
          293,
          286,
          478,
          516,
          281,
          2052,
          322,
          341,
          293,
          437,
          291,
          434,
          2577,
          510,
          307,
          437,
          366,
          264,
          2283,
          300,
          366,
          3845,
          281,
          472,
          2487,
          51428
        ]
      },
      {
        "avg_logprob": -0.29690898259480797,
        "compression_ratio": 1.7978339350180506,
        "end": 323.44,
        "id": 54,
        "no_speech_prob": 0.19926847517490387,
        "seek": 29568,
        "start": 316.96000000000004,
        "temperature": 0,
        "text": " but not in another text. So this is something you could also really do with word counting and look at a comparison of two texts.",
        "tokens": [
          51428,
          457,
          406,
          294,
          1071,
          2487,
          13,
          407,
          341,
          307,
          746,
          291,
          727,
          611,
          534,
          360,
          365,
          1349,
          13251,
          293,
          574,
          412,
          257,
          9660,
          295,
          732,
          15765,
          13,
          51752
        ]
      },
      {
        "avg_logprob": -0.2445059335359963,
        "compression_ratio": 1.872037914691943,
        "end": 331.6,
        "id": 55,
        "no_speech_prob": 0.0004044777015224099,
        "seek": 32344,
        "start": 323.84,
        "temperature": 0,
        "text": " By the way, this is how spam filtering works. Let's look at all of my emails which are not spam and count all the word frequencies in those",
        "tokens": [
          50384,
          3146,
          264,
          636,
          11,
          341,
          307,
          577,
          24028,
          30822,
          1985,
          13,
          961,
          311,
          574,
          412,
          439,
          295,
          452,
          12524,
          597,
          366,
          406,
          24028,
          293,
          1207,
          439,
          264,
          1349,
          20250,
          294,
          729,
          50772
        ]
      },
      {
        "avg_logprob": -0.2445059335359963,
        "compression_ratio": 1.872037914691943,
        "end": 339.44,
        "id": 56,
        "no_speech_prob": 0.0004044777015224099,
        "seek": 32344,
        "start": 331.6,
        "temperature": 0,
        "text": " and let's look at all of my emails that are spam and count all the frequencies in those and then build a statistical probability",
        "tokens": [
          50772,
          293,
          718,
          311,
          574,
          412,
          439,
          295,
          452,
          12524,
          300,
          366,
          24028,
          293,
          1207,
          439,
          264,
          20250,
          294,
          729,
          293,
          550,
          1322,
          257,
          22820,
          8482,
          51164
        ]
      },
      {
        "avg_logprob": -0.2445059335359963,
        "compression_ratio": 1.872037914691943,
        "end": 346.96,
        "id": 57,
        "no_speech_prob": 0.0004044777015224099,
        "seek": 32344,
        "start": 339.44,
        "temperature": 0,
        "text": " that a new email fits into one of those categories. And you can see it's not just about what kind of words are in a spam email",
        "tokens": [
          51164,
          300,
          257,
          777,
          3796,
          9001,
          666,
          472,
          295,
          729,
          10479,
          13,
          400,
          291,
          393,
          536,
          309,
          311,
          406,
          445,
          466,
          437,
          733,
          295,
          2283,
          366,
          294,
          257,
          24028,
          3796,
          51540
        ]
      },
      {
        "avg_logprob": -0.2916766687766793,
        "compression_ratio": 1.7918552036199096,
        "end": 355.03999999999996,
        "id": 58,
        "no_speech_prob": 0.08881119638681412,
        "seek": 34696,
        "start": 347.03999999999996,
        "temperature": 0,
        "text": " it's all about the relationship of spam to regular emails. One way to think about this by the way is I get a lot of spam emails",
        "tokens": [
          50368,
          309,
          311,
          439,
          466,
          264,
          2480,
          295,
          24028,
          281,
          3890,
          12524,
          13,
          1485,
          636,
          281,
          519,
          466,
          341,
          538,
          264,
          636,
          307,
          286,
          483,
          257,
          688,
          295,
          24028,
          12524,
          50768
        ]
      },
      {
        "avg_logprob": -0.2916766687766793,
        "compression_ratio": 1.7918552036199096,
        "end": 361.03999999999996,
        "id": 59,
        "no_speech_prob": 0.08881119638681412,
        "seek": 34696,
        "start": 355.03999999999996,
        "temperature": 0,
        "text": " that are trying to sell me like a mortgage. So mortgage is a word frequency that appears in a lot of my spam emails.",
        "tokens": [
          50768,
          300,
          366,
          1382,
          281,
          3607,
          385,
          411,
          257,
          20236,
          13,
          407,
          20236,
          307,
          257,
          1349,
          7893,
          300,
          7038,
          294,
          257,
          688,
          295,
          452,
          24028,
          12524,
          13,
          51068
        ]
      },
      {
        "avg_logprob": -0.2916766687766793,
        "compression_ratio": 1.7918552036199096,
        "end": 370,
        "id": 60,
        "no_speech_prob": 0.08881119638681412,
        "seek": 34696,
        "start": 361.03999999999996,
        "temperature": 0,
        "text": " So that's a good indicator but if I were a mortgage broker I'd probably get a lot of actual mortgage emails so that word wouldn't be as relevant to me.",
        "tokens": [
          51068,
          407,
          300,
          311,
          257,
          665,
          16961,
          457,
          498,
          286,
          645,
          257,
          20236,
          26502,
          286,
          1116,
          1391,
          483,
          257,
          688,
          295,
          3539,
          20236,
          12524,
          370,
          300,
          1349,
          2759,
          380,
          312,
          382,
          7340,
          281,
          385,
          13,
          51516
        ]
      },
      {
        "avg_logprob": -0.2989166309307148,
        "compression_ratio": 1.5471698113207548,
        "end": 377.6,
        "id": 61,
        "no_speech_prob": 0.11914004385471344,
        "seek": 37000,
        "start": 370.08,
        "temperature": 0,
        "text": " So just detecting just word frequency in one document but how it compares to others is kind of a key way to do text analysis and we'll see some examples of that.",
        "tokens": [
          50368,
          407,
          445,
          40237,
          445,
          1349,
          7893,
          294,
          472,
          4166,
          457,
          577,
          309,
          38334,
          281,
          2357,
          307,
          733,
          295,
          257,
          2141,
          636,
          281,
          360,
          2487,
          5215,
          293,
          321,
          603,
          536,
          512,
          5110,
          295,
          300,
          13,
          50744
        ]
      },
      {
        "avg_logprob": -0.2989166309307148,
        "compression_ratio": 1.5471698113207548,
        "end": 390.08,
        "id": 62,
        "no_speech_prob": 0.11914004385471344,
        "seek": 37000,
        "start": 377.6,
        "temperature": 0,
        "text": " You can also look, Sarah's project also has the ability for you to look at the words that appear in both texts I believe if I click over to see both of them together.",
        "tokens": [
          50744,
          509,
          393,
          611,
          574,
          11,
          9519,
          311,
          1716,
          611,
          575,
          264,
          3485,
          337,
          291,
          281,
          574,
          412,
          264,
          2283,
          300,
          4204,
          294,
          1293,
          15765,
          286,
          1697,
          498,
          286,
          2052,
          670,
          281,
          536,
          1293,
          295,
          552,
          1214,
          13,
          51368
        ]
      },
      {
        "avg_logprob": -0.3419305029369536,
        "compression_ratio": 1.6563876651982379,
        "end": 397.2,
        "id": 63,
        "no_speech_prob": 0.5847680568695068,
        "seek": 39008,
        "start": 390.15999999999997,
        "temperature": 0,
        "text": " Another wonderful work in data visualization, another artist that I love is Stephanie Posavec.",
        "tokens": [
          50368,
          3996,
          3715,
          589,
          294,
          1412,
          25801,
          11,
          1071,
          5748,
          300,
          286,
          959,
          307,
          18634,
          25906,
          946,
          66,
          13,
          50720
        ]
      },
      {
        "avg_logprob": -0.3419305029369536,
        "compression_ratio": 1.6563876651982379,
        "end": 407.68,
        "id": 64,
        "no_speech_prob": 0.5847680568695068,
        "seek": 39008,
        "start": 397.2,
        "temperature": 0,
        "text": " And Stephanie has done a lot of work with hand drawn data visualization. She has a beautiful project with Georgia Newby called Dear Data",
        "tokens": [
          50720,
          400,
          18634,
          575,
          1096,
          257,
          688,
          295,
          589,
          365,
          1011,
          10117,
          1412,
          25801,
          13,
          1240,
          575,
          257,
          2238,
          1716,
          365,
          11859,
          1873,
          2322,
          1219,
          14383,
          11888,
          51244
        ]
      },
      {
        "avg_logprob": -0.3419305029369536,
        "compression_ratio": 1.6563876651982379,
        "end": 413.68,
        "id": 65,
        "no_speech_prob": 0.5847680568695068,
        "seek": 39008,
        "start": 407.68,
        "temperature": 0,
        "text": " which they sent these data visualization hand drawn postcards to each other. Look that up, find it, I'll put a link in this video's description.",
        "tokens": [
          51244,
          597,
          436,
          2279,
          613,
          1412,
          25801,
          1011,
          10117,
          2183,
          40604,
          281,
          1184,
          661,
          13,
          2053,
          300,
          493,
          11,
          915,
          309,
          11,
          286,
          603,
          829,
          257,
          2113,
          294,
          341,
          960,
          311,
          3855,
          13,
          51544
        ]
      },
      {
        "avg_logprob": -0.3687064409255981,
        "compression_ratio": 1.5357142857142858,
        "end": 424.24,
        "id": 66,
        "no_speech_prob": 0.373515784740448,
        "seek": 41368,
        "start": 414.24,
        "temperature": 0,
        "text": " She worked on an album artwork for OK Go and doing visualization of word frequencies in their song lyrics.",
        "tokens": [
          50392,
          1240,
          2732,
          322,
          364,
          6030,
          15829,
          337,
          2264,
          1037,
          293,
          884,
          25801,
          295,
          1349,
          20250,
          294,
          641,
          2153,
          12189,
          13,
          50892
        ]
      },
      {
        "avg_logprob": -0.3687064409255981,
        "compression_ratio": 1.5357142857142858,
        "end": 431.6,
        "id": 67,
        "no_speech_prob": 0.373515784740448,
        "seek": 41368,
        "start": 424.24,
        "temperature": 0,
        "text": " You can see how some of these came out, what kind of visual quality they have. And you can also see what's going on here.",
        "tokens": [
          50892,
          509,
          393,
          536,
          577,
          512,
          295,
          613,
          1361,
          484,
          11,
          437,
          733,
          295,
          5056,
          3125,
          436,
          362,
          13,
          400,
          291,
          393,
          611,
          536,
          437,
          311,
          516,
          322,
          510,
          13,
          51260
        ]
      },
      {
        "avg_logprob": -0.3687064409255981,
        "compression_ratio": 1.5357142857142858,
        "end": 440.64,
        "id": 68,
        "no_speech_prob": 0.373515784740448,
        "seek": 41368,
        "start": 431.6,
        "temperature": 0,
        "text": " There's some diagrams showing exactly parts of speech concordance, common words that are both common to both texts,",
        "tokens": [
          51260,
          821,
          311,
          512,
          36709,
          4099,
          2293,
          3166,
          295,
          6218,
          1588,
          765,
          719,
          11,
          2689,
          2283,
          300,
          366,
          1293,
          2689,
          281,
          1293,
          15765,
          11,
          51712
        ]
      },
      {
        "avg_logprob": -0.34258247824276195,
        "compression_ratio": 1.6425855513307985,
        "end": 448.96,
        "id": 69,
        "no_speech_prob": 0.14604660868644714,
        "seek": 44064,
        "start": 440.64,
        "temperature": 0,
        "text": " counting what texts have different syllables in them, sentence length. So there's a lot of ways you can use the raw numbers of a text",
        "tokens": [
          50364,
          13251,
          437,
          15765,
          362,
          819,
          45364,
          294,
          552,
          11,
          8174,
          4641,
          13,
          407,
          456,
          311,
          257,
          688,
          295,
          2098,
          291,
          393,
          764,
          264,
          8936,
          3547,
          295,
          257,
          2487,
          50780
        ]
      },
      {
        "avg_logprob": -0.34258247824276195,
        "compression_ratio": 1.6425855513307985,
        "end": 452.96,
        "id": 70,
        "no_speech_prob": 0.14604660868644714,
        "seek": 44064,
        "start": 448.96,
        "temperature": 0,
        "text": " to glean something from it and to play around with different visual ideas.",
        "tokens": [
          50780,
          281,
          290,
          28499,
          746,
          490,
          309,
          293,
          281,
          862,
          926,
          365,
          819,
          5056,
          3487,
          13,
          50980
        ]
      },
      {
        "avg_logprob": -0.34258247824276195,
        "compression_ratio": 1.6425855513307985,
        "end": 461.2,
        "id": 71,
        "no_speech_prob": 0.14604660868644714,
        "seek": 44064,
        "start": 452.96,
        "temperature": 0,
        "text": " So these are ideas that I encourage you to think about. What's a text that really interests you? Where can you get text?",
        "tokens": [
          50980,
          407,
          613,
          366,
          3487,
          300,
          286,
          5373,
          291,
          281,
          519,
          466,
          13,
          708,
          311,
          257,
          2487,
          300,
          534,
          8847,
          291,
          30,
          2305,
          393,
          291,
          483,
          2487,
          30,
          51392
        ]
      },
      {
        "avg_logprob": -0.34258247824276195,
        "compression_ratio": 1.6425855513307985,
        "end": 467.91999999999996,
        "id": 72,
        "no_speech_prob": 0.14604660868644714,
        "seek": 44064,
        "start": 461.2,
        "temperature": 0,
        "text": " By the way, let me mention here also if you're thinking about places you might look for text examples,",
        "tokens": [
          51392,
          3146,
          264,
          636,
          11,
          718,
          385,
          2152,
          510,
          611,
          498,
          291,
          434,
          1953,
          466,
          3190,
          291,
          1062,
          574,
          337,
          2487,
          5110,
          11,
          51728
        ]
      },
      {
        "avg_logprob": -0.2854763454861111,
        "compression_ratio": 1.593073593073593,
        "end": 476.24,
        "id": 73,
        "no_speech_prob": 0.02228340320289135,
        "seek": 46792,
        "start": 468,
        "temperature": 0,
        "text": " and I think I just have two links on this page here. Send me your links and I'll add them to this page.",
        "tokens": [
          50368,
          293,
          286,
          519,
          286,
          445,
          362,
          732,
          6123,
          322,
          341,
          3028,
          510,
          13,
          17908,
          385,
          428,
          6123,
          293,
          286,
          603,
          909,
          552,
          281,
          341,
          3028,
          13,
          50780
        ]
      },
      {
        "avg_logprob": -0.2854763454861111,
        "compression_ratio": 1.593073593073593,
        "end": 485.68,
        "id": 74,
        "no_speech_prob": 0.02228340320289135,
        "seek": 46792,
        "start": 476.24,
        "temperature": 0,
        "text": " But Project Gutenberg is certainly one that you might explore. Project Gutenberg is an online repository of texts that are in the public domain.",
        "tokens": [
          50780,
          583,
          9849,
          42833,
          6873,
          307,
          3297,
          472,
          300,
          291,
          1062,
          6839,
          13,
          9849,
          42833,
          6873,
          307,
          364,
          2950,
          25841,
          295,
          15765,
          300,
          366,
          294,
          264,
          1908,
          9274,
          13,
          51252
        ]
      },
      {
        "avg_logprob": -0.2854763454861111,
        "compression_ratio": 1.593073593073593,
        "end": 491.28000000000003,
        "id": 75,
        "no_speech_prob": 0.02228340320289135,
        "seek": 46792,
        "start": 485.68,
        "temperature": 0,
        "text": " So there's Jane Austen's work is there, all of William Shakespeare's plays are there, you'll find lots of other things.",
        "tokens": [
          51252,
          407,
          456,
          311,
          13048,
          4126,
          268,
          311,
          589,
          307,
          456,
          11,
          439,
          295,
          6740,
          22825,
          311,
          5749,
          366,
          456,
          11,
          291,
          603,
          915,
          3195,
          295,
          661,
          721,
          13,
          51532
        ]
      },
      {
        "avg_logprob": -0.26614202771868023,
        "compression_ratio": 1.6567164179104477,
        "end": 495.59999999999997,
        "id": 76,
        "no_speech_prob": 0.11121241003274918,
        "seek": 49128,
        "start": 491.84,
        "temperature": 0,
        "text": " I have a Node example that does a text concordance for Pride and Prejudice.",
        "tokens": [
          50392,
          286,
          362,
          257,
          38640,
          1365,
          300,
          775,
          257,
          2487,
          1588,
          765,
          719,
          337,
          30319,
          293,
          6001,
          9218,
          573,
          13,
          50580
        ]
      },
      {
        "avg_logprob": -0.26614202771868023,
        "compression_ratio": 1.6567164179104477,
        "end": 503.28,
        "id": 77,
        "no_speech_prob": 0.11121241003274918,
        "seek": 49128,
        "start": 495.59999999999997,
        "temperature": 0,
        "text": " So this is something that you might consider looking at. You can get the text in raw txt files which are convenient to use in a project.",
        "tokens": [
          50580,
          407,
          341,
          307,
          746,
          300,
          291,
          1062,
          1949,
          1237,
          412,
          13,
          509,
          393,
          483,
          264,
          2487,
          294,
          8936,
          256,
          734,
          7098,
          597,
          366,
          10851,
          281,
          764,
          294,
          257,
          1716,
          13,
          50964
        ]
      },
      {
        "avg_logprob": -0.26614202771868023,
        "compression_ratio": 1.6567164179104477,
        "end": 512.88,
        "id": 78,
        "no_speech_prob": 0.11121241003274918,
        "seek": 49128,
        "start": 503.28,
        "temperature": 0,
        "text": " Okay, so this is sort of the summary of this building block. So what are the things that I'm going to show you that you can use this idea of word counting for",
        "tokens": [
          50964,
          1033,
          11,
          370,
          341,
          307,
          1333,
          295,
          264,
          12691,
          295,
          341,
          2390,
          3461,
          13,
          407,
          437,
          366,
          264,
          721,
          300,
          286,
          478,
          516,
          281,
          855,
          291,
          300,
          291,
          393,
          764,
          341,
          1558,
          295,
          1349,
          13251,
          337,
          51444
        ]
      },
      {
        "avg_logprob": -0.26614202771868023,
        "compression_ratio": 1.6567164179104477,
        "end": 516.3199999999999,
        "id": 79,
        "no_speech_prob": 0.11121241003274918,
        "seek": 49128,
        "start": 512.88,
        "temperature": 0,
        "text": " and how will I demonstrate it? What is going to be in these next videos?",
        "tokens": [
          51444,
          293,
          577,
          486,
          286,
          11698,
          309,
          30,
          708,
          307,
          516,
          281,
          312,
          294,
          613,
          958,
          2145,
          30,
          51616
        ]
      },
      {
        "avg_logprob": -0.2648480938326928,
        "compression_ratio": 1.530612244897959,
        "end": 527.36,
        "id": 80,
        "no_speech_prob": 0.059202030301094055,
        "seek": 51632,
        "start": 516.4000000000001,
        "temperature": 0,
        "text": " So the examples, I'm going to first just show you the basics of how to in JavaScript read in a body of text, count how many times each word appears,",
        "tokens": [
          50368,
          407,
          264,
          5110,
          11,
          286,
          478,
          516,
          281,
          700,
          445,
          855,
          291,
          264,
          14688,
          295,
          577,
          281,
          294,
          15778,
          1401,
          294,
          257,
          1772,
          295,
          2487,
          11,
          1207,
          577,
          867,
          1413,
          1184,
          1349,
          7038,
          11,
          50916
        ]
      },
      {
        "avg_logprob": -0.2648480938326928,
        "compression_ratio": 1.530612244897959,
        "end": 534.72,
        "id": 81,
        "no_speech_prob": 0.059202030301094055,
        "seek": 51632,
        "start": 527.36,
        "temperature": 0,
        "text": " and sort that list in order of frequency. Now conceptually it's a very simple thing to do. I could do it by hand.",
        "tokens": [
          50916,
          293,
          1333,
          300,
          1329,
          294,
          1668,
          295,
          7893,
          13,
          823,
          3410,
          671,
          309,
          311,
          257,
          588,
          2199,
          551,
          281,
          360,
          13,
          286,
          727,
          360,
          309,
          538,
          1011,
          13,
          51284
        ]
      },
      {
        "avg_logprob": -0.2648480938326928,
        "compression_ratio": 1.530612244897959,
        "end": 541.6,
        "id": 82,
        "no_speech_prob": 0.059202030301094055,
        "seek": 51632,
        "start": 534.72,
        "temperature": 0,
        "text": " But how you do that in a computer program opens up a topic which is about something called an associative array.",
        "tokens": [
          51284,
          583,
          577,
          291,
          360,
          300,
          294,
          257,
          3820,
          1461,
          9870,
          493,
          257,
          4829,
          597,
          307,
          466,
          746,
          1219,
          364,
          4180,
          1166,
          10225,
          13,
          51628
        ]
      },
      {
        "avg_logprob": -0.2698037764605354,
        "compression_ratio": 1.844155844155844,
        "end": 545.9200000000001,
        "id": 83,
        "no_speech_prob": 0.04336011782288551,
        "seek": 54160,
        "start": 541.6800000000001,
        "temperature": 0,
        "text": " So what is an associative array? Sometimes referred to as a hash map or a dictionary.",
        "tokens": [
          50368,
          407,
          437,
          307,
          364,
          4180,
          1166,
          10225,
          30,
          4803,
          10839,
          281,
          382,
          257,
          22019,
          4471,
          420,
          257,
          25890,
          13,
          50580
        ]
      },
      {
        "avg_logprob": -0.2698037764605354,
        "compression_ratio": 1.844155844155844,
        "end": 551.0400000000001,
        "id": 84,
        "no_speech_prob": 0.04336011782288551,
        "seek": 54160,
        "start": 545.9200000000001,
        "temperature": 0,
        "text": " So I'm going to make a video just about this topic. What is an associative array and how can you implement one in JavaScript?",
        "tokens": [
          50580,
          407,
          286,
          478,
          516,
          281,
          652,
          257,
          960,
          445,
          466,
          341,
          4829,
          13,
          708,
          307,
          364,
          4180,
          1166,
          10225,
          293,
          577,
          393,
          291,
          4445,
          472,
          294,
          15778,
          30,
          50836
        ]
      },
      {
        "avg_logprob": -0.2698037764605354,
        "compression_ratio": 1.844155844155844,
        "end": 554.16,
        "id": 85,
        "no_speech_prob": 0.04336011782288551,
        "seek": 54160,
        "start": 551.0400000000001,
        "temperature": 0,
        "text": " And I might mention a bit about how you can do it in processing in Java as well.",
        "tokens": [
          50836,
          400,
          286,
          1062,
          2152,
          257,
          857,
          466,
          577,
          291,
          393,
          360,
          309,
          294,
          9007,
          294,
          10745,
          382,
          731,
          13,
          50992
        ]
      },
      {
        "avg_logprob": -0.2698037764605354,
        "compression_ratio": 1.844155844155844,
        "end": 558.5600000000001,
        "id": 86,
        "no_speech_prob": 0.04336011782288551,
        "seek": 54160,
        "start": 554.16,
        "temperature": 0,
        "text": " And then I'm going to build a word counting application, simple word counting application.",
        "tokens": [
          50992,
          400,
          550,
          286,
          478,
          516,
          281,
          1322,
          257,
          1349,
          13251,
          3861,
          11,
          2199,
          1349,
          13251,
          3861,
          13,
          51212
        ]
      },
      {
        "avg_logprob": -0.2698037764605354,
        "compression_ratio": 1.844155844155844,
        "end": 562.5600000000001,
        "id": 87,
        "no_speech_prob": 0.04336011782288551,
        "seek": 54160,
        "start": 558.5600000000001,
        "temperature": 0,
        "text": " It won't do an interesting visualization but you could take it and visualize it in your own way.",
        "tokens": [
          51212,
          467,
          1582,
          380,
          360,
          364,
          1880,
          25801,
          457,
          291,
          727,
          747,
          309,
          293,
          23273,
          309,
          294,
          428,
          1065,
          636,
          13,
          51412
        ]
      },
      {
        "avg_logprob": -0.2698037764605354,
        "compression_ratio": 1.844155844155844,
        "end": 567.0400000000001,
        "id": 88,
        "no_speech_prob": 0.04336011782288551,
        "seek": 54160,
        "start": 562.5600000000001,
        "temperature": 0,
        "text": " I'll mention to you that you could do the same thing but with counting parts of speech.",
        "tokens": [
          51412,
          286,
          603,
          2152,
          281,
          291,
          300,
          291,
          727,
          360,
          264,
          912,
          551,
          457,
          365,
          13251,
          3166,
          295,
          6218,
          13,
          51636
        ]
      },
      {
        "avg_logprob": -0.35176414913601345,
        "compression_ratio": 1.59915611814346,
        "end": 575.28,
        "id": 89,
        "no_speech_prob": 0.19189698994159698,
        "seek": 56704,
        "start": 567.4399999999999,
        "temperature": 0,
        "text": " And then I want to look at keyword extraction using an algorithm called TFIDF or term frequency inverse document frequency.",
        "tokens": [
          50384,
          400,
          550,
          286,
          528,
          281,
          574,
          412,
          20428,
          30197,
          1228,
          364,
          9284,
          1219,
          40964,
          2777,
          37,
          420,
          1433,
          7893,
          17340,
          4166,
          7893,
          13,
          50776
        ]
      },
      {
        "avg_logprob": -0.35176414913601345,
        "compression_ratio": 1.59915611814346,
        "end": 582.0799999999999,
        "id": 90,
        "no_speech_prob": 0.19189698994159698,
        "seek": 56704,
        "start": 575.28,
        "temperature": 0,
        "text": " Where you'll read in multiple texts and look at words that are unique to one text but not in the others.",
        "tokens": [
          50776,
          2305,
          291,
          603,
          1401,
          294,
          3866,
          15765,
          293,
          574,
          412,
          2283,
          300,
          366,
          3845,
          281,
          472,
          2487,
          457,
          406,
          294,
          264,
          2357,
          13,
          51116
        ]
      },
      {
        "avg_logprob": -0.35176414913601345,
        "compression_ratio": 1.59915611814346,
        "end": 587.28,
        "id": 91,
        "no_speech_prob": 0.19189698994159698,
        "seek": 56704,
        "start": 582.0799999999999,
        "temperature": 0,
        "text": " And that's a way of knowing, well, the appears in a lot of text but not necessarily...",
        "tokens": [
          51116,
          400,
          300,
          311,
          257,
          636,
          295,
          5276,
          11,
          731,
          11,
          264,
          7038,
          294,
          257,
          688,
          295,
          2487,
          457,
          406,
          4725,
          485,
          51376
        ]
      },
      {
        "avg_logprob": -0.35176414913601345,
        "compression_ratio": 1.59915611814346,
        "end": 592.7199999999999,
        "id": 92,
        "no_speech_prob": 0.19189698994159698,
        "seek": 56704,
        "start": 587.28,
        "temperature": 0,
        "text": " So the is not a keyword even though it appears very frequently.",
        "tokens": [
          51376,
          407,
          264,
          307,
          406,
          257,
          20428,
          754,
          1673,
          309,
          7038,
          588,
          10374,
          13,
          51648
        ]
      },
      {
        "avg_logprob": -0.2529092516217913,
        "compression_ratio": 1.7773584905660378,
        "end": 598.64,
        "id": 93,
        "no_speech_prob": 0.06752627342939377,
        "seek": 59272,
        "start": 592.8000000000001,
        "temperature": 0,
        "text": " But in the rainbow Wikipedia article, rainbow appears a lot but it doesn't appear in a lot of other Wikipedia articles.",
        "tokens": [
          50368,
          583,
          294,
          264,
          18526,
          28999,
          7222,
          11,
          18526,
          7038,
          257,
          688,
          457,
          309,
          1177,
          380,
          4204,
          294,
          257,
          688,
          295,
          661,
          28999,
          11290,
          13,
          50660
        ]
      },
      {
        "avg_logprob": -0.2529092516217913,
        "compression_ratio": 1.7773584905660378,
        "end": 604.88,
        "id": 94,
        "no_speech_prob": 0.06752627342939377,
        "seek": 59272,
        "start": 598.64,
        "temperature": 0,
        "text": " So this idea of inverse, it's frequent in this document but not frequent in other documents.",
        "tokens": [
          50660,
          407,
          341,
          1558,
          295,
          17340,
          11,
          309,
          311,
          18004,
          294,
          341,
          4166,
          457,
          406,
          18004,
          294,
          661,
          8512,
          13,
          50972
        ]
      },
      {
        "avg_logprob": -0.2529092516217913,
        "compression_ratio": 1.7773584905660378,
        "end": 609.44,
        "id": 95,
        "no_speech_prob": 0.06752627342939377,
        "seek": 59272,
        "start": 604.88,
        "temperature": 0,
        "text": " So I'm going to look at that algorithm. And I don't think I'm going to be doing this today.",
        "tokens": [
          50972,
          407,
          286,
          478,
          516,
          281,
          574,
          412,
          300,
          9284,
          13,
          400,
          286,
          500,
          380,
          519,
          286,
          478,
          516,
          281,
          312,
          884,
          341,
          965,
          13,
          51200
        ]
      },
      {
        "avg_logprob": -0.2529092516217913,
        "compression_ratio": 1.7773584905660378,
        "end": 617.6800000000001,
        "id": 96,
        "no_speech_prob": 0.06752627342939377,
        "seek": 59272,
        "start": 609.44,
        "temperature": 0,
        "text": " But at some point in this playlist, hopefully it will also appear a set of lessons about doing text classification.",
        "tokens": [
          51200,
          583,
          412,
          512,
          935,
          294,
          341,
          16788,
          11,
          4696,
          309,
          486,
          611,
          4204,
          257,
          992,
          295,
          8820,
          466,
          884,
          2487,
          21538,
          13,
          51612
        ]
      },
      {
        "avg_logprob": -0.2529092516217913,
        "compression_ratio": 1.7773584905660378,
        "end": 621.12,
        "id": 97,
        "no_speech_prob": 0.06752627342939377,
        "seek": 59272,
        "start": 617.6800000000001,
        "temperature": 0,
        "text": " Is it spam or not spam using Bayesian probability?",
        "tokens": [
          51612,
          1119,
          309,
          24028,
          420,
          406,
          24028,
          1228,
          7840,
          42434,
          8482,
          30,
          51784
        ]
      },
      {
        "avg_logprob": -0.2896937427904782,
        "compression_ratio": 1.670487106017192,
        "end": 625.6,
        "id": 98,
        "no_speech_prob": 0.2017374187707901,
        "seek": 62112,
        "start": 621.2,
        "temperature": 0,
        "text": " This is a complex topic. I'm going to have to spend another full day on that probably.",
        "tokens": [
          50368,
          639,
          307,
          257,
          3997,
          4829,
          13,
          286,
          478,
          516,
          281,
          362,
          281,
          3496,
          1071,
          1577,
          786,
          322,
          300,
          1391,
          13,
          50588
        ]
      },
      {
        "avg_logprob": -0.2896937427904782,
        "compression_ratio": 1.670487106017192,
        "end": 631.04,
        "id": 99,
        "no_speech_prob": 0.2017374187707901,
        "seek": 62112,
        "start": 625.6,
        "temperature": 0,
        "text": " And I want to also mention how you might do this stuff in processing in Java and also maybe server side programming with Node.",
        "tokens": [
          50588,
          400,
          286,
          528,
          281,
          611,
          2152,
          577,
          291,
          1062,
          360,
          341,
          1507,
          294,
          9007,
          294,
          10745,
          293,
          611,
          1310,
          7154,
          1252,
          9410,
          365,
          38640,
          13,
          50860
        ]
      },
      {
        "avg_logprob": -0.2896937427904782,
        "compression_ratio": 1.670487106017192,
        "end": 636.5600000000001,
        "id": 100,
        "no_speech_prob": 0.2017374187707901,
        "seek": 62112,
        "start": 631.04,
        "temperature": 0,
        "text": " So that's kind of the summary of the things and then I'll come back at the end and I'll talk you through some exercise ideas.",
        "tokens": [
          50860,
          407,
          300,
          311,
          733,
          295,
          264,
          12691,
          295,
          264,
          721,
          293,
          550,
          286,
          603,
          808,
          646,
          412,
          264,
          917,
          293,
          286,
          603,
          751,
          291,
          807,
          512,
          5380,
          3487,
          13,
          51136
        ]
      },
      {
        "avg_logprob": -0.2896937427904782,
        "compression_ratio": 1.670487106017192,
        "end": 638.08,
        "id": 101,
        "no_speech_prob": 0.2017374187707901,
        "seek": 62112,
        "start": 636.5600000000001,
        "temperature": 0,
        "text": " Ah! And this reminds me.",
        "tokens": [
          51136,
          2438,
          0,
          400,
          341,
          12025,
          385,
          13,
          51212
        ]
      },
      {
        "avg_logprob": -0.2896937427904782,
        "compression_ratio": 1.670487106017192,
        "end": 644.32,
        "id": 102,
        "no_speech_prob": 0.2017374187707901,
        "seek": 62112,
        "start": 638.08,
        "temperature": 0,
        "text": " So I want to talk about something. This video could very well be over but it's not.",
        "tokens": [
          51212,
          407,
          286,
          528,
          281,
          751,
          466,
          746,
          13,
          639,
          960,
          727,
          588,
          731,
          312,
          670,
          457,
          309,
          311,
          406,
          13,
          51524
        ]
      },
      {
        "avg_logprob": -0.2896937427904782,
        "compression_ratio": 1.670487106017192,
        "end": 650.88,
        "id": 103,
        "no_speech_prob": 0.2017374187707901,
        "seek": 62112,
        "start": 644.32,
        "temperature": 0,
        "text": " Because I want to talk to you about one of my favorite books that I've read called The Secret Life of Pronouns by James W. Pennebaker.",
        "tokens": [
          51524,
          1436,
          286,
          528,
          281,
          751,
          281,
          291,
          466,
          472,
          295,
          452,
          2954,
          3642,
          300,
          286,
          600,
          1401,
          1219,
          440,
          7400,
          7720,
          295,
          27723,
          1733,
          82,
          538,
          5678,
          343,
          13,
          10571,
          716,
          65,
          4003,
          13,
          51852
        ]
      },
      {
        "avg_logprob": -0.3157647705078125,
        "compression_ratio": 1.6448275862068966,
        "end": 653.12,
        "id": 104,
        "no_speech_prob": 0.0025112798903137445,
        "seek": 65088,
        "start": 650.96,
        "temperature": 0,
        "text": " I encourage you to read this book.",
        "tokens": [
          50368,
          286,
          5373,
          291,
          281,
          1401,
          341,
          1446,
          13,
          50476
        ]
      },
      {
        "avg_logprob": -0.3157647705078125,
        "compression_ratio": 1.6448275862068966,
        "end": 659.52,
        "id": 105,
        "no_speech_prob": 0.0025112798903137445,
        "seek": 65088,
        "start": 653.12,
        "temperature": 0,
        "text": " I will link in this video's description to Pennebaker's TED Talk which gives kind of a 20 minute summary of the topic.",
        "tokens": [
          50476,
          286,
          486,
          2113,
          294,
          341,
          960,
          311,
          3855,
          281,
          10571,
          716,
          65,
          4003,
          311,
          43036,
          8780,
          597,
          2709,
          733,
          295,
          257,
          945,
          3456,
          12691,
          295,
          264,
          4829,
          13,
          50796
        ]
      },
      {
        "avg_logprob": -0.3157647705078125,
        "compression_ratio": 1.6448275862068966,
        "end": 663.12,
        "id": 106,
        "no_speech_prob": 0.0025112798903137445,
        "seek": 65088,
        "start": 659.52,
        "temperature": 0,
        "text": " But here's the thing. I mentioned before sentiment analysis.",
        "tokens": [
          50796,
          583,
          510,
          311,
          264,
          551,
          13,
          286,
          2835,
          949,
          16149,
          5215,
          13,
          50976
        ]
      },
      {
        "avg_logprob": -0.3157647705078125,
        "compression_ratio": 1.6448275862068966,
        "end": 665.04,
        "id": 107,
        "no_speech_prob": 0.0025112798903137445,
        "seek": 65088,
        "start": 663.12,
        "temperature": 0,
        "text": " So let's think about that.",
        "tokens": [
          50976,
          407,
          718,
          311,
          519,
          466,
          300,
          13,
          51072
        ]
      },
      {
        "avg_logprob": -0.3157647705078125,
        "compression_ratio": 1.6448275862068966,
        "end": 668.64,
        "id": 108,
        "no_speech_prob": 0.0025112798903137445,
        "seek": 65088,
        "start": 665.04,
        "temperature": 0,
        "text": " Sentiment analysis. I want to know is this text positive or negative.",
        "tokens": [
          51072,
          23652,
          2328,
          5215,
          13,
          286,
          528,
          281,
          458,
          307,
          341,
          2487,
          3353,
          420,
          3671,
          13,
          51252
        ]
      },
      {
        "avg_logprob": -0.3157647705078125,
        "compression_ratio": 1.6448275862068966,
        "end": 671.4399999999999,
        "id": 109,
        "no_speech_prob": 0.0025112798903137445,
        "seek": 65088,
        "start": 668.64,
        "temperature": 0,
        "text": " Well I might look for content based words.",
        "tokens": [
          51252,
          1042,
          286,
          1062,
          574,
          337,
          2701,
          2361,
          2283,
          13,
          51392
        ]
      },
      {
        "avg_logprob": -0.3157647705078125,
        "compression_ratio": 1.6448275862068966,
        "end": 674.96,
        "id": 110,
        "no_speech_prob": 0.0025112798903137445,
        "seek": 65088,
        "start": 671.4399999999999,
        "temperature": 0,
        "text": " What kind of... Is the word happy, joyful, are those frequent?",
        "tokens": [
          51392,
          708,
          733,
          295,
          485,
          1119,
          264,
          1349,
          2055,
          11,
          33090,
          11,
          366,
          729,
          18004,
          30,
          51568
        ]
      },
      {
        "avg_logprob": -0.3157647705078125,
        "compression_ratio": 1.6448275862068966,
        "end": 678.32,
        "id": 111,
        "no_speech_prob": 0.0025112798903137445,
        "seek": 65088,
        "start": 674.96,
        "temperature": 0,
        "text": " Or is the word sad or depressed, are those frequently used?",
        "tokens": [
          51568,
          1610,
          307,
          264,
          1349,
          4227,
          420,
          18713,
          11,
          366,
          729,
          10374,
          1143,
          30,
          51736
        ]
      },
      {
        "avg_logprob": -0.31174617343478733,
        "compression_ratio": 1.6747404844290656,
        "end": 686,
        "id": 112,
        "no_speech_prob": 0.010012181475758553,
        "seek": 67832,
        "start": 678.4000000000001,
        "temperature": 0,
        "text": " So this idea, I've always thought at least, that text analysis is associated with word frequencies, analyzing word counts.",
        "tokens": [
          50368,
          407,
          341,
          1558,
          11,
          286,
          600,
          1009,
          1194,
          412,
          1935,
          11,
          300,
          2487,
          5215,
          307,
          6615,
          365,
          1349,
          20250,
          11,
          23663,
          1349,
          14893,
          13,
          50748
        ]
      },
      {
        "avg_logprob": -0.31174617343478733,
        "compression_ratio": 1.6747404844290656,
        "end": 690.1600000000001,
        "id": 113,
        "no_speech_prob": 0.010012181475758553,
        "seek": 67832,
        "start": 686,
        "temperature": 0,
        "text": " But looking at the sort of big words, the content words, the descriptive words.",
        "tokens": [
          50748,
          583,
          1237,
          412,
          264,
          1333,
          295,
          955,
          2283,
          11,
          264,
          2701,
          2283,
          11,
          264,
          42585,
          2283,
          13,
          50956
        ]
      },
      {
        "avg_logprob": -0.31174617343478733,
        "compression_ratio": 1.6747404844290656,
        "end": 698.88,
        "id": 114,
        "no_speech_prob": 0.010012181475758553,
        "seek": 67832,
        "start": 690.1600000000001,
        "temperature": 0,
        "text": " James Pennebaker who's a psychologist who's done a lot of work with analyzing the ways that writing can help people get through trauma.",
        "tokens": [
          50956,
          5678,
          10571,
          716,
          65,
          4003,
          567,
          311,
          257,
          29514,
          567,
          311,
          1096,
          257,
          688,
          295,
          589,
          365,
          23663,
          264,
          2098,
          300,
          3579,
          393,
          854,
          561,
          483,
          807,
          11407,
          13,
          51392
        ]
      },
      {
        "avg_logprob": -0.31174617343478733,
        "compression_ratio": 1.6747404844290656,
        "end": 703.12,
        "id": 115,
        "no_speech_prob": 0.010012181475758553,
        "seek": 67832,
        "start": 698.88,
        "temperature": 0,
        "text": " Discovered something, from what I understand in his research, somewhat accidentally.",
        "tokens": [
          51392,
          40386,
          292,
          746,
          11,
          490,
          437,
          286,
          1223,
          294,
          702,
          2132,
          11,
          8344,
          15715,
          13,
          51604
        ]
      },
      {
        "avg_logprob": -0.31174617343478733,
        "compression_ratio": 1.6747404844290656,
        "end": 705.6800000000001,
        "id": 116,
        "no_speech_prob": 0.010012181475758553,
        "seek": 67832,
        "start": 703.12,
        "temperature": 0,
        "text": " Because he started out also looking for these content words.",
        "tokens": [
          51604,
          1436,
          415,
          1409,
          484,
          611,
          1237,
          337,
          613,
          2701,
          2283,
          13,
          51732
        ]
      },
      {
        "avg_logprob": -0.25002549906245997,
        "compression_ratio": 1.776061776061776,
        "end": 707.1999999999999,
        "id": 117,
        "no_speech_prob": 0.03567370027303696,
        "seek": 70568,
        "start": 705.76,
        "temperature": 0,
        "text": " But what he discovered is...",
        "tokens": [
          50368,
          583,
          437,
          415,
          6941,
          307,
          485,
          50440
        ]
      },
      {
        "avg_logprob": -0.25002549906245997,
        "compression_ratio": 1.776061776061776,
        "end": 710.2399999999999,
        "id": 118,
        "no_speech_prob": 0.03567370027303696,
        "seek": 70568,
        "start": 707.1999999999999,
        "temperature": 0,
        "text": " And in text analysis, in word counting, what you're often doing is saying like,",
        "tokens": [
          50440,
          400,
          294,
          2487,
          5215,
          11,
          294,
          1349,
          13251,
          11,
          437,
          291,
          434,
          2049,
          884,
          307,
          1566,
          411,
          11,
          50592
        ]
      },
      {
        "avg_logprob": -0.25002549906245997,
        "compression_ratio": 1.776061776061776,
        "end": 715.68,
        "id": 119,
        "no_speech_prob": 0.03567370027303696,
        "seek": 70568,
        "start": 710.2399999999999,
        "temperature": 0,
        "text": " Oh I'm always getting the, and he, and she, and I, and me, and my, and they.",
        "tokens": [
          50592,
          876,
          286,
          478,
          1009,
          1242,
          264,
          11,
          293,
          415,
          11,
          293,
          750,
          11,
          293,
          286,
          11,
          293,
          385,
          11,
          293,
          452,
          11,
          293,
          436,
          13,
          50864
        ]
      },
      {
        "avg_logprob": -0.25002549906245997,
        "compression_ratio": 1.776061776061776,
        "end": 719.68,
        "id": 120,
        "no_speech_prob": 0.03567370027303696,
        "seek": 70568,
        "start": 715.68,
        "temperature": 0,
        "text": " I should just make a list of stop words or junk words to ignore.",
        "tokens": [
          50864,
          286,
          820,
          445,
          652,
          257,
          1329,
          295,
          1590,
          2283,
          420,
          19109,
          2283,
          281,
          11200,
          13,
          51064
        ]
      },
      {
        "avg_logprob": -0.25002549906245997,
        "compression_ratio": 1.776061776061776,
        "end": 721.52,
        "id": 121,
        "no_speech_prob": 0.03567370027303696,
        "seek": 70568,
        "start": 719.68,
        "temperature": 0,
        "text": " And that's kind of a common technique.",
        "tokens": [
          51064,
          400,
          300,
          311,
          733,
          295,
          257,
          2689,
          6532,
          13,
          51156
        ]
      },
      {
        "avg_logprob": -0.25002549906245997,
        "compression_ratio": 1.776061776061776,
        "end": 730.4,
        "id": 122,
        "no_speech_prob": 0.03567370027303696,
        "seek": 70568,
        "start": 721.52,
        "temperature": 0,
        "text": " But what he discovered was that these words actually unlock the key in many ways to analyzing the emotional state of an author.",
        "tokens": [
          51156,
          583,
          437,
          415,
          6941,
          390,
          300,
          613,
          2283,
          767,
          11634,
          264,
          2141,
          294,
          867,
          2098,
          281,
          23663,
          264,
          6863,
          1785,
          295,
          364,
          3793,
          13,
          51600
        ]
      },
      {
        "avg_logprob": -0.25002549906245997,
        "compression_ratio": 1.776061776061776,
        "end": 732.4799999999999,
        "id": 123,
        "no_speech_prob": 0.03567370027303696,
        "seek": 70568,
        "start": 730.4,
        "temperature": 0,
        "text": " And lots of other properties of an author.",
        "tokens": [
          51600,
          400,
          3195,
          295,
          661,
          7221,
          295,
          364,
          3793,
          13,
          51704
        ]
      },
      {
        "avg_logprob": -0.20746249389648438,
        "compression_ratio": 1.7157190635451505,
        "end": 735.84,
        "id": 124,
        "no_speech_prob": 0.24789920449256897,
        "seek": 73248,
        "start": 732.48,
        "temperature": 0,
        "text": " Is an author or a speaker lying or telling the truth?",
        "tokens": [
          50364,
          1119,
          364,
          3793,
          420,
          257,
          8145,
          8493,
          420,
          3585,
          264,
          3494,
          30,
          50532
        ]
      },
      {
        "avg_logprob": -0.20746249389648438,
        "compression_ratio": 1.7157190635451505,
        "end": 739.04,
        "id": 125,
        "no_speech_prob": 0.24789920449256897,
        "seek": 73248,
        "start": 735.84,
        "temperature": 0,
        "text": " The use of the personal pronoun I, me, or my.",
        "tokens": [
          50532,
          440,
          764,
          295,
          264,
          2973,
          14144,
          286,
          11,
          385,
          11,
          420,
          452,
          13,
          50692
        ]
      },
      {
        "avg_logprob": -0.20746249389648438,
        "compression_ratio": 1.7157190635451505,
        "end": 744.32,
        "id": 126,
        "no_speech_prob": 0.24789920449256897,
        "seek": 73248,
        "start": 739.04,
        "temperature": 0,
        "text": " The frequency of that reveals a lot about a person's status in relationship to another person's status.",
        "tokens": [
          50692,
          440,
          7893,
          295,
          300,
          20893,
          257,
          688,
          466,
          257,
          954,
          311,
          6558,
          294,
          2480,
          281,
          1071,
          954,
          311,
          6558,
          13,
          50956
        ]
      },
      {
        "avg_logprob": -0.20746249389648438,
        "compression_ratio": 1.7157190635451505,
        "end": 746.4,
        "id": 127,
        "no_speech_prob": 0.24789920449256897,
        "seek": 73248,
        "start": 744.32,
        "temperature": 0,
        "text": " So I encourage you to think about that.",
        "tokens": [
          50956,
          407,
          286,
          5373,
          291,
          281,
          519,
          466,
          300,
          13,
          51060
        ]
      },
      {
        "avg_logprob": -0.20746249389648438,
        "compression_ratio": 1.7157190635451505,
        "end": 755.52,
        "id": 128,
        "no_speech_prob": 0.24789920449256897,
        "seek": 73248,
        "start": 746.4,
        "temperature": 0,
        "text": " And one of the exercises for this week could be to implement a sort of interactive system based on some of Pennebaker's research.",
        "tokens": [
          51060,
          400,
          472,
          295,
          264,
          11900,
          337,
          341,
          1243,
          727,
          312,
          281,
          4445,
          257,
          1333,
          295,
          15141,
          1185,
          2361,
          322,
          512,
          295,
          10571,
          716,
          65,
          4003,
          311,
          2132,
          13,
          51516
        ]
      },
      {
        "avg_logprob": -0.20746249389648438,
        "compression_ratio": 1.7157190635451505,
        "end": 762.16,
        "id": 129,
        "no_speech_prob": 0.24789920449256897,
        "seek": 73248,
        "start": 755.52,
        "temperature": 0,
        "text": " So you could read the book and I'll point you to some resources where you can kind of get some more information about his research as well.",
        "tokens": [
          51516,
          407,
          291,
          727,
          1401,
          264,
          1446,
          293,
          286,
          603,
          935,
          291,
          281,
          512,
          3593,
          689,
          291,
          393,
          733,
          295,
          483,
          512,
          544,
          1589,
          466,
          702,
          2132,
          382,
          731,
          13,
          51848
        ]
      },
      {
        "avg_logprob": -0.2824535230650519,
        "compression_ratio": 1.7985611510791366,
        "end": 767.36,
        "id": 130,
        "no_speech_prob": 0.0035931540187448263,
        "seek": 76216,
        "start": 762.16,
        "temperature": 0,
        "text": " And just to plug it a little bit more, I'll mention something that I would encourage you to do.",
        "tokens": [
          50364,
          400,
          445,
          281,
          5452,
          309,
          257,
          707,
          857,
          544,
          11,
          286,
          603,
          2152,
          746,
          300,
          286,
          576,
          5373,
          291,
          281,
          360,
          13,
          50624
        ]
      },
      {
        "avg_logprob": -0.2824535230650519,
        "compression_ratio": 1.7985611510791366,
        "end": 769.52,
        "id": 131,
        "no_speech_prob": 0.0035931540187448263,
        "seek": 76216,
        "start": 767.36,
        "temperature": 0,
        "text": " Secret life of pronouns.",
        "tokens": [
          50624,
          7400,
          993,
          295,
          35883,
          13,
          50732
        ]
      },
      {
        "avg_logprob": -0.2824535230650519,
        "compression_ratio": 1.7985611510791366,
        "end": 771.36,
        "id": 132,
        "no_speech_prob": 0.0035931540187448263,
        "seek": 76216,
        "start": 769.52,
        "temperature": 0,
        "text": " Let me go to the website.",
        "tokens": [
          50732,
          961,
          385,
          352,
          281,
          264,
          3144,
          13,
          50824
        ]
      },
      {
        "avg_logprob": -0.2824535230650519,
        "compression_ratio": 1.7985611510791366,
        "end": 774.9599999999999,
        "id": 133,
        "no_speech_prob": 0.0035931540187448263,
        "seek": 76216,
        "start": 771.36,
        "temperature": 0,
        "text": " Is click on the exercises at the top.",
        "tokens": [
          50824,
          1119,
          2052,
          322,
          264,
          11900,
          412,
          264,
          1192,
          13,
          51004
        ]
      },
      {
        "avg_logprob": -0.2824535230650519,
        "compression_ratio": 1.7985611510791366,
        "end": 781.12,
        "id": 134,
        "no_speech_prob": 0.0035931540187448263,
        "seek": 76216,
        "start": 774.9599999999999,
        "temperature": 0,
        "text": " And you can, I would encourage you to try some of these.",
        "tokens": [
          51004,
          400,
          291,
          393,
          11,
          286,
          576,
          5373,
          291,
          281,
          853,
          512,
          295,
          613,
          13,
          51312
        ]
      },
      {
        "avg_logprob": -0.2824535230650519,
        "compression_ratio": 1.7985611510791366,
        "end": 784.24,
        "id": 135,
        "no_speech_prob": 0.0035931540187448263,
        "seek": 76216,
        "start": 781.12,
        "temperature": 0,
        "text": " Don't click on them to look at them thinking you're going to try them later.",
        "tokens": [
          51312,
          1468,
          380,
          2052,
          322,
          552,
          281,
          574,
          412,
          552,
          1953,
          291,
          434,
          516,
          281,
          853,
          552,
          1780,
          13,
          51468
        ]
      },
      {
        "avg_logprob": -0.2824535230650519,
        "compression_ratio": 1.7985611510791366,
        "end": 785.76,
        "id": 136,
        "no_speech_prob": 0.0035931540187448263,
        "seek": 76216,
        "start": 784.24,
        "temperature": 0,
        "text": " Because you want to come to them fresh.",
        "tokens": [
          51468,
          1436,
          291,
          528,
          281,
          808,
          281,
          552,
          4451,
          13,
          51544
        ]
      },
      {
        "avg_logprob": -0.2824535230650519,
        "compression_ratio": 1.7985611510791366,
        "end": 789.12,
        "id": 137,
        "no_speech_prob": 0.0035931540187448263,
        "seek": 76216,
        "start": 785.76,
        "temperature": 0,
        "text": " They're going to ask you to write for five or ten minutes and then it's going to analyze your text for you.",
        "tokens": [
          51544,
          814,
          434,
          516,
          281,
          1029,
          291,
          281,
          2464,
          337,
          1732,
          420,
          2064,
          2077,
          293,
          550,
          309,
          311,
          516,
          281,
          12477,
          428,
          2487,
          337,
          291,
          13,
          51712
        ]
      },
      {
        "avg_logprob": -0.2824535230650519,
        "compression_ratio": 1.7985611510791366,
        "end": 790.56,
        "id": 138,
        "no_speech_prob": 0.0035931540187448263,
        "seek": 76216,
        "start": 789.12,
        "temperature": 0,
        "text": " But I encourage you to try these.",
        "tokens": [
          51712,
          583,
          286,
          5373,
          291,
          281,
          853,
          613,
          13,
          51784
        ]
      },
      {
        "avg_logprob": -0.3432432111802992,
        "compression_ratio": 1.5869565217391304,
        "end": 797.04,
        "id": 139,
        "no_speech_prob": 0.06752823293209076,
        "seek": 79056,
        "start": 790.56,
        "temperature": 0,
        "text": " I think you'll learn a lot about text analysis and about yourself and sort of thinking about project ideas around this space.",
        "tokens": [
          50364,
          286,
          519,
          291,
          603,
          1466,
          257,
          688,
          466,
          2487,
          5215,
          293,
          466,
          1803,
          293,
          1333,
          295,
          1953,
          466,
          1716,
          3487,
          926,
          341,
          1901,
          13,
          50688
        ]
      },
      {
        "avg_logprob": -0.3432432111802992,
        "compression_ratio": 1.5869565217391304,
        "end": 797.68,
        "id": 140,
        "no_speech_prob": 0.06752823293209076,
        "seek": 79056,
        "start": 797.04,
        "temperature": 0,
        "text": " Okay?",
        "tokens": [
          50688,
          1033,
          30,
          50720
        ]
      },
      {
        "avg_logprob": -0.3432432111802992,
        "compression_ratio": 1.5869565217391304,
        "end": 801.4399999999999,
        "id": 141,
        "no_speech_prob": 0.06752823293209076,
        "seek": 79056,
        "start": 797.68,
        "temperature": 0,
        "text": " So I'm going to get started actually looking at code and talking about the technical stuff now.",
        "tokens": [
          50720,
          407,
          286,
          478,
          516,
          281,
          483,
          1409,
          767,
          1237,
          412,
          3089,
          293,
          1417,
          466,
          264,
          6191,
          1507,
          586,
          13,
          50908
        ]
      },
      {
        "avg_logprob": -0.3432432111802992,
        "compression_ratio": 1.5869565217391304,
        "end": 806.56,
        "id": 142,
        "no_speech_prob": 0.06752823293209076,
        "seek": 79056,
        "start": 801.4399999999999,
        "temperature": 0,
        "text": " And in the next video I'm going to talk about associative arrays in JavaScript.",
        "tokens": [
          50908,
          400,
          294,
          264,
          958,
          960,
          286,
          478,
          516,
          281,
          751,
          466,
          4180,
          1166,
          41011,
          294,
          15778,
          13,
          51164
        ]
      },
      {
        "avg_logprob": -0.3432432111802992,
        "compression_ratio": 1.5869565217391304,
        "end": 807.04,
        "id": 143,
        "no_speech_prob": 0.06752823293209076,
        "seek": 79056,
        "start": 806.56,
        "temperature": 0,
        "text": " Okay?",
        "tokens": [
          51164,
          1033,
          30,
          51188
        ]
      },
      {
        "avg_logprob": -0.3432432111802992,
        "compression_ratio": 1.5869565217391304,
        "end": 807.92,
        "id": 144,
        "no_speech_prob": 0.06752823293209076,
        "seek": 79056,
        "start": 807.04,
        "temperature": 0,
        "text": " Look forward to seeing you there.",
        "tokens": [
          51188,
          2053,
          2128,
          281,
          2577,
          291,
          456,
          13,
          51232
        ]
      },
      {
        "avg_logprob": -0.3432432111802992,
        "compression_ratio": 1.5869565217391304,
        "end": 808.64,
        "id": 145,
        "no_speech_prob": 0.06752823293209076,
        "seek": 79056,
        "start": 807.92,
        "temperature": 0,
        "text": " Hopefully, maybe.",
        "tokens": [
          51232,
          10429,
          11,
          1310,
          13,
          51268
        ]
      }
    ],
    "transcription": " Hello, welcome to session 5 of programming from A to Z, which is like a thing that I'm doing on the internet, on YouTube some tutorials topics around programming with text. Now on the one hand you could divide this course, this set of playlists together into two parts. There's analyzing text and there's generating text and and today's session is really about, and you can also think of that as reading text and writing text, right? And most of the projects that I've been demonstrating or that I will show or talk about do some combination of both. Maybe they read in a source material, mash it up, chop it up and generate something from it. Today though I really want to focus on the reading in of text, the analysis of text. And when I say today I mean what I'm doing right now. But ultimately the set of tutorials that I'm going to make. So if you, I'm going to give you a sort of summary of the topic. I'm going to show you some relevant projects that might inspire you with ideas. But if you want to just get to the coding part you can skip to the next video in this playlist. You should be in the session 5 or week 5 programming from A to Z playlist. So the building block, so text analysis, by text analysis I mean reading in a text, analyzing it and producing some result. That could, that result could be sentiment analysis. It's a positive text as a negative text. It could be ah this text is very similar in style to another text. There are so many different kinds of ways you can about this. This is a difficult text, an easy text. You can imagine what kinds of outcome, statistical analysis, whatever you could do from reading in text with a computer program. But the, we need a building block. And the building block for every single one of these text analysis examples and scenarios that I will present and show you how to code is word counting. Actually word counting is a totally simple thing and this is not a new thing. So I'm going to pop over to this Wikipedia page here on this idea of a concordance. So concordance Concordance, concordance. A concordance is an alphabetical list of the principal words used in a book or body of work listing every instance of each word with its immediate context. Blah blah blah blah blah. What that means is a concordance is hey I want to know all the words that were in this text and I want to know how many times they appeared and where they appeared. So I'm going to do something simpler when I build an actual code example which is just here's a list of all the words in a text and how many times those words appeared. And this is not something that's just sort of a computing thing. This has been done, you know, this has been done by hand many years with, you know, you know, well, you know, this used to be done by hand and so there's some information here about biblical concordances, you know, reading the text manually, making this big list, making references, you know, an index in a way is very similar to a concordance in many ways as well. It's a sort of list of topics and where they appear in a particular book. So I encourage you, but you can see that here again this list I think is useful. Here I am. This is, hello, this is Daniel Shiffman reading you a Wikipedia page on the internet. But this is kind of useful context and it has some nice, it's kind of, it has some nice reference material for you to like kind of expand further into this territory. But you can see this idea of analyzing, one thing you can do with a concordance is try to figure out are there key words associated with a piece of text. One of the examples I'll show you is a term frequency inverse document frequency algorithm that pulls out key words from a text and all sorts of other things that you can do with a concordance algorithm. So let me show you some examples. So first of all, here's a great example. This is by Rune Madsen and they'll be in this video's description, there'll be links to all these projects. This is called speech comparison. It's a project created in processing and this is a kind of what you might typically see as a kind of example project idea with a text concordance. This is visualizing a bunch of speeches by these particular speakers and counting words and you can see here a list of words that appear in the text and then drawing a color coded visualization of the frequency of that term. And you can see here climate. Hmm, climate is used a lot by this purple person. I like to think of myself as a purple person. I don't know what color you think of yourself, I think of myself as purple. And you can see that person is Al Gore, well known and famous for his work in climate, I was going to say climate science but I don't know if that's accurate, promotion or getting a message out there about the issue of climate change. And so this is something you might think about doing. How could you creatively visualize the words that appear in a given text. Let me show you another project by Sarah Groff Palermo. This project is called BookBook and I encourage you to take a look more deeply at this project but I'm going to click here. This is a comparison of The Jungle by Sinclair versus The Jungle Book by Kipling and I'm going to click on this and what you're seeing here is what are the words that are unique to one text but not in another text. So this is something you could also really do with word counting and look at a comparison of two texts. By the way, this is how spam filtering works. Let's look at all of my emails which are not spam and count all the word frequencies in those and let's look at all of my emails that are spam and count all the frequencies in those and then build a statistical probability that a new email fits into one of those categories. And you can see it's not just about what kind of words are in a spam email it's all about the relationship of spam to regular emails. One way to think about this by the way is I get a lot of spam emails that are trying to sell me like a mortgage. So mortgage is a word frequency that appears in a lot of my spam emails. So that's a good indicator but if I were a mortgage broker I'd probably get a lot of actual mortgage emails so that word wouldn't be as relevant to me. So just detecting just word frequency in one document but how it compares to others is kind of a key way to do text analysis and we'll see some examples of that. You can also look, Sarah's project also has the ability for you to look at the words that appear in both texts I believe if I click over to see both of them together. Another wonderful work in data visualization, another artist that I love is Stephanie Posavec. And Stephanie has done a lot of work with hand drawn data visualization. She has a beautiful project with Georgia Newby called Dear Data which they sent these data visualization hand drawn postcards to each other. Look that up, find it, I'll put a link in this video's description. She worked on an album artwork for OK Go and doing visualization of word frequencies in their song lyrics. You can see how some of these came out, what kind of visual quality they have. And you can also see what's going on here. There's some diagrams showing exactly parts of speech concordance, common words that are both common to both texts, counting what texts have different syllables in them, sentence length. So there's a lot of ways you can use the raw numbers of a text to glean something from it and to play around with different visual ideas. So these are ideas that I encourage you to think about. What's a text that really interests you? Where can you get text? By the way, let me mention here also if you're thinking about places you might look for text examples, and I think I just have two links on this page here. Send me your links and I'll add them to this page. But Project Gutenberg is certainly one that you might explore. Project Gutenberg is an online repository of texts that are in the public domain. So there's Jane Austen's work is there, all of William Shakespeare's plays are there, you'll find lots of other things. I have a Node example that does a text concordance for Pride and Prejudice. So this is something that you might consider looking at. You can get the text in raw txt files which are convenient to use in a project. Okay, so this is sort of the summary of this building block. So what are the things that I'm going to show you that you can use this idea of word counting for and how will I demonstrate it? What is going to be in these next videos? So the examples, I'm going to first just show you the basics of how to in JavaScript read in a body of text, count how many times each word appears, and sort that list in order of frequency. Now conceptually it's a very simple thing to do. I could do it by hand. But how you do that in a computer program opens up a topic which is about something called an associative array. So what is an associative array? Sometimes referred to as a hash map or a dictionary. So I'm going to make a video just about this topic. What is an associative array and how can you implement one in JavaScript? And I might mention a bit about how you can do it in processing in Java as well. And then I'm going to build a word counting application, simple word counting application. It won't do an interesting visualization but you could take it and visualize it in your own way. I'll mention to you that you could do the same thing but with counting parts of speech. And then I want to look at keyword extraction using an algorithm called TFIDF or term frequency inverse document frequency. Where you'll read in multiple texts and look at words that are unique to one text but not in the others. And that's a way of knowing, well, the appears in a lot of text but not necessarily... So the is not a keyword even though it appears very frequently. But in the rainbow Wikipedia article, rainbow appears a lot but it doesn't appear in a lot of other Wikipedia articles. So this idea of inverse, it's frequent in this document but not frequent in other documents. So I'm going to look at that algorithm. And I don't think I'm going to be doing this today. But at some point in this playlist, hopefully it will also appear a set of lessons about doing text classification. Is it spam or not spam using Bayesian probability? This is a complex topic. I'm going to have to spend another full day on that probably. And I want to also mention how you might do this stuff in processing in Java and also maybe server side programming with Node. So that's kind of the summary of the things and then I'll come back at the end and I'll talk you through some exercise ideas. Ah! And this reminds me. So I want to talk about something. This video could very well be over but it's not. Because I want to talk to you about one of my favorite books that I've read called The Secret Life of Pronouns by James W. Pennebaker. I encourage you to read this book. I will link in this video's description to Pennebaker's TED Talk which gives kind of a 20 minute summary of the topic. But here's the thing. I mentioned before sentiment analysis. So let's think about that. Sentiment analysis. I want to know is this text positive or negative. Well I might look for content based words. What kind of... Is the word happy, joyful, are those frequent? Or is the word sad or depressed, are those frequently used? So this idea, I've always thought at least, that text analysis is associated with word frequencies, analyzing word counts. But looking at the sort of big words, the content words, the descriptive words. James Pennebaker who's a psychologist who's done a lot of work with analyzing the ways that writing can help people get through trauma. Discovered something, from what I understand in his research, somewhat accidentally. Because he started out also looking for these content words. But what he discovered is... And in text analysis, in word counting, what you're often doing is saying like, Oh I'm always getting the, and he, and she, and I, and me, and my, and they. I should just make a list of stop words or junk words to ignore. And that's kind of a common technique. But what he discovered was that these words actually unlock the key in many ways to analyzing the emotional state of an author. And lots of other properties of an author. Is an author or a speaker lying or telling the truth? The use of the personal pronoun I, me, or my. The frequency of that reveals a lot about a person's status in relationship to another person's status. So I encourage you to think about that. And one of the exercises for this week could be to implement a sort of interactive system based on some of Pennebaker's research. So you could read the book and I'll point you to some resources where you can kind of get some more information about his research as well. And just to plug it a little bit more, I'll mention something that I would encourage you to do. Secret life of pronouns. Let me go to the website. Is click on the exercises at the top. And you can, I would encourage you to try some of these. Don't click on them to look at them thinking you're going to try them later. Because you want to come to them fresh. They're going to ask you to write for five or ten minutes and then it's going to analyze your text for you. But I encourage you to try these. I think you'll learn a lot about text analysis and about yourself and sort of thinking about project ideas around this space. Okay? So I'm going to get started actually looking at code and talking about the technical stuff now. And in the next video I'm going to talk about associative arrays in JavaScript. Okay? Look forward to seeing you there. Hopefully, maybe.",
    "translation": null
  },
  "error": null,
  "status": "succeeded",
  "created_at": "2023-09-26T21:03:47.575371Z",
  "started_at": "2023-09-26T21:17:29.04192Z",
  "completed_at": "2023-09-26T21:20:30.993634Z",
  "webhook": "https://83ceaa0b612c.ngrok.app/?video_id=tE-ZYXU8A8U",
  "webhook_events_filter": [
    "completed"
  ],
  "metrics": {
    "predict_time": 181.951714
  },
  "urls": {
    "cancel": "https://api.replicate.com/v1/predictions/wbgfnerbdvmkldcby5xmonssvu/cancel",
    "get": "https://api.replicate.com/v1/predictions/wbgfnerbdvmkldcby5xmonssvu"
  }
}