{
  "id": "kqfku2bbuxdzjhbwm7l4jutu5e",
  "version": "91ee9c0c3df30478510ff8c8a3a545add1ad0259ad3a9f78fba57fbc05ee64f7",
  "input": {
    "audio": "https://upcdn.io/FW25b4F/raw/coding-train/iN3WAko2rL8.m4a"
  },
  "logs": "Transcribe with large-v2 model\nDetected language: English\n  0%|          | 0/171848 [00:00<?, ?frames/s]\n  2%|▏         | 2908/171848 [00:08<08:17, 339.83frames/s]\n  3%|▎         | 5688/171848 [00:18<09:08, 303.10frames/s]\n  5%|▍         | 8548/171848 [00:27<08:58, 302.98frames/s]\n  7%|▋         | 11280/171848 [00:35<08:15, 324.00frames/s]\n  8%|▊         | 14216/171848 [00:43<07:48, 336.44frames/s]\n 10%|▉         | 17100/171848 [00:50<07:06, 362.94frames/s]\n 12%|█▏        | 19876/171848 [00:57<06:48, 371.61frames/s]\n 13%|█▎        | 22772/171848 [01:03<06:19, 392.88frames/s]\n 15%|█▍        | 25616/171848 [01:11<06:23, 381.78frames/s]\n 17%|█▋        | 28394/171848 [01:20<06:32, 365.10frames/s]\n 18%|█▊        | 31394/171848 [01:25<05:47, 403.89frames/s]\n 20%|█▉        | 34226/171848 [01:33<05:45, 398.37frames/s]\n 22%|██▏       | 37086/171848 [01:40<05:44, 391.08frames/s]\n 23%|██▎       | 40046/171848 [01:46<05:13, 420.46frames/s]\n 25%|██▍       | 42774/171848 [01:53<05:04, 423.97frames/s]\n 27%|██▋       | 45650/171848 [02:03<05:52, 357.55frames/s]\n 28%|██▊       | 48614/171848 [02:10<05:27, 376.05frames/s]\n 30%|██▉       | 51366/171848 [02:17<05:11, 386.32frames/s]\n 32%|███▏      | 54366/171848 [02:24<04:49, 405.57frames/s]\n 33%|███▎      | 57142/171848 [02:32<04:57, 385.26frames/s]\n 35%|███▍      | 60098/171848 [02:41<05:07, 363.88frames/s]\n 37%|███▋      | 62982/171848 [02:48<04:51, 373.52frames/s]\n 38%|███▊      | 65802/171848 [02:56<04:42, 375.35frames/s]\n 40%|███▉      | 68566/171848 [03:02<04:24, 390.25frames/s]\n 42%|████▏     | 71566/171848 [03:08<03:55, 425.97frames/s]\n 43%|████▎     | 74378/171848 [03:14<03:47, 427.73frames/s]\n 45%|████▍     | 77312/171848 [03:21<03:38, 431.91frames/s]\n 47%|████▋     | 80312/171848 [03:27<03:27, 440.23frames/s]\n 48%|████▊     | 83026/171848 [03:34<03:27, 428.47frames/s]\n 50%|████▉     | 85894/171848 [03:40<03:18, 432.42frames/s]\n 51%|█████▏    | 88370/171848 [03:46<03:14, 429.09frames/s]\n 53%|█████▎    | 91162/171848 [03:51<02:52, 468.43frames/s]\n 55%|█████▍    | 93810/171848 [03:58<02:53, 450.34frames/s]\n 56%|█████▌    | 96602/171848 [04:03<02:44, 457.52frames/s]\n 58%|█████▊    | 99594/171848 [04:11<02:48, 429.45frames/s]\n 60%|█████▉    | 102410/171848 [04:18<02:43, 425.37frames/s]\n 61%|██████▏   | 105386/171848 [04:23<02:23, 462.58frames/s]\n 63%|██████▎   | 108346/171848 [04:29<02:15, 469.64frames/s]\n 65%|██████▍   | 111346/171848 [04:35<02:04, 484.33frames/s]\n 66%|██████▋   | 114014/171848 [04:40<01:54, 505.65frames/s]\n 68%|██████▊   | 116994/171848 [04:47<01:57, 467.41frames/s]\n 70%|██████▉   | 119806/171848 [04:53<01:47, 482.60frames/s]\n 71%|███████▏  | 122550/171848 [05:02<01:59, 411.27frames/s]\n 73%|███████▎  | 125414/171848 [05:11<02:06, 368.04frames/s]\n 75%|███████▍  | 128138/171848 [05:18<01:55, 377.42frames/s]\n 76%|███████▌  | 130922/171848 [05:26<01:48, 377.00frames/s]\n 78%|███████▊  | 133642/171848 [05:34<01:48, 352.70frames/s]\n 79%|███████▉  | 136462/171848 [05:44<01:48, 326.89frames/s]\n 81%|████████  | 139314/171848 [05:55<01:44, 312.21frames/s]\n 83%|████████▎ | 142058/171848 [06:03<01:33, 317.30frames/s]\n 84%|████████▍ | 144750/171848 [06:11<01:24, 321.35frames/s]\n 86%|████████▌ | 147690/171848 [06:17<01:07, 359.58frames/s]\n 88%|████████▊ | 150582/171848 [06:26<01:00, 353.59frames/s]\n 89%|████████▉ | 153578/171848 [06:35<00:54, 335.70frames/s]\n 91%|█████████ | 156290/171848 [06:45<00:49, 315.85frames/s]\n 92%|█████████▏| 158922/171848 [06:53<00:39, 323.65frames/s]\n 94%|█████████▍| 161762/171848 [07:01<00:30, 328.88frames/s]\n 96%|█████████▌| 164726/171848 [07:12<00:23, 306.21frames/s]\n 98%|█████████▊| 167722/171848 [07:22<00:13, 304.45frames/s]\n 99%|█████████▉| 170566/171848 [07:30<00:04, 317.25frames/s]\n 99%|█████████▉| 170566/171848 [07:48<00:04, 317.25frames/s]\n100%|██████████| 171848/171848 [08:01<00:00, 155.23frames/s]\n100%|██████████| 171848/171848 [08:01<00:00, 357.08frames/s]\n",
  "output": {
    "detected_language": "english",
    "segments": [
      {
        "avg_logprob": -0.2561637661981244,
        "compression_ratio": 1.560897435897436,
        "end": 5,
        "id": 0,
        "no_speech_prob": 0.039019644260406494,
        "seek": 0,
        "start": 0,
        "temperature": 0,
        "text": " Hello, welcome to another guest video on the Coding Train.",
        "tokens": [
          50364,
          2425,
          11,
          2928,
          281,
          1071,
          8341,
          960,
          322,
          264,
          383,
          8616,
          28029,
          13,
          50614
        ]
      },
      {
        "avg_logprob": -0.2561637661981244,
        "compression_ratio": 1.560897435897436,
        "end": 8.22,
        "id": 1,
        "no_speech_prob": 0.039019644260406494,
        "seek": 0,
        "start": 5.92,
        "temperature": 0,
        "text": " Today I have a very exciting guest for you,",
        "tokens": [
          50660,
          2692,
          286,
          362,
          257,
          588,
          4670,
          8341,
          337,
          291,
          11,
          50775
        ]
      },
      {
        "avg_logprob": -0.2561637661981244,
        "compression_ratio": 1.560897435897436,
        "end": 11.620000000000001,
        "id": 2,
        "no_speech_prob": 0.039019644260406494,
        "seek": 0,
        "start": 9.6,
        "temperature": 0,
        "text": " Jabril from Ceph Science.",
        "tokens": [
          50844,
          40319,
          24216,
          490,
          383,
          595,
          71,
          8976,
          13,
          50945
        ]
      },
      {
        "avg_logprob": -0.2561637661981244,
        "compression_ratio": 1.560897435897436,
        "end": 12.700000000000001,
        "id": 3,
        "no_speech_prob": 0.039019644260406494,
        "seek": 0,
        "start": 11.620000000000001,
        "temperature": 0,
        "text": " I'm probably saying that wrong.",
        "tokens": [
          50945,
          286,
          478,
          1391,
          1566,
          300,
          2085,
          13,
          50999
        ]
      },
      {
        "avg_logprob": -0.2561637661981244,
        "compression_ratio": 1.560897435897436,
        "end": 15.08,
        "id": 4,
        "no_speech_prob": 0.039019644260406494,
        "seek": 0,
        "start": 12.700000000000001,
        "temperature": 0,
        "text": " Jabril told me like 10 different times how to say it.",
        "tokens": [
          50999,
          40319,
          24216,
          1907,
          385,
          411,
          1266,
          819,
          1413,
          577,
          281,
          584,
          309,
          13,
          51118
        ]
      },
      {
        "avg_logprob": -0.2561637661981244,
        "compression_ratio": 1.560897435897436,
        "end": 15.96,
        "id": 5,
        "no_speech_prob": 0.039019644260406494,
        "seek": 0,
        "start": 15.08,
        "temperature": 0,
        "text": " I still couldn't get it right.",
        "tokens": [
          51118,
          286,
          920,
          2809,
          380,
          483,
          309,
          558,
          13,
          51162
        ]
      },
      {
        "avg_logprob": -0.2561637661981244,
        "compression_ratio": 1.560897435897436,
        "end": 17.14,
        "id": 6,
        "no_speech_prob": 0.039019644260406494,
        "seek": 0,
        "start": 15.96,
        "temperature": 0,
        "text": " Anyway, Jabril is awesome.",
        "tokens": [
          51162,
          5684,
          11,
          40319,
          24216,
          307,
          3476,
          13,
          51221
        ]
      },
      {
        "avg_logprob": -0.2561637661981244,
        "compression_ratio": 1.560897435897436,
        "end": 19.2,
        "id": 7,
        "no_speech_prob": 0.039019644260406494,
        "seek": 0,
        "start": 17.14,
        "temperature": 0,
        "text": " I'm a big fan of his YouTube channel.",
        "tokens": [
          51221,
          286,
          478,
          257,
          955,
          3429,
          295,
          702,
          3088,
          2269,
          13,
          51324
        ]
      },
      {
        "avg_logprob": -0.2561637661981244,
        "compression_ratio": 1.560897435897436,
        "end": 21.88,
        "id": 8,
        "no_speech_prob": 0.039019644260406494,
        "seek": 0,
        "start": 19.2,
        "temperature": 0,
        "text": " He actually came to visit NYU for a whole week",
        "tokens": [
          51324,
          634,
          767,
          1361,
          281,
          3441,
          42682,
          337,
          257,
          1379,
          1243,
          51458
        ]
      },
      {
        "avg_logprob": -0.2561637661981244,
        "compression_ratio": 1.560897435897436,
        "end": 23.92,
        "id": 9,
        "no_speech_prob": 0.039019644260406494,
        "seek": 0,
        "start": 21.88,
        "temperature": 0,
        "text": " and did a workshop and a talk and made a project",
        "tokens": [
          51458,
          293,
          630,
          257,
          13541,
          293,
          257,
          751,
          293,
          1027,
          257,
          1716,
          51560
        ]
      },
      {
        "avg_logprob": -0.2561637661981244,
        "compression_ratio": 1.560897435897436,
        "end": 27,
        "id": 10,
        "no_speech_prob": 0.039019644260406494,
        "seek": 0,
        "start": 23.92,
        "temperature": 0,
        "text": " and it's been sort of been an inspiring presence",
        "tokens": [
          51560,
          293,
          309,
          311,
          668,
          1333,
          295,
          668,
          364,
          15883,
          6814,
          51714
        ]
      },
      {
        "avg_logprob": -0.2561637661981244,
        "compression_ratio": 1.560897435897436,
        "end": 29.080000000000002,
        "id": 11,
        "no_speech_prob": 0.039019644260406494,
        "seek": 0,
        "start": 27,
        "temperature": 0,
        "text": " to have here for all this time.",
        "tokens": [
          51714,
          281,
          362,
          510,
          337,
          439,
          341,
          565,
          13,
          51818
        ]
      },
      {
        "avg_logprob": -0.22374911057321648,
        "compression_ratio": 1.7435897435897436,
        "end": 30.4,
        "id": 12,
        "no_speech_prob": 0.00006108159868745133,
        "seek": 2908,
        "start": 29.08,
        "temperature": 0,
        "text": " And anyway, what you're about to watch",
        "tokens": [
          50364,
          400,
          4033,
          11,
          437,
          291,
          434,
          466,
          281,
          1159,
          50430
        ]
      },
      {
        "avg_logprob": -0.22374911057321648,
        "compression_ratio": 1.7435897435897436,
        "end": 32.239999999999995,
        "id": 13,
        "no_speech_prob": 0.00006108159868745133,
        "seek": 2908,
        "start": 30.4,
        "temperature": 0,
        "text": " is an edited version of a live stream",
        "tokens": [
          50430,
          307,
          364,
          23016,
          3037,
          295,
          257,
          1621,
          4309,
          50522
        ]
      },
      {
        "avg_logprob": -0.22374911057321648,
        "compression_ratio": 1.7435897435897436,
        "end": 33.199999999999996,
        "id": 14,
        "no_speech_prob": 0.00006108159868745133,
        "seek": 2908,
        "start": 32.239999999999995,
        "temperature": 0,
        "text": " that the two of us did.",
        "tokens": [
          50522,
          300,
          264,
          732,
          295,
          505,
          630,
          13,
          50570
        ]
      },
      {
        "avg_logprob": -0.22374911057321648,
        "compression_ratio": 1.7435897435897436,
        "end": 35.839999999999996,
        "id": 15,
        "no_speech_prob": 0.00006108159868745133,
        "seek": 2908,
        "start": 33.199999999999996,
        "temperature": 0,
        "text": " He is going to create or talk about a project",
        "tokens": [
          50570,
          634,
          307,
          516,
          281,
          1884,
          420,
          751,
          466,
          257,
          1716,
          50702
        ]
      },
      {
        "avg_logprob": -0.22374911057321648,
        "compression_ratio": 1.7435897435897436,
        "end": 37.66,
        "id": 16,
        "no_speech_prob": 0.00006108159868745133,
        "seek": 2908,
        "start": 35.839999999999996,
        "temperature": 0,
        "text": " that he recently made in JavaScript",
        "tokens": [
          50702,
          300,
          415,
          3938,
          1027,
          294,
          15778,
          50793
        ]
      },
      {
        "avg_logprob": -0.22374911057321648,
        "compression_ratio": 1.7435897435897436,
        "end": 40.879999999999995,
        "id": 17,
        "no_speech_prob": 0.00006108159868745133,
        "seek": 2908,
        "start": 37.66,
        "temperature": 0,
        "text": " with his own from scratch neural network code",
        "tokens": [
          50793,
          365,
          702,
          1065,
          490,
          8459,
          18161,
          3209,
          3089,
          50954
        ]
      },
      {
        "avg_logprob": -0.22374911057321648,
        "compression_ratio": 1.7435897435897436,
        "end": 42.76,
        "id": 18,
        "no_speech_prob": 0.00006108159868745133,
        "seek": 2908,
        "start": 40.879999999999995,
        "temperature": 0,
        "text": " where he makes a color predictor.",
        "tokens": [
          50954,
          689,
          415,
          1669,
          257,
          2017,
          6069,
          284,
          13,
          51048
        ]
      },
      {
        "avg_logprob": -0.22374911057321648,
        "compression_ratio": 1.7435897435897436,
        "end": 44.96,
        "id": 19,
        "no_speech_prob": 0.00006108159868745133,
        "seek": 2908,
        "start": 42.76,
        "temperature": 0,
        "text": " And in fact, if you're interested in more",
        "tokens": [
          51048,
          400,
          294,
          1186,
          11,
          498,
          291,
          434,
          3102,
          294,
          544,
          51158
        ]
      },
      {
        "avg_logprob": -0.22374911057321648,
        "compression_ratio": 1.7435897435897436,
        "end": 47.84,
        "id": 20,
        "no_speech_prob": 0.00006108159868745133,
        "seek": 2908,
        "start": 44.96,
        "temperature": 0,
        "text": " about the color predictor, you can click on,",
        "tokens": [
          51158,
          466,
          264,
          2017,
          6069,
          284,
          11,
          291,
          393,
          2052,
          322,
          11,
          51302
        ]
      },
      {
        "avg_logprob": -0.22374911057321648,
        "compression_ratio": 1.7435897435897436,
        "end": 48.8,
        "id": 21,
        "no_speech_prob": 0.00006108159868745133,
        "seek": 2908,
        "start": 47.84,
        "temperature": 0,
        "text": " well, you can't click on that,",
        "tokens": [
          51302,
          731,
          11,
          291,
          393,
          380,
          2052,
          322,
          300,
          11,
          51350
        ]
      },
      {
        "avg_logprob": -0.22374911057321648,
        "compression_ratio": 1.7435897435897436,
        "end": 50.72,
        "id": 22,
        "no_speech_prob": 0.00006108159868745133,
        "seek": 2908,
        "start": 48.8,
        "temperature": 0,
        "text": " but I don't know, something will come up over here",
        "tokens": [
          51350,
          457,
          286,
          500,
          380,
          458,
          11,
          746,
          486,
          808,
          493,
          670,
          510,
          51446
        ]
      },
      {
        "avg_logprob": -0.22374911057321648,
        "compression_ratio": 1.7435897435897436,
        "end": 52.68,
        "id": 23,
        "no_speech_prob": 0.00006108159868745133,
        "seek": 2908,
        "start": 50.72,
        "temperature": 0,
        "text": " that will maybe suggest that video over there",
        "tokens": [
          51446,
          300,
          486,
          1310,
          3402,
          300,
          960,
          670,
          456,
          51544
        ]
      },
      {
        "avg_logprob": -0.22374911057321648,
        "compression_ratio": 1.7435897435897436,
        "end": 54.92,
        "id": 24,
        "no_speech_prob": 0.00006108159868745133,
        "seek": 2908,
        "start": 52.68,
        "temperature": 0,
        "text": " that the two of us made together for his channel.",
        "tokens": [
          51544,
          300,
          264,
          732,
          295,
          505,
          1027,
          1214,
          337,
          702,
          2269,
          13,
          51656
        ]
      },
      {
        "avg_logprob": -0.22374911057321648,
        "compression_ratio": 1.7435897435897436,
        "end": 56.879999999999995,
        "id": 25,
        "no_speech_prob": 0.00006108159868745133,
        "seek": 2908,
        "start": 54.92,
        "temperature": 0,
        "text": " So enjoy Jabril.",
        "tokens": [
          51656,
          407,
          2103,
          40319,
          24216,
          13,
          51754
        ]
      },
      {
        "avg_logprob": -0.22317231328863846,
        "compression_ratio": 1.6336633663366336,
        "end": 59.400000000000006,
        "id": 26,
        "no_speech_prob": 0.004829004872590303,
        "seek": 5688,
        "start": 56.88,
        "temperature": 0,
        "text": " Stay tuned, later this week,",
        "tokens": [
          50364,
          8691,
          10870,
          11,
          1780,
          341,
          1243,
          11,
          50490
        ]
      },
      {
        "avg_logprob": -0.22317231328863846,
        "compression_ratio": 1.6336633663366336,
        "end": 62.800000000000004,
        "id": 27,
        "no_speech_prob": 0.004829004872590303,
        "seek": 5688,
        "start": 59.400000000000006,
        "temperature": 0,
        "text": " I will do my own coding challenge",
        "tokens": [
          50490,
          286,
          486,
          360,
          452,
          1065,
          17720,
          3430,
          50660
        ]
      },
      {
        "avg_logprob": -0.22317231328863846,
        "compression_ratio": 1.6336633663366336,
        "end": 64,
        "id": 28,
        "no_speech_prob": 0.004829004872590303,
        "seek": 5688,
        "start": 62.800000000000004,
        "temperature": 0,
        "text": " to try to make my own color predictor.",
        "tokens": [
          50660,
          281,
          853,
          281,
          652,
          452,
          1065,
          2017,
          6069,
          284,
          13,
          50720
        ]
      },
      {
        "avg_logprob": -0.22317231328863846,
        "compression_ratio": 1.6336633663366336,
        "end": 65.32000000000001,
        "id": 29,
        "no_speech_prob": 0.004829004872590303,
        "seek": 5688,
        "start": 64,
        "temperature": 0,
        "text": " So we'll see how that goes.",
        "tokens": [
          50720,
          407,
          321,
          603,
          536,
          577,
          300,
          1709,
          13,
          50786
        ]
      },
      {
        "avg_logprob": -0.22317231328863846,
        "compression_ratio": 1.6336633663366336,
        "end": 66.8,
        "id": 30,
        "no_speech_prob": 0.004829004872590303,
        "seek": 5688,
        "start": 65.32000000000001,
        "temperature": 0,
        "text": " Thanks Jabril for being here",
        "tokens": [
          50786,
          2561,
          40319,
          24216,
          337,
          885,
          510,
          50860
        ]
      },
      {
        "avg_logprob": -0.22317231328863846,
        "compression_ratio": 1.6336633663366336,
        "end": 69.22,
        "id": 31,
        "no_speech_prob": 0.004829004872590303,
        "seek": 5688,
        "start": 66.8,
        "temperature": 0,
        "text": " and hope you enjoy this edited version",
        "tokens": [
          50860,
          293,
          1454,
          291,
          2103,
          341,
          23016,
          3037,
          50981
        ]
      },
      {
        "avg_logprob": -0.22317231328863846,
        "compression_ratio": 1.6336633663366336,
        "end": 70.64,
        "id": 32,
        "no_speech_prob": 0.004829004872590303,
        "seek": 5688,
        "start": 69.22,
        "temperature": 0,
        "text": " of the live stream we did together.",
        "tokens": [
          50981,
          295,
          264,
          1621,
          4309,
          321,
          630,
          1214,
          13,
          51052
        ]
      },
      {
        "avg_logprob": -0.22317231328863846,
        "compression_ratio": 1.6336633663366336,
        "end": 71.68,
        "id": 33,
        "no_speech_prob": 0.004829004872590303,
        "seek": 5688,
        "start": 70.64,
        "temperature": 0,
        "text": " Thanks very much.",
        "tokens": [
          51052,
          2561,
          588,
          709,
          13,
          51104
        ]
      },
      {
        "avg_logprob": -0.22317231328863846,
        "compression_ratio": 1.6336633663366336,
        "end": 72.92,
        "id": 34,
        "no_speech_prob": 0.004829004872590303,
        "seek": 5688,
        "start": 71.68,
        "temperature": 0,
        "text": " All right, howdy everyone.",
        "tokens": [
          51104,
          1057,
          558,
          11,
          577,
          3173,
          1518,
          13,
          51166
        ]
      },
      {
        "avg_logprob": -0.22317231328863846,
        "compression_ratio": 1.6336633663366336,
        "end": 74.84,
        "id": 35,
        "no_speech_prob": 0.004829004872590303,
        "seek": 5688,
        "start": 72.92,
        "temperature": 0,
        "text": " How is it going?",
        "tokens": [
          51166,
          1012,
          307,
          309,
          516,
          30,
          51262
        ]
      },
      {
        "avg_logprob": -0.22317231328863846,
        "compression_ratio": 1.6336633663366336,
        "end": 76.88,
        "id": 36,
        "no_speech_prob": 0.004829004872590303,
        "seek": 5688,
        "start": 74.84,
        "temperature": 0,
        "text": " So yeah, I'm going to give a little brief overview",
        "tokens": [
          51262,
          407,
          1338,
          11,
          286,
          478,
          516,
          281,
          976,
          257,
          707,
          5353,
          12492,
          51364
        ]
      },
      {
        "avg_logprob": -0.22317231328863846,
        "compression_ratio": 1.6336633663366336,
        "end": 79.76,
        "id": 37,
        "no_speech_prob": 0.004829004872590303,
        "seek": 5688,
        "start": 76.88,
        "temperature": 0,
        "text": " about who I am for those of you that do not know,",
        "tokens": [
          51364,
          466,
          567,
          286,
          669,
          337,
          729,
          295,
          291,
          300,
          360,
          406,
          458,
          11,
          51508
        ]
      },
      {
        "avg_logprob": -0.22317231328863846,
        "compression_ratio": 1.6336633663366336,
        "end": 81.88,
        "id": 38,
        "no_speech_prob": 0.004829004872590303,
        "seek": 5688,
        "start": 79.76,
        "temperature": 0,
        "text": " which I'm sure is all of you.",
        "tokens": [
          51508,
          597,
          286,
          478,
          988,
          307,
          439,
          295,
          291,
          13,
          51614
        ]
      },
      {
        "avg_logprob": -0.22317231328863846,
        "compression_ratio": 1.6336633663366336,
        "end": 83.4,
        "id": 39,
        "no_speech_prob": 0.004829004872590303,
        "seek": 5688,
        "start": 81.88,
        "temperature": 0,
        "text": " So my name is Jabril.",
        "tokens": [
          51614,
          407,
          452,
          1315,
          307,
          40319,
          24216,
          13,
          51690
        ]
      },
      {
        "avg_logprob": -0.22317231328863846,
        "compression_ratio": 1.6336633663366336,
        "end": 85.48,
        "id": 40,
        "no_speech_prob": 0.004829004872590303,
        "seek": 5688,
        "start": 83.4,
        "temperature": 0,
        "text": " I run a little YouTube channel called Jabril's",
        "tokens": [
          51690,
          286,
          1190,
          257,
          707,
          3088,
          2269,
          1219,
          40319,
          24216,
          311,
          51794
        ]
      },
      {
        "avg_logprob": -0.25161859263544495,
        "compression_ratio": 1.6771653543307086,
        "end": 86.48,
        "id": 41,
        "no_speech_prob": 0.09528722614049911,
        "seek": 8548,
        "start": 85.48,
        "temperature": 0,
        "text": " here on YouTube.",
        "tokens": [
          50364,
          510,
          322,
          3088,
          13,
          50414
        ]
      },
      {
        "avg_logprob": -0.25161859263544495,
        "compression_ratio": 1.6771653543307086,
        "end": 89.16000000000001,
        "id": 42,
        "no_speech_prob": 0.09528722614049911,
        "seek": 8548,
        "start": 86.48,
        "temperature": 0,
        "text": " And recently I converted my channel",
        "tokens": [
          50414,
          400,
          3938,
          286,
          16424,
          452,
          2269,
          50548
        ]
      },
      {
        "avg_logprob": -0.25161859263544495,
        "compression_ratio": 1.6771653543307086,
        "end": 90.7,
        "id": 43,
        "no_speech_prob": 0.09528722614049911,
        "seek": 8548,
        "start": 89.16000000000001,
        "temperature": 0,
        "text": " to focus on computer science.",
        "tokens": [
          50548,
          281,
          1879,
          322,
          3820,
          3497,
          13,
          50625
        ]
      },
      {
        "avg_logprob": -0.25161859263544495,
        "compression_ratio": 1.6771653543307086,
        "end": 92.24000000000001,
        "id": 44,
        "no_speech_prob": 0.09528722614049911,
        "seek": 8548,
        "start": 90.7,
        "temperature": 0,
        "text": " That happened in September.",
        "tokens": [
          50625,
          663,
          2011,
          294,
          7216,
          13,
          50702
        ]
      },
      {
        "avg_logprob": -0.25161859263544495,
        "compression_ratio": 1.6771653543307086,
        "end": 94.64,
        "id": 45,
        "no_speech_prob": 0.09528722614049911,
        "seek": 8548,
        "start": 92.24000000000001,
        "temperature": 0,
        "text": " And that was probably one of the best things I've ever done",
        "tokens": [
          50702,
          400,
          300,
          390,
          1391,
          472,
          295,
          264,
          1151,
          721,
          286,
          600,
          1562,
          1096,
          50822
        ]
      },
      {
        "avg_logprob": -0.25161859263544495,
        "compression_ratio": 1.6771653543307086,
        "end": 96.84,
        "id": 46,
        "no_speech_prob": 0.09528722614049911,
        "seek": 8548,
        "start": 94.64,
        "temperature": 0,
        "text": " because I learned that, you know,",
        "tokens": [
          50822,
          570,
          286,
          3264,
          300,
          11,
          291,
          458,
          11,
          50932
        ]
      },
      {
        "avg_logprob": -0.25161859263544495,
        "compression_ratio": 1.6771653543307086,
        "end": 98.68,
        "id": 47,
        "no_speech_prob": 0.09528722614049911,
        "seek": 8548,
        "start": 96.84,
        "temperature": 0,
        "text": " I had a great passion for writing code",
        "tokens": [
          50932,
          286,
          632,
          257,
          869,
          5418,
          337,
          3579,
          3089,
          51024
        ]
      },
      {
        "avg_logprob": -0.25161859263544495,
        "compression_ratio": 1.6771653543307086,
        "end": 101.68,
        "id": 48,
        "no_speech_prob": 0.09528722614049911,
        "seek": 8548,
        "start": 98.68,
        "temperature": 0,
        "text": " and making products and projects",
        "tokens": [
          51024,
          293,
          1455,
          3383,
          293,
          4455,
          51174
        ]
      },
      {
        "avg_logprob": -0.25161859263544495,
        "compression_ratio": 1.6771653543307086,
        "end": 103.36,
        "id": 49,
        "no_speech_prob": 0.09528722614049911,
        "seek": 8548,
        "start": 101.68,
        "temperature": 0,
        "text": " that were based on computer science.",
        "tokens": [
          51174,
          300,
          645,
          2361,
          322,
          3820,
          3497,
          13,
          51258
        ]
      },
      {
        "avg_logprob": -0.25161859263544495,
        "compression_ratio": 1.6771653543307086,
        "end": 107.80000000000001,
        "id": 50,
        "no_speech_prob": 0.09528722614049911,
        "seek": 8548,
        "start": 103.36,
        "temperature": 0,
        "text": " And so, yeah, I mean, obviously if I had a passion for that,",
        "tokens": [
          51258,
          400,
          370,
          11,
          1338,
          11,
          286,
          914,
          11,
          2745,
          498,
          286,
          632,
          257,
          5418,
          337,
          300,
          11,
          51480
        ]
      },
      {
        "avg_logprob": -0.25161859263544495,
        "compression_ratio": 1.6771653543307086,
        "end": 112.80000000000001,
        "id": 51,
        "no_speech_prob": 0.09528722614049911,
        "seek": 8548,
        "start": 107.80000000000001,
        "temperature": 0,
        "text": " it was easy to show that in video projects as well.",
        "tokens": [
          51480,
          309,
          390,
          1858,
          281,
          855,
          300,
          294,
          960,
          4455,
          382,
          731,
          13,
          51730
        ]
      },
      {
        "avg_logprob": -0.23133685772235577,
        "compression_ratio": 1.7137809187279152,
        "end": 116.52,
        "id": 52,
        "no_speech_prob": 0.0013244730653241277,
        "seek": 11280,
        "start": 113.11999999999999,
        "temperature": 0,
        "text": " And fast forward, so one of the biggest projects",
        "tokens": [
          50380,
          400,
          2370,
          2128,
          11,
          370,
          472,
          295,
          264,
          3880,
          4455,
          50550
        ]
      },
      {
        "avg_logprob": -0.23133685772235577,
        "compression_ratio": 1.7137809187279152,
        "end": 118.67999999999999,
        "id": 53,
        "no_speech_prob": 0.0013244730653241277,
        "seek": 11280,
        "start": 116.52,
        "temperature": 0,
        "text": " that I've produced to date is the Run Forest project",
        "tokens": [
          50550,
          300,
          286,
          600,
          7126,
          281,
          4002,
          307,
          264,
          8950,
          18124,
          1716,
          50658
        ]
      },
      {
        "avg_logprob": -0.23133685772235577,
        "compression_ratio": 1.7137809187279152,
        "end": 120.88,
        "id": 54,
        "no_speech_prob": 0.0013244730653241277,
        "seek": 11280,
        "start": 118.67999999999999,
        "temperature": 0,
        "text": " that got a lot of eyes.",
        "tokens": [
          50658,
          300,
          658,
          257,
          688,
          295,
          2575,
          13,
          50768
        ]
      },
      {
        "avg_logprob": -0.23133685772235577,
        "compression_ratio": 1.7137809187279152,
        "end": 122.44,
        "id": 55,
        "no_speech_prob": 0.0013244730653241277,
        "seek": 11280,
        "start": 120.88,
        "temperature": 0,
        "text": " I'm really grateful for.",
        "tokens": [
          50768,
          286,
          478,
          534,
          7941,
          337,
          13,
          50846
        ]
      },
      {
        "avg_logprob": -0.23133685772235577,
        "compression_ratio": 1.7137809187279152,
        "end": 125.36,
        "id": 56,
        "no_speech_prob": 0.0013244730653241277,
        "seek": 11280,
        "start": 122.44,
        "temperature": 0,
        "text": " And that really harnessed the power of machine learning,",
        "tokens": [
          50846,
          400,
          300,
          534,
          276,
          1083,
          10830,
          264,
          1347,
          295,
          3479,
          2539,
          11,
          50992
        ]
      },
      {
        "avg_logprob": -0.23133685772235577,
        "compression_ratio": 1.7137809187279152,
        "end": 128.4,
        "id": 57,
        "no_speech_prob": 0.0013244730653241277,
        "seek": 11280,
        "start": 125.36,
        "temperature": 0,
        "text": " which is a really big buzzword these days.",
        "tokens": [
          50992,
          597,
          307,
          257,
          534,
          955,
          13036,
          7462,
          613,
          1708,
          13,
          51144
        ]
      },
      {
        "avg_logprob": -0.23133685772235577,
        "compression_ratio": 1.7137809187279152,
        "end": 129.84,
        "id": 58,
        "no_speech_prob": 0.0013244730653241277,
        "seek": 11280,
        "start": 128.4,
        "temperature": 0,
        "text": " But yeah, that's pretty much the overview.",
        "tokens": [
          51144,
          583,
          1338,
          11,
          300,
          311,
          1238,
          709,
          264,
          12492,
          13,
          51216
        ]
      },
      {
        "avg_logprob": -0.23133685772235577,
        "compression_ratio": 1.7137809187279152,
        "end": 131.51999999999998,
        "id": 59,
        "no_speech_prob": 0.0013244730653241277,
        "seek": 11280,
        "start": 129.84,
        "temperature": 0,
        "text": " I spent about nine months learning",
        "tokens": [
          51216,
          286,
          4418,
          466,
          4949,
          2493,
          2539,
          51300
        ]
      },
      {
        "avg_logprob": -0.23133685772235577,
        "compression_ratio": 1.7137809187279152,
        "end": 135.44,
        "id": 60,
        "no_speech_prob": 0.0013244730653241277,
        "seek": 11280,
        "start": 131.51999999999998,
        "temperature": 0,
        "text": " how to write machine learning algorithms from scratch",
        "tokens": [
          51300,
          577,
          281,
          2464,
          3479,
          2539,
          14642,
          490,
          8459,
          51496
        ]
      },
      {
        "avg_logprob": -0.23133685772235577,
        "compression_ratio": 1.7137809187279152,
        "end": 139.51999999999998,
        "id": 61,
        "no_speech_prob": 0.0013244730653241277,
        "seek": 11280,
        "start": 135.44,
        "temperature": 0,
        "text": " because it was something, AI is really cool to me, I think.",
        "tokens": [
          51496,
          570,
          309,
          390,
          746,
          11,
          7318,
          307,
          534,
          1627,
          281,
          385,
          11,
          286,
          519,
          13,
          51700
        ]
      },
      {
        "avg_logprob": -0.23133685772235577,
        "compression_ratio": 1.7137809187279152,
        "end": 142.16,
        "id": 62,
        "no_speech_prob": 0.0013244730653241277,
        "seek": 11280,
        "start": 139.51999999999998,
        "temperature": 0,
        "text": " And so, yeah, the Run Forest was released.",
        "tokens": [
          51700,
          400,
          370,
          11,
          1338,
          11,
          264,
          8950,
          18124,
          390,
          4736,
          13,
          51832
        ]
      },
      {
        "avg_logprob": -0.22183000124417818,
        "compression_ratio": 1.662280701754386,
        "end": 147.16,
        "id": 63,
        "no_speech_prob": 0.0005702800117433071,
        "seek": 14216,
        "start": 142.16,
        "temperature": 0,
        "text": " And today, what we're gonna do is we're going to examine",
        "tokens": [
          50364,
          400,
          965,
          11,
          437,
          321,
          434,
          799,
          360,
          307,
          321,
          434,
          516,
          281,
          17496,
          50614
        ]
      },
      {
        "avg_logprob": -0.22183000124417818,
        "compression_ratio": 1.662280701754386,
        "end": 151.44,
        "id": 64,
        "no_speech_prob": 0.0005702800117433071,
        "seek": 14216,
        "start": 147.16,
        "temperature": 0,
        "text": " this really simple JavaScript machine learning application,",
        "tokens": [
          50614,
          341,
          534,
          2199,
          15778,
          3479,
          2539,
          3861,
          11,
          50828
        ]
      },
      {
        "avg_logprob": -0.22183000124417818,
        "compression_ratio": 1.662280701754386,
        "end": 153.48,
        "id": 65,
        "no_speech_prob": 0.0005702800117433071,
        "seek": 14216,
        "start": 151.44,
        "temperature": 0,
        "text": " kind of how it was done.",
        "tokens": [
          50828,
          733,
          295,
          577,
          309,
          390,
          1096,
          13,
          50930
        ]
      },
      {
        "avg_logprob": -0.22183000124417818,
        "compression_ratio": 1.662280701754386,
        "end": 156.96,
        "id": 66,
        "no_speech_prob": 0.0005702800117433071,
        "seek": 14216,
        "start": 153.48,
        "temperature": 0,
        "text": " It's another machine learning application",
        "tokens": [
          50930,
          467,
          311,
          1071,
          3479,
          2539,
          3861,
          51104
        ]
      },
      {
        "avg_logprob": -0.22183000124417818,
        "compression_ratio": 1.662280701754386,
        "end": 158.5,
        "id": 67,
        "no_speech_prob": 0.0005702800117433071,
        "seek": 14216,
        "start": 156.96,
        "temperature": 0,
        "text": " written from scratch.",
        "tokens": [
          51104,
          3720,
          490,
          8459,
          13,
          51181
        ]
      },
      {
        "avg_logprob": -0.22183000124417818,
        "compression_ratio": 1.662280701754386,
        "end": 160.35999999999999,
        "id": 68,
        "no_speech_prob": 0.0005702800117433071,
        "seek": 14216,
        "start": 158.5,
        "temperature": 0,
        "text": " So we're gonna take a look at the code",
        "tokens": [
          51181,
          407,
          321,
          434,
          799,
          747,
          257,
          574,
          412,
          264,
          3089,
          51274
        ]
      },
      {
        "avg_logprob": -0.22183000124417818,
        "compression_ratio": 1.662280701754386,
        "end": 161.78,
        "id": 69,
        "no_speech_prob": 0.0005702800117433071,
        "seek": 14216,
        "start": 160.35999999999999,
        "temperature": 0,
        "text": " and all that good stuff.",
        "tokens": [
          51274,
          293,
          439,
          300,
          665,
          1507,
          13,
          51345
        ]
      },
      {
        "avg_logprob": -0.22183000124417818,
        "compression_ratio": 1.662280701754386,
        "end": 164.6,
        "id": 70,
        "no_speech_prob": 0.0005702800117433071,
        "seek": 14216,
        "start": 161.78,
        "temperature": 0,
        "text": " So let's get into this.",
        "tokens": [
          51345,
          407,
          718,
          311,
          483,
          666,
          341,
          13,
          51486
        ]
      },
      {
        "avg_logprob": -0.22183000124417818,
        "compression_ratio": 1.662280701754386,
        "end": 166.68,
        "id": 71,
        "no_speech_prob": 0.0005702800117433071,
        "seek": 14216,
        "start": 164.6,
        "temperature": 0,
        "text": " So here we have this example.",
        "tokens": [
          51486,
          407,
          510,
          321,
          362,
          341,
          1365,
          13,
          51590
        ]
      },
      {
        "avg_logprob": -0.22183000124417818,
        "compression_ratio": 1.662280701754386,
        "end": 171,
        "id": 72,
        "no_speech_prob": 0.0005702800117433071,
        "seek": 14216,
        "start": 166.68,
        "temperature": 0,
        "text": " It's what I call a color predictor neural network demo.",
        "tokens": [
          51590,
          467,
          311,
          437,
          286,
          818,
          257,
          2017,
          6069,
          284,
          18161,
          3209,
          10723,
          13,
          51806
        ]
      },
      {
        "avg_logprob": -0.23015578733671696,
        "compression_ratio": 1.628099173553719,
        "end": 173.64,
        "id": 73,
        "no_speech_prob": 0.00006605098315048963,
        "seek": 17100,
        "start": 171.04,
        "temperature": 0,
        "text": " And it asks you a simple question.",
        "tokens": [
          50366,
          400,
          309,
          8962,
          291,
          257,
          2199,
          1168,
          13,
          50496
        ]
      },
      {
        "avg_logprob": -0.23015578733671696,
        "compression_ratio": 1.628099173553719,
        "end": 177.4,
        "id": 74,
        "no_speech_prob": 0.00006605098315048963,
        "seek": 17100,
        "start": 173.64,
        "temperature": 0,
        "text": " Does white or black look better over this color?",
        "tokens": [
          50496,
          4402,
          2418,
          420,
          2211,
          574,
          1101,
          670,
          341,
          2017,
          30,
          50684
        ]
      },
      {
        "avg_logprob": -0.23015578733671696,
        "compression_ratio": 1.628099173553719,
        "end": 180.2,
        "id": 75,
        "no_speech_prob": 0.00006605098315048963,
        "seek": 17100,
        "start": 177.4,
        "temperature": 0,
        "text": " And so the color is within the circle",
        "tokens": [
          50684,
          400,
          370,
          264,
          2017,
          307,
          1951,
          264,
          6329,
          50824
        ]
      },
      {
        "avg_logprob": -0.23015578733671696,
        "compression_ratio": 1.628099173553719,
        "end": 181.8,
        "id": 76,
        "no_speech_prob": 0.00006605098315048963,
        "seek": 17100,
        "start": 180.2,
        "temperature": 0,
        "text": " and it's randomly generated.",
        "tokens": [
          50824,
          293,
          309,
          311,
          16979,
          10833,
          13,
          50904
        ]
      },
      {
        "avg_logprob": -0.23015578733671696,
        "compression_ratio": 1.628099173553719,
        "end": 184.36,
        "id": 77,
        "no_speech_prob": 0.00006605098315048963,
        "seek": 17100,
        "start": 181.8,
        "temperature": 0,
        "text": " Okay, so what's important for us to start",
        "tokens": [
          50904,
          1033,
          11,
          370,
          437,
          311,
          1021,
          337,
          505,
          281,
          722,
          51032
        ]
      },
      {
        "avg_logprob": -0.23015578733671696,
        "compression_ratio": 1.628099173553719,
        "end": 187.28,
        "id": 78,
        "no_speech_prob": 0.00006605098315048963,
        "seek": 17100,
        "start": 184.36,
        "temperature": 0,
        "text": " before we can get into the application,",
        "tokens": [
          51032,
          949,
          321,
          393,
          483,
          666,
          264,
          3861,
          11,
          51178
        ]
      },
      {
        "avg_logprob": -0.23015578733671696,
        "compression_ratio": 1.628099173553719,
        "end": 190.66,
        "id": 79,
        "no_speech_prob": 0.00006605098315048963,
        "seek": 17100,
        "start": 187.28,
        "temperature": 0,
        "text": " we have to understand the main computational part",
        "tokens": [
          51178,
          321,
          362,
          281,
          1223,
          264,
          2135,
          28270,
          644,
          51347
        ]
      },
      {
        "avg_logprob": -0.23015578733671696,
        "compression_ratio": 1.628099173553719,
        "end": 192.12,
        "id": 80,
        "no_speech_prob": 0.00006605098315048963,
        "seek": 17100,
        "start": 190.66,
        "temperature": 0,
        "text": " of this application.",
        "tokens": [
          51347,
          295,
          341,
          3861,
          13,
          51420
        ]
      },
      {
        "avg_logprob": -0.23015578733671696,
        "compression_ratio": 1.628099173553719,
        "end": 193.68,
        "id": 81,
        "no_speech_prob": 0.00006605098315048963,
        "seek": 17100,
        "start": 192.12,
        "temperature": 0,
        "text": " So we have a color.",
        "tokens": [
          51420,
          407,
          321,
          362,
          257,
          2017,
          13,
          51498
        ]
      },
      {
        "avg_logprob": -0.23015578733671696,
        "compression_ratio": 1.628099173553719,
        "end": 195.44,
        "id": 82,
        "no_speech_prob": 0.00006605098315048963,
        "seek": 17100,
        "start": 193.68,
        "temperature": 0,
        "text": " And as you know, colors are,",
        "tokens": [
          51498,
          400,
          382,
          291,
          458,
          11,
          4577,
          366,
          11,
          51586
        ]
      },
      {
        "avg_logprob": -0.23015578733671696,
        "compression_ratio": 1.628099173553719,
        "end": 198.76,
        "id": 83,
        "no_speech_prob": 0.00006605098315048963,
        "seek": 17100,
        "start": 195.44,
        "temperature": 0,
        "text": " they're represented as a vector of three,",
        "tokens": [
          51586,
          436,
          434,
          10379,
          382,
          257,
          8062,
          295,
          1045,
          11,
          51752
        ]
      },
      {
        "avg_logprob": -0.2029042292122889,
        "compression_ratio": 1.5047619047619047,
        "end": 201.23999999999998,
        "id": 84,
        "no_speech_prob": 0.00011959757830481976,
        "seek": 19876,
        "start": 198.76,
        "temperature": 0,
        "text": " or sometimes four if you include the alpha,",
        "tokens": [
          50364,
          420,
          2171,
          1451,
          498,
          291,
          4090,
          264,
          8961,
          11,
          50488
        ]
      },
      {
        "avg_logprob": -0.2029042292122889,
        "compression_ratio": 1.5047619047619047,
        "end": 202.64,
        "id": 85,
        "no_speech_prob": 0.00011959757830481976,
        "seek": 19876,
        "start": 201.23999999999998,
        "temperature": 0,
        "text": " but we're not including the alpha.",
        "tokens": [
          50488,
          457,
          321,
          434,
          406,
          3009,
          264,
          8961,
          13,
          50558
        ]
      },
      {
        "avg_logprob": -0.2029042292122889,
        "compression_ratio": 1.5047619047619047,
        "end": 205.56,
        "id": 86,
        "no_speech_prob": 0.00011959757830481976,
        "seek": 19876,
        "start": 202.64,
        "temperature": 0,
        "text": " We're only gonna use the RGB values.",
        "tokens": [
          50558,
          492,
          434,
          787,
          799,
          764,
          264,
          31231,
          4190,
          13,
          50704
        ]
      },
      {
        "avg_logprob": -0.2029042292122889,
        "compression_ratio": 1.5047619047619047,
        "end": 210.56,
        "id": 87,
        "no_speech_prob": 0.00011959757830481976,
        "seek": 19876,
        "start": 205.56,
        "temperature": 0,
        "text": " So we have our inputs, which is three.",
        "tokens": [
          50704,
          407,
          321,
          362,
          527,
          15743,
          11,
          597,
          307,
          1045,
          13,
          50954
        ]
      },
      {
        "avg_logprob": -0.2029042292122889,
        "compression_ratio": 1.5047619047619047,
        "end": 211.6,
        "id": 88,
        "no_speech_prob": 0.00011959757830481976,
        "seek": 19876,
        "start": 210.64,
        "temperature": 0,
        "text": " Is that on frame?",
        "tokens": [
          50958,
          1119,
          300,
          322,
          3920,
          30,
          51006
        ]
      },
      {
        "avg_logprob": -0.2029042292122889,
        "compression_ratio": 1.5047619047619047,
        "end": 212.88,
        "id": 89,
        "no_speech_prob": 0.00011959757830481976,
        "seek": 19876,
        "start": 211.6,
        "temperature": 0,
        "text": " Yes, all right.",
        "tokens": [
          51006,
          1079,
          11,
          439,
          558,
          13,
          51070
        ]
      },
      {
        "avg_logprob": -0.2029042292122889,
        "compression_ratio": 1.5047619047619047,
        "end": 217,
        "id": 90,
        "no_speech_prob": 0.00011959757830481976,
        "seek": 19876,
        "start": 212.88,
        "temperature": 0,
        "text": " So we have red, green, and blue.",
        "tokens": [
          51070,
          407,
          321,
          362,
          2182,
          11,
          3092,
          11,
          293,
          3344,
          13,
          51276
        ]
      },
      {
        "avg_logprob": -0.2029042292122889,
        "compression_ratio": 1.5047619047619047,
        "end": 222,
        "id": 91,
        "no_speech_prob": 0.00011959757830481976,
        "seek": 19876,
        "start": 217,
        "temperature": 0,
        "text": " And these are values between zero and 255",
        "tokens": [
          51276,
          400,
          613,
          366,
          4190,
          1296,
          4018,
          293,
          3552,
          20,
          51526
        ]
      },
      {
        "avg_logprob": -0.2029042292122889,
        "compression_ratio": 1.5047619047619047,
        "end": 223.14,
        "id": 92,
        "no_speech_prob": 0.00011959757830481976,
        "seek": 19876,
        "start": 222,
        "temperature": 0,
        "text": " for each input.",
        "tokens": [
          51526,
          337,
          1184,
          4846,
          13,
          51583
        ]
      },
      {
        "avg_logprob": -0.2029042292122889,
        "compression_ratio": 1.5047619047619047,
        "end": 227.72,
        "id": 93,
        "no_speech_prob": 0.00011959757830481976,
        "seek": 19876,
        "start": 224.12,
        "temperature": 0,
        "text": " So we need to build a neural network",
        "tokens": [
          51632,
          407,
          321,
          643,
          281,
          1322,
          257,
          18161,
          3209,
          51812
        ]
      },
      {
        "avg_logprob": -0.23003210232952448,
        "compression_ratio": 1.7470355731225296,
        "end": 229.6,
        "id": 94,
        "no_speech_prob": 0.0004238806723151356,
        "seek": 22772,
        "start": 227.72,
        "temperature": 0,
        "text": " that will be able to take these inputs",
        "tokens": [
          50364,
          300,
          486,
          312,
          1075,
          281,
          747,
          613,
          15743,
          50458
        ]
      },
      {
        "avg_logprob": -0.23003210232952448,
        "compression_ratio": 1.7470355731225296,
        "end": 231.52,
        "id": 95,
        "no_speech_prob": 0.0004238806723151356,
        "seek": 22772,
        "start": 229.6,
        "temperature": 0,
        "text": " and then do a computation on them",
        "tokens": [
          50458,
          293,
          550,
          360,
          257,
          24903,
          322,
          552,
          50554
        ]
      },
      {
        "avg_logprob": -0.23003210232952448,
        "compression_ratio": 1.7470355731225296,
        "end": 233.44,
        "id": 96,
        "no_speech_prob": 0.0004238806723151356,
        "seek": 22772,
        "start": 231.52,
        "temperature": 0,
        "text": " and then pass into an output to predict",
        "tokens": [
          50554,
          293,
          550,
          1320,
          666,
          364,
          5598,
          281,
          6069,
          50650
        ]
      },
      {
        "avg_logprob": -0.23003210232952448,
        "compression_ratio": 1.7470355731225296,
        "end": 236.2,
        "id": 97,
        "no_speech_prob": 0.0004238806723151356,
        "seek": 22772,
        "start": 233.44,
        "temperature": 0,
        "text": " if it looks better over black or white.",
        "tokens": [
          50650,
          498,
          309,
          1542,
          1101,
          670,
          2211,
          420,
          2418,
          13,
          50788
        ]
      },
      {
        "avg_logprob": -0.23003210232952448,
        "compression_ratio": 1.7470355731225296,
        "end": 238.84,
        "id": 98,
        "no_speech_prob": 0.0004238806723151356,
        "seek": 22772,
        "start": 237.16,
        "temperature": 0,
        "text": " So let's first draw our outputs.",
        "tokens": [
          50836,
          407,
          718,
          311,
          700,
          2642,
          527,
          23930,
          13,
          50920
        ]
      },
      {
        "avg_logprob": -0.23003210232952448,
        "compression_ratio": 1.7470355731225296,
        "end": 241.24,
        "id": 99,
        "no_speech_prob": 0.0004238806723151356,
        "seek": 22772,
        "start": 238.84,
        "temperature": 0,
        "text": " Just make sure it's all in frame.",
        "tokens": [
          50920,
          1449,
          652,
          988,
          309,
          311,
          439,
          294,
          3920,
          13,
          51040
        ]
      },
      {
        "avg_logprob": -0.23003210232952448,
        "compression_ratio": 1.7470355731225296,
        "end": 242.56,
        "id": 100,
        "no_speech_prob": 0.0004238806723151356,
        "seek": 22772,
        "start": 241.24,
        "temperature": 0,
        "text": " Yes, that's good.",
        "tokens": [
          51040,
          1079,
          11,
          300,
          311,
          665,
          13,
          51106
        ]
      },
      {
        "avg_logprob": -0.23003210232952448,
        "compression_ratio": 1.7470355731225296,
        "end": 245.76,
        "id": 101,
        "no_speech_prob": 0.0004238806723151356,
        "seek": 22772,
        "start": 243.48,
        "temperature": 0,
        "text": " And this is gonna be, it predicts black",
        "tokens": [
          51152,
          400,
          341,
          307,
          799,
          312,
          11,
          309,
          6069,
          82,
          2211,
          51266
        ]
      },
      {
        "avg_logprob": -0.23003210232952448,
        "compression_ratio": 1.7470355731225296,
        "end": 247.48,
        "id": 102,
        "no_speech_prob": 0.0004238806723151356,
        "seek": 22772,
        "start": 245.76,
        "temperature": 0,
        "text": " and this predicts white.",
        "tokens": [
          51266,
          293,
          341,
          6069,
          82,
          2418,
          13,
          51352
        ]
      },
      {
        "avg_logprob": -0.23003210232952448,
        "compression_ratio": 1.7470355731225296,
        "end": 250.16,
        "id": 103,
        "no_speech_prob": 0.0004238806723151356,
        "seek": 22772,
        "start": 247.48,
        "temperature": 0,
        "text": " So now we need a hidden layer,",
        "tokens": [
          51352,
          407,
          586,
          321,
          643,
          257,
          7633,
          4583,
          11,
          51486
        ]
      },
      {
        "avg_logprob": -0.23003210232952448,
        "compression_ratio": 1.7470355731225296,
        "end": 252.26,
        "id": 104,
        "no_speech_prob": 0.0004238806723151356,
        "seek": 22772,
        "start": 250.16,
        "temperature": 0,
        "text": " is what we call a hidden layer",
        "tokens": [
          51486,
          307,
          437,
          321,
          818,
          257,
          7633,
          4583,
          51591
        ]
      },
      {
        "avg_logprob": -0.23003210232952448,
        "compression_ratio": 1.7470355731225296,
        "end": 254.24,
        "id": 105,
        "no_speech_prob": 0.0004238806723151356,
        "seek": 22772,
        "start": 252.26,
        "temperature": 0,
        "text": " with artificial neural networks in the middle",
        "tokens": [
          51591,
          365,
          11677,
          18161,
          9590,
          294,
          264,
          2808,
          51690
        ]
      },
      {
        "avg_logprob": -0.23003210232952448,
        "compression_ratio": 1.7470355731225296,
        "end": 256.16,
        "id": 106,
        "no_speech_prob": 0.0004238806723151356,
        "seek": 22772,
        "start": 254.24,
        "temperature": 0,
        "text": " that does the computation part.",
        "tokens": [
          51690,
          300,
          775,
          264,
          24903,
          644,
          13,
          51786
        ]
      },
      {
        "avg_logprob": -0.20247548227091783,
        "compression_ratio": 1.6964980544747081,
        "end": 257.72,
        "id": 107,
        "no_speech_prob": 0.0008295803563669324,
        "seek": 25616,
        "start": 256.16,
        "temperature": 0,
        "text": " And this is our guess.",
        "tokens": [
          50364,
          400,
          341,
          307,
          527,
          2041,
          13,
          50442
        ]
      },
      {
        "avg_logprob": -0.20247548227091783,
        "compression_ratio": 1.6964980544747081,
        "end": 261.32000000000005,
        "id": 108,
        "no_speech_prob": 0.0008295803563669324,
        "seek": 25616,
        "start": 258.8,
        "temperature": 0,
        "text": " And so we are just gonna arbitrarily",
        "tokens": [
          50496,
          400,
          370,
          321,
          366,
          445,
          799,
          19071,
          3289,
          50622
        ]
      },
      {
        "avg_logprob": -0.20247548227091783,
        "compression_ratio": 1.6964980544747081,
        "end": 264.40000000000003,
        "id": 109,
        "no_speech_prob": 0.0008295803563669324,
        "seek": 25616,
        "start": 261.32000000000005,
        "temperature": 0,
        "text": " just duplicate the same size of our inputs",
        "tokens": [
          50622,
          445,
          23976,
          264,
          912,
          2744,
          295,
          527,
          15743,
          50776
        ]
      },
      {
        "avg_logprob": -0.20247548227091783,
        "compression_ratio": 1.6964980544747081,
        "end": 265.64000000000004,
        "id": 110,
        "no_speech_prob": 0.0008295803563669324,
        "seek": 25616,
        "start": 264.40000000000003,
        "temperature": 0,
        "text": " for our hidden layer nodes.",
        "tokens": [
          50776,
          337,
          527,
          7633,
          4583,
          13891,
          13,
          50838
        ]
      },
      {
        "avg_logprob": -0.20247548227091783,
        "compression_ratio": 1.6964980544747081,
        "end": 267.20000000000005,
        "id": 111,
        "no_speech_prob": 0.0008295803563669324,
        "seek": 25616,
        "start": 265.64000000000004,
        "temperature": 0,
        "text": " We're just gonna say three.",
        "tokens": [
          50838,
          492,
          434,
          445,
          799,
          584,
          1045,
          13,
          50916
        ]
      },
      {
        "avg_logprob": -0.20247548227091783,
        "compression_ratio": 1.6964980544747081,
        "end": 269.16,
        "id": 112,
        "no_speech_prob": 0.0008295803563669324,
        "seek": 25616,
        "start": 267.20000000000005,
        "temperature": 0,
        "text": " It's a good place to start.",
        "tokens": [
          50916,
          467,
          311,
          257,
          665,
          1081,
          281,
          722,
          13,
          51014
        ]
      },
      {
        "avg_logprob": -0.20247548227091783,
        "compression_ratio": 1.6964980544747081,
        "end": 270.56,
        "id": 113,
        "no_speech_prob": 0.0008295803563669324,
        "seek": 25616,
        "start": 269.16,
        "temperature": 0,
        "text": " If we're really serious about this,",
        "tokens": [
          51014,
          759,
          321,
          434,
          534,
          3156,
          466,
          341,
          11,
          51084
        ]
      },
      {
        "avg_logprob": -0.20247548227091783,
        "compression_ratio": 1.6964980544747081,
        "end": 273.12,
        "id": 114,
        "no_speech_prob": 0.0008295803563669324,
        "seek": 25616,
        "start": 270.56,
        "temperature": 0,
        "text": " we could expand it, try five, try seven,",
        "tokens": [
          51084,
          321,
          727,
          5268,
          309,
          11,
          853,
          1732,
          11,
          853,
          3407,
          11,
          51212
        ]
      },
      {
        "avg_logprob": -0.20247548227091783,
        "compression_ratio": 1.6964980544747081,
        "end": 275.88,
        "id": 115,
        "no_speech_prob": 0.0008295803563669324,
        "seek": 25616,
        "start": 273.12,
        "temperature": 0,
        "text": " and just log the results for all of them",
        "tokens": [
          51212,
          293,
          445,
          3565,
          264,
          3542,
          337,
          439,
          295,
          552,
          51350
        ]
      },
      {
        "avg_logprob": -0.20247548227091783,
        "compression_ratio": 1.6964980544747081,
        "end": 277.04,
        "id": 116,
        "no_speech_prob": 0.0008295803563669324,
        "seek": 25616,
        "start": 275.88,
        "temperature": 0,
        "text": " and see which one works the best.",
        "tokens": [
          51350,
          293,
          536,
          597,
          472,
          1985,
          264,
          1151,
          13,
          51408
        ]
      },
      {
        "avg_logprob": -0.20247548227091783,
        "compression_ratio": 1.6964980544747081,
        "end": 279,
        "id": 117,
        "no_speech_prob": 0.0008295803563669324,
        "seek": 25616,
        "start": 277.04,
        "temperature": 0,
        "text": " But we're just gonna say three for this example,",
        "tokens": [
          51408,
          583,
          321,
          434,
          445,
          799,
          584,
          1045,
          337,
          341,
          1365,
          11,
          51506
        ]
      },
      {
        "avg_logprob": -0.20247548227091783,
        "compression_ratio": 1.6964980544747081,
        "end": 280.34000000000003,
        "id": 118,
        "no_speech_prob": 0.0008295803563669324,
        "seek": 25616,
        "start": 279,
        "temperature": 0,
        "text": " make it nice and simple.",
        "tokens": [
          51506,
          652,
          309,
          1481,
          293,
          2199,
          13,
          51573
        ]
      },
      {
        "avg_logprob": -0.20247548227091783,
        "compression_ratio": 1.6964980544747081,
        "end": 283.94000000000005,
        "id": 119,
        "no_speech_prob": 0.0008295803563669324,
        "seek": 25616,
        "start": 281.64000000000004,
        "temperature": 0,
        "text": " And so we have our RGB.",
        "tokens": [
          51638,
          400,
          370,
          321,
          362,
          527,
          31231,
          13,
          51753
        ]
      },
      {
        "avg_logprob": -0.2892905706646799,
        "compression_ratio": 1.5411764705882354,
        "end": 287.18,
        "id": 120,
        "no_speech_prob": 0.0010004836367443204,
        "seek": 28394,
        "start": 283.94,
        "temperature": 0,
        "text": " And if we go back to our example,",
        "tokens": [
          50364,
          400,
          498,
          321,
          352,
          646,
          281,
          527,
          1365,
          11,
          50526
        ]
      },
      {
        "avg_logprob": -0.2892905706646799,
        "compression_ratio": 1.5411764705882354,
        "end": 289.02,
        "id": 121,
        "no_speech_prob": 0.0010004836367443204,
        "seek": 28394,
        "start": 287.18,
        "temperature": 0,
        "text": " what's happening here is",
        "tokens": [
          50526,
          437,
          311,
          2737,
          510,
          307,
          50618
        ]
      },
      {
        "avg_logprob": -0.2892905706646799,
        "compression_ratio": 1.5411764705882354,
        "end": 293.7,
        "id": 122,
        "no_speech_prob": 0.0010004836367443204,
        "seek": 28394,
        "start": 290.42,
        "temperature": 0,
        "text": " there's a computation that happens",
        "tokens": [
          50688,
          456,
          311,
          257,
          24903,
          300,
          2314,
          50852
        ]
      },
      {
        "avg_logprob": -0.2892905706646799,
        "compression_ratio": 1.5411764705882354,
        "end": 297.02,
        "id": 123,
        "no_speech_prob": 0.0010004836367443204,
        "seek": 28394,
        "start": 295.18,
        "temperature": 0,
        "text": " within our network.",
        "tokens": [
          50926,
          1951,
          527,
          3209,
          13,
          51018
        ]
      },
      {
        "avg_logprob": -0.2892905706646799,
        "compression_ratio": 1.5411764705882354,
        "end": 303.46,
        "id": 124,
        "no_speech_prob": 0.0010004836367443204,
        "seek": 28394,
        "start": 298.46,
        "temperature": 0,
        "text": " Two, three, one, two, three, one, two, three.",
        "tokens": [
          51090,
          4453,
          11,
          1045,
          11,
          472,
          11,
          732,
          11,
          1045,
          11,
          472,
          11,
          732,
          11,
          1045,
          13,
          51340
        ]
      },
      {
        "avg_logprob": -0.2892905706646799,
        "compression_ratio": 1.5411764705882354,
        "end": 305.82,
        "id": 125,
        "no_speech_prob": 0.0010004836367443204,
        "seek": 28394,
        "start": 304.06,
        "temperature": 0,
        "text": " And Dan, feel free to interrupt",
        "tokens": [
          51370,
          400,
          3394,
          11,
          841,
          1737,
          281,
          12729,
          51458
        ]
      },
      {
        "avg_logprob": -0.2892905706646799,
        "compression_ratio": 1.5411764705882354,
        "end": 308.54,
        "id": 126,
        "no_speech_prob": 0.0010004836367443204,
        "seek": 28394,
        "start": 305.82,
        "temperature": 0,
        "text": " if you think that I'm a little off base with anything.",
        "tokens": [
          51458,
          498,
          291,
          519,
          300,
          286,
          478,
          257,
          707,
          766,
          3096,
          365,
          1340,
          13,
          51594
        ]
      },
      {
        "avg_logprob": -0.2892905706646799,
        "compression_ratio": 1.5411764705882354,
        "end": 310.46,
        "id": 127,
        "no_speech_prob": 0.0010004836367443204,
        "seek": 28394,
        "start": 309.62,
        "temperature": 0,
        "text": " Uh-huh.",
        "tokens": [
          51648,
          4019,
          12,
          18710,
          13,
          51690
        ]
      },
      {
        "avg_logprob": -0.2892905706646799,
        "compression_ratio": 1.5411764705882354,
        "end": 312.62,
        "id": 128,
        "no_speech_prob": 0.0010004836367443204,
        "seek": 28394,
        "start": 311.78,
        "temperature": 0,
        "text": " Uh-huh.",
        "tokens": [
          51756,
          4019,
          12,
          18710,
          13,
          51798
        ]
      },
      {
        "avg_logprob": -0.23442750141538424,
        "compression_ratio": 1.6126482213438735,
        "end": 315.18,
        "id": 129,
        "no_speech_prob": 0.00039820754318498075,
        "seek": 31394,
        "start": 313.94,
        "temperature": 0,
        "text": " Uh-huh.",
        "tokens": [
          50364,
          4019,
          12,
          18710,
          13,
          50426
        ]
      },
      {
        "avg_logprob": -0.23442750141538424,
        "compression_ratio": 1.6126482213438735,
        "end": 317.3,
        "id": 130,
        "no_speech_prob": 0.00039820754318498075,
        "seek": 31394,
        "start": 315.18,
        "temperature": 0,
        "text": " Okay, so this, what did I just do?",
        "tokens": [
          50426,
          1033,
          11,
          370,
          341,
          11,
          437,
          630,
          286,
          445,
          360,
          30,
          50532
        ]
      },
      {
        "avg_logprob": -0.23442750141538424,
        "compression_ratio": 1.6126482213438735,
        "end": 318.36,
        "id": 131,
        "no_speech_prob": 0.00039820754318498075,
        "seek": 31394,
        "start": 317.3,
        "temperature": 0,
        "text": " That looks really confusing.",
        "tokens": [
          50532,
          663,
          1542,
          534,
          13181,
          13,
          50585
        ]
      },
      {
        "avg_logprob": -0.23442750141538424,
        "compression_ratio": 1.6126482213438735,
        "end": 320.02,
        "id": 132,
        "no_speech_prob": 0.00039820754318498075,
        "seek": 31394,
        "start": 318.36,
        "temperature": 0,
        "text": " But it's actually really simple.",
        "tokens": [
          50585,
          583,
          309,
          311,
          767,
          534,
          2199,
          13,
          50668
        ]
      },
      {
        "avg_logprob": -0.23442750141538424,
        "compression_ratio": 1.6126482213438735,
        "end": 324.7,
        "id": 133,
        "no_speech_prob": 0.00039820754318498075,
        "seek": 31394,
        "start": 320.02,
        "temperature": 0,
        "text": " So we need to somehow get our inputs,",
        "tokens": [
          50668,
          407,
          321,
          643,
          281,
          6063,
          483,
          527,
          15743,
          11,
          50902
        ]
      },
      {
        "avg_logprob": -0.23442750141538424,
        "compression_ratio": 1.6126482213438735,
        "end": 326.86,
        "id": 134,
        "no_speech_prob": 0.00039820754318498075,
        "seek": 31394,
        "start": 324.7,
        "temperature": 0,
        "text": " computation, and then to our outputs.",
        "tokens": [
          50902,
          24903,
          11,
          293,
          550,
          281,
          527,
          23930,
          13,
          51010
        ]
      },
      {
        "avg_logprob": -0.23442750141538424,
        "compression_ratio": 1.6126482213438735,
        "end": 328.78,
        "id": 135,
        "no_speech_prob": 0.00039820754318498075,
        "seek": 31394,
        "start": 326.86,
        "temperature": 0,
        "text": " And the way that we do that is we use",
        "tokens": [
          51010,
          400,
          264,
          636,
          300,
          321,
          360,
          300,
          307,
          321,
          764,
          51106
        ]
      },
      {
        "avg_logprob": -0.23442750141538424,
        "compression_ratio": 1.6126482213438735,
        "end": 330.94,
        "id": 136,
        "no_speech_prob": 0.00039820754318498075,
        "seek": 31394,
        "start": 328.78,
        "temperature": 0,
        "text": " what I'm using, bubbles, to represent",
        "tokens": [
          51106,
          437,
          286,
          478,
          1228,
          11,
          16295,
          11,
          281,
          2906,
          51214
        ]
      },
      {
        "avg_logprob": -0.23442750141538424,
        "compression_ratio": 1.6126482213438735,
        "end": 333.7,
        "id": 137,
        "no_speech_prob": 0.00039820754318498075,
        "seek": 31394,
        "start": 330.94,
        "temperature": 0,
        "text": " what are called weights within our neural network.",
        "tokens": [
          51214,
          437,
          366,
          1219,
          17443,
          1951,
          527,
          18161,
          3209,
          13,
          51352
        ]
      },
      {
        "avg_logprob": -0.23442750141538424,
        "compression_ratio": 1.6126482213438735,
        "end": 337.9,
        "id": 138,
        "no_speech_prob": 0.00039820754318498075,
        "seek": 31394,
        "start": 333.7,
        "temperature": 0,
        "text": " And so every single node within our hidden layer",
        "tokens": [
          51352,
          400,
          370,
          633,
          2167,
          9984,
          1951,
          527,
          7633,
          4583,
          51562
        ]
      },
      {
        "avg_logprob": -0.23442750141538424,
        "compression_ratio": 1.6126482213438735,
        "end": 342.26,
        "id": 139,
        "no_speech_prob": 0.00039820754318498075,
        "seek": 31394,
        "start": 337.9,
        "temperature": 0,
        "text": " has the same amount of weights as there are inputs.",
        "tokens": [
          51562,
          575,
          264,
          912,
          2372,
          295,
          17443,
          382,
          456,
          366,
          15743,
          13,
          51780
        ]
      },
      {
        "avg_logprob": -0.2364231771674038,
        "compression_ratio": 2.2865168539325844,
        "end": 347.18,
        "id": 140,
        "no_speech_prob": 0.00015356106450781226,
        "seek": 34226,
        "start": 342.26,
        "temperature": 0,
        "text": " So what that means is there's one weight for this input,",
        "tokens": [
          50364,
          407,
          437,
          300,
          1355,
          307,
          456,
          311,
          472,
          3364,
          337,
          341,
          4846,
          11,
          50610
        ]
      },
      {
        "avg_logprob": -0.2364231771674038,
        "compression_ratio": 2.2865168539325844,
        "end": 348.78,
        "id": 141,
        "no_speech_prob": 0.00015356106450781226,
        "seek": 34226,
        "start": 347.18,
        "temperature": 0,
        "text": " there's one weight for this input,",
        "tokens": [
          50610,
          456,
          311,
          472,
          3364,
          337,
          341,
          4846,
          11,
          50690
        ]
      },
      {
        "avg_logprob": -0.2364231771674038,
        "compression_ratio": 2.2865168539325844,
        "end": 350.58,
        "id": 142,
        "no_speech_prob": 0.00015356106450781226,
        "seek": 34226,
        "start": 348.78,
        "temperature": 0,
        "text": " and there's one weight for this input.",
        "tokens": [
          50690,
          293,
          456,
          311,
          472,
          3364,
          337,
          341,
          4846,
          13,
          50780
        ]
      },
      {
        "avg_logprob": -0.2364231771674038,
        "compression_ratio": 2.2865168539325844,
        "end": 352.02,
        "id": 143,
        "no_speech_prob": 0.00015356106450781226,
        "seek": 34226,
        "start": 350.58,
        "temperature": 0,
        "text": " And the same for the rest of them.",
        "tokens": [
          50780,
          400,
          264,
          912,
          337,
          264,
          1472,
          295,
          552,
          13,
          50852
        ]
      },
      {
        "avg_logprob": -0.2364231771674038,
        "compression_ratio": 2.2865168539325844,
        "end": 355.34,
        "id": 144,
        "no_speech_prob": 0.00015356106450781226,
        "seek": 34226,
        "start": 352.02,
        "temperature": 0,
        "text": " One weight for this input, one weight for this input,",
        "tokens": [
          50852,
          1485,
          3364,
          337,
          341,
          4846,
          11,
          472,
          3364,
          337,
          341,
          4846,
          11,
          51018
        ]
      },
      {
        "avg_logprob": -0.2364231771674038,
        "compression_ratio": 2.2865168539325844,
        "end": 359.2,
        "id": 145,
        "no_speech_prob": 0.00015356106450781226,
        "seek": 34226,
        "start": 355.34,
        "temperature": 0,
        "text": " one weight for that input, and repeat.",
        "tokens": [
          51018,
          472,
          3364,
          337,
          300,
          4846,
          11,
          293,
          7149,
          13,
          51211
        ]
      },
      {
        "avg_logprob": -0.2364231771674038,
        "compression_ratio": 2.2865168539325844,
        "end": 361.21999999999997,
        "id": 146,
        "no_speech_prob": 0.00015356106450781226,
        "seek": 34226,
        "start": 360.38,
        "temperature": 0,
        "text": " I didn't do that right.",
        "tokens": [
          51270,
          286,
          994,
          380,
          360,
          300,
          558,
          13,
          51312
        ]
      },
      {
        "avg_logprob": -0.2364231771674038,
        "compression_ratio": 2.2865168539325844,
        "end": 362.36,
        "id": 147,
        "no_speech_prob": 0.00015356106450781226,
        "seek": 34226,
        "start": 361.21999999999997,
        "temperature": 0,
        "text": " Boom, boom.",
        "tokens": [
          51312,
          15523,
          11,
          9351,
          13,
          51369
        ]
      },
      {
        "avg_logprob": -0.2364231771674038,
        "compression_ratio": 2.2865168539325844,
        "end": 366.21999999999997,
        "id": 148,
        "no_speech_prob": 0.00015356106450781226,
        "seek": 34226,
        "start": 363.34,
        "temperature": 0,
        "text": " And so what then happens is that we pass this through,",
        "tokens": [
          51418,
          400,
          370,
          437,
          550,
          2314,
          307,
          300,
          321,
          1320,
          341,
          807,
          11,
          51562
        ]
      },
      {
        "avg_logprob": -0.2364231771674038,
        "compression_ratio": 2.2865168539325844,
        "end": 369.28,
        "id": 149,
        "no_speech_prob": 0.00015356106450781226,
        "seek": 34226,
        "start": 366.21999999999997,
        "temperature": 0,
        "text": " we do our input times the weight,",
        "tokens": [
          51562,
          321,
          360,
          527,
          4846,
          1413,
          264,
          3364,
          11,
          51715
        ]
      },
      {
        "avg_logprob": -0.2364231771674038,
        "compression_ratio": 2.2865168539325844,
        "end": 370.86,
        "id": 150,
        "no_speech_prob": 0.00015356106450781226,
        "seek": 34226,
        "start": 369.28,
        "temperature": 0,
        "text": " and then plus our bias,",
        "tokens": [
          51715,
          293,
          550,
          1804,
          527,
          12577,
          11,
          51794
        ]
      },
      {
        "avg_logprob": -0.2675255832509098,
        "compression_ratio": 1.7213930348258706,
        "end": 372.82,
        "id": 151,
        "no_speech_prob": 0.000911033246666193,
        "seek": 37086,
        "start": 370.86,
        "temperature": 0,
        "text": " and we can repeat the process.",
        "tokens": [
          50364,
          293,
          321,
          393,
          7149,
          264,
          1399,
          13,
          50462
        ]
      },
      {
        "avg_logprob": -0.2675255832509098,
        "compression_ratio": 1.7213930348258706,
        "end": 373.92,
        "id": 152,
        "no_speech_prob": 0.000911033246666193,
        "seek": 37086,
        "start": 372.82,
        "temperature": 0,
        "text": " This will give us a value.",
        "tokens": [
          50462,
          639,
          486,
          976,
          505,
          257,
          2158,
          13,
          50517
        ]
      },
      {
        "avg_logprob": -0.2675255832509098,
        "compression_ratio": 1.7213930348258706,
        "end": 375.74,
        "id": 153,
        "no_speech_prob": 0.000911033246666193,
        "seek": 37086,
        "start": 373.92,
        "temperature": 0,
        "text": " Let's say that after we compute all these,",
        "tokens": [
          50517,
          961,
          311,
          584,
          300,
          934,
          321,
          14722,
          439,
          613,
          11,
          50608
        ]
      },
      {
        "avg_logprob": -0.2675255832509098,
        "compression_ratio": 1.7213930348258706,
        "end": 378.7,
        "id": 154,
        "no_speech_prob": 0.000911033246666193,
        "seek": 37086,
        "start": 375.74,
        "temperature": 0,
        "text": " sum them up, add a bias, it will give us,",
        "tokens": [
          50608,
          2408,
          552,
          493,
          11,
          909,
          257,
          12577,
          11,
          309,
          486,
          976,
          505,
          11,
          50756
        ]
      },
      {
        "avg_logprob": -0.2675255832509098,
        "compression_ratio": 1.7213930348258706,
        "end": 381.42,
        "id": 155,
        "no_speech_prob": 0.000911033246666193,
        "seek": 37086,
        "start": 378.7,
        "temperature": 0,
        "text": " let's just say.5, and then we'll pass that",
        "tokens": [
          50756,
          718,
          311,
          445,
          584,
          2411,
          20,
          11,
          293,
          550,
          321,
          603,
          1320,
          300,
          50892
        ]
      },
      {
        "avg_logprob": -0.2675255832509098,
        "compression_ratio": 1.7213930348258706,
        "end": 384.12,
        "id": 156,
        "no_speech_prob": 0.000911033246666193,
        "seek": 37086,
        "start": 381.42,
        "temperature": 0,
        "text": " to our outputs, which is three.",
        "tokens": [
          50892,
          281,
          527,
          23930,
          11,
          597,
          307,
          1045,
          13,
          51027
        ]
      },
      {
        "avg_logprob": -0.2675255832509098,
        "compression_ratio": 1.7213930348258706,
        "end": 390.58000000000004,
        "id": 157,
        "no_speech_prob": 0.000911033246666193,
        "seek": 37086,
        "start": 389.74,
        "temperature": 0,
        "text": " Boom.",
        "tokens": [
          51308,
          15523,
          13,
          51350
        ]
      },
      {
        "avg_logprob": -0.2675255832509098,
        "compression_ratio": 1.7213930348258706,
        "end": 396.22,
        "id": 158,
        "no_speech_prob": 0.000911033246666193,
        "seek": 37086,
        "start": 394.12,
        "temperature": 0,
        "text": " Pass this to our output, and then that will give us",
        "tokens": [
          51527,
          10319,
          341,
          281,
          527,
          5598,
          11,
          293,
          550,
          300,
          486,
          976,
          505,
          51632
        ]
      },
      {
        "avg_logprob": -0.2675255832509098,
        "compression_ratio": 1.7213930348258706,
        "end": 397.38,
        "id": 159,
        "no_speech_prob": 0.000911033246666193,
        "seek": 37086,
        "start": 396.22,
        "temperature": 0,
        "text": " a value for each of these.",
        "tokens": [
          51632,
          257,
          2158,
          337,
          1184,
          295,
          613,
          13,
          51690
        ]
      },
      {
        "avg_logprob": -0.2675255832509098,
        "compression_ratio": 1.7213930348258706,
        "end": 400.46000000000004,
        "id": 160,
        "no_speech_prob": 0.000911033246666193,
        "seek": 37086,
        "start": 397.38,
        "temperature": 0,
        "text": " So let's say this is.3, and then this is.7.",
        "tokens": [
          51690,
          407,
          718,
          311,
          584,
          341,
          307,
          2411,
          18,
          11,
          293,
          550,
          341,
          307,
          2411,
          22,
          13,
          51844
        ]
      },
      {
        "avg_logprob": -0.28350826731899326,
        "compression_ratio": 1.6048387096774193,
        "end": 402.29999999999995,
        "id": 161,
        "no_speech_prob": 0.00017400458455085754,
        "seek": 40046,
        "start": 400.97999999999996,
        "temperature": 0,
        "text": " And then it's just as simple as we'll just say",
        "tokens": [
          50390,
          400,
          550,
          309,
          311,
          445,
          382,
          2199,
          382,
          321,
          603,
          445,
          584,
          50456
        ]
      },
      {
        "avg_logprob": -0.28350826731899326,
        "compression_ratio": 1.6048387096774193,
        "end": 404.97999999999996,
        "id": 162,
        "no_speech_prob": 0.00017400458455085754,
        "seek": 40046,
        "start": 402.29999999999995,
        "temperature": 0,
        "text": " that this is higher,.7, so it's guessing white.",
        "tokens": [
          50456,
          300,
          341,
          307,
          2946,
          11,
          2411,
          22,
          11,
          370,
          309,
          311,
          17939,
          2418,
          13,
          50590
        ]
      },
      {
        "avg_logprob": -0.28350826731899326,
        "compression_ratio": 1.6048387096774193,
        "end": 409.08,
        "id": 163,
        "no_speech_prob": 0.00017400458455085754,
        "seek": 40046,
        "start": 406.18,
        "temperature": 0,
        "text": " So that's a quick overview on what's going on here.",
        "tokens": [
          50650,
          407,
          300,
          311,
          257,
          1702,
          12492,
          322,
          437,
          311,
          516,
          322,
          510,
          13,
          50795
        ]
      },
      {
        "avg_logprob": -0.28350826731899326,
        "compression_ratio": 1.6048387096774193,
        "end": 413.91999999999996,
        "id": 164,
        "no_speech_prob": 0.00017400458455085754,
        "seek": 40046,
        "start": 411.06,
        "temperature": 0,
        "text": " Daniel's gonna post a more in-depth tutorial on this,",
        "tokens": [
          50894,
          8033,
          311,
          799,
          2183,
          257,
          544,
          294,
          12,
          25478,
          7073,
          322,
          341,
          11,
          51037
        ]
      },
      {
        "avg_logprob": -0.28350826731899326,
        "compression_ratio": 1.6048387096774193,
        "end": 415.12,
        "id": 165,
        "no_speech_prob": 0.00017400458455085754,
        "seek": 40046,
        "start": 413.91999999999996,
        "temperature": 0,
        "text": " or you already have.",
        "tokens": [
          51037,
          420,
          291,
          1217,
          362,
          13,
          51097
        ]
      },
      {
        "avg_logprob": -0.28350826731899326,
        "compression_ratio": 1.6048387096774193,
        "end": 420.74,
        "id": 166,
        "no_speech_prob": 0.00017400458455085754,
        "seek": 40046,
        "start": 416.21999999999997,
        "temperature": 0,
        "text": " Well, so I have tutorials on neural network stuff",
        "tokens": [
          51152,
          1042,
          11,
          370,
          286,
          362,
          17616,
          322,
          18161,
          3209,
          1507,
          51378
        ]
      },
      {
        "avg_logprob": -0.28350826731899326,
        "compression_ratio": 1.6048387096774193,
        "end": 422.29999999999995,
        "id": 167,
        "no_speech_prob": 0.00017400458455085754,
        "seek": 40046,
        "start": 420.74,
        "temperature": 0,
        "text": " like this that people could go back.",
        "tokens": [
          51378,
          411,
          341,
          300,
          561,
          727,
          352,
          646,
          13,
          51456
        ]
      },
      {
        "avg_logprob": -0.28350826731899326,
        "compression_ratio": 1.6048387096774193,
        "end": 424.46,
        "id": 168,
        "no_speech_prob": 0.00017400458455085754,
        "seek": 40046,
        "start": 422.29999999999995,
        "temperature": 0,
        "text": " So this is the same kind of structure",
        "tokens": [
          51456,
          407,
          341,
          307,
          264,
          912,
          733,
          295,
          3877,
          51564
        ]
      },
      {
        "avg_logprob": -0.28350826731899326,
        "compression_ratio": 1.6048387096774193,
        "end": 427.74,
        "id": 169,
        "no_speech_prob": 0.00017400458455085754,
        "seek": 40046,
        "start": 424.46,
        "temperature": 0,
        "text": " that I've used in my neural network library videos.",
        "tokens": [
          51564,
          300,
          286,
          600,
          1143,
          294,
          452,
          18161,
          3209,
          6405,
          2145,
          13,
          51728
        ]
      },
      {
        "avg_logprob": -0.24915627872242646,
        "compression_ratio": 1.7296511627906976,
        "end": 431.3,
        "id": 170,
        "no_speech_prob": 0.0010322046000510454,
        "seek": 42774,
        "start": 428.34000000000003,
        "temperature": 0,
        "text": " I was thinking at some point, maybe next week hopefully,",
        "tokens": [
          50394,
          286,
          390,
          1953,
          412,
          512,
          935,
          11,
          1310,
          958,
          1243,
          4696,
          11,
          50542
        ]
      },
      {
        "avg_logprob": -0.24915627872242646,
        "compression_ratio": 1.7296511627906976,
        "end": 433.18,
        "id": 171,
        "no_speech_prob": 0.0010322046000510454,
        "seek": 42774,
        "start": 431.3,
        "temperature": 0,
        "text": " I might try to recreate your project",
        "tokens": [
          50542,
          286,
          1062,
          853,
          281,
          25833,
          428,
          1716,
          50636
        ]
      },
      {
        "avg_logprob": -0.24915627872242646,
        "compression_ratio": 1.7296511627906976,
        "end": 436.18,
        "id": 172,
        "no_speech_prob": 0.0010322046000510454,
        "seek": 42774,
        "start": 433.18,
        "temperature": 0,
        "text": " as a coding challenge, in which case I'll revisit this.",
        "tokens": [
          50636,
          382,
          257,
          17720,
          3430,
          11,
          294,
          597,
          1389,
          286,
          603,
          32676,
          341,
          13,
          50786
        ]
      },
      {
        "avg_logprob": -0.24915627872242646,
        "compression_ratio": 1.7296511627906976,
        "end": 438.26,
        "id": 173,
        "no_speech_prob": 0.0010322046000510454,
        "seek": 42774,
        "start": 436.18,
        "temperature": 0,
        "text": " So we can put a link to your, okay.",
        "tokens": [
          50786,
          407,
          321,
          393,
          829,
          257,
          2113,
          281,
          428,
          11,
          1392,
          13,
          50890
        ]
      },
      {
        "avg_logprob": -0.24915627872242646,
        "compression_ratio": 1.7296511627906976,
        "end": 440.18,
        "id": 174,
        "no_speech_prob": 0.0010322046000510454,
        "seek": 42774,
        "start": 438.26,
        "temperature": 0,
        "text": " So we'll put a link to Daniel's Shipman series",
        "tokens": [
          50890,
          407,
          321,
          603,
          829,
          257,
          2113,
          281,
          8033,
          311,
          38407,
          1601,
          2638,
          50986
        ]
      },
      {
        "avg_logprob": -0.24915627872242646,
        "compression_ratio": 1.7296511627906976,
        "end": 442.22,
        "id": 175,
        "no_speech_prob": 0.0010322046000510454,
        "seek": 42774,
        "start": 440.18,
        "temperature": 0,
        "text": " in which he goes in-depth with this,",
        "tokens": [
          50986,
          294,
          597,
          415,
          1709,
          294,
          12,
          25478,
          365,
          341,
          11,
          51088
        ]
      },
      {
        "avg_logprob": -0.24915627872242646,
        "compression_ratio": 1.7296511627906976,
        "end": 444.06,
        "id": 176,
        "no_speech_prob": 0.0010322046000510454,
        "seek": 42774,
        "start": 442.22,
        "temperature": 0,
        "text": " so if you wanna learn more about what's going on here.",
        "tokens": [
          51088,
          370,
          498,
          291,
          1948,
          1466,
          544,
          466,
          437,
          311,
          516,
          322,
          510,
          13,
          51180
        ]
      },
      {
        "avg_logprob": -0.24915627872242646,
        "compression_ratio": 1.7296511627906976,
        "end": 445.86,
        "id": 177,
        "no_speech_prob": 0.0010322046000510454,
        "seek": 42774,
        "start": 444.06,
        "temperature": 0,
        "text": " But that's a quick overview on the math,",
        "tokens": [
          51180,
          583,
          300,
          311,
          257,
          1702,
          12492,
          322,
          264,
          5221,
          11,
          51270
        ]
      },
      {
        "avg_logprob": -0.24915627872242646,
        "compression_ratio": 1.7296511627906976,
        "end": 446.92,
        "id": 178,
        "no_speech_prob": 0.0010322046000510454,
        "seek": 42774,
        "start": 445.86,
        "temperature": 0,
        "text": " on the computation.",
        "tokens": [
          51270,
          322,
          264,
          24903,
          13,
          51323
        ]
      },
      {
        "avg_logprob": -0.24915627872242646,
        "compression_ratio": 1.7296511627906976,
        "end": 449.58,
        "id": 179,
        "no_speech_prob": 0.0010322046000510454,
        "seek": 42774,
        "start": 446.92,
        "temperature": 0,
        "text": " So our inputs, it gets times by weight and biases,",
        "tokens": [
          51323,
          407,
          527,
          15743,
          11,
          309,
          2170,
          1413,
          538,
          3364,
          293,
          32152,
          11,
          51456
        ]
      },
      {
        "avg_logprob": -0.24915627872242646,
        "compression_ratio": 1.7296511627906976,
        "end": 452.22,
        "id": 180,
        "no_speech_prob": 0.0010322046000510454,
        "seek": 42774,
        "start": 449.58,
        "temperature": 0,
        "text": " then we get a value, and then we pass that to our output,",
        "tokens": [
          51456,
          550,
          321,
          483,
          257,
          2158,
          11,
          293,
          550,
          321,
          1320,
          300,
          281,
          527,
          5598,
          11,
          51588
        ]
      },
      {
        "avg_logprob": -0.24915627872242646,
        "compression_ratio": 1.7296511627906976,
        "end": 454.82,
        "id": 181,
        "no_speech_prob": 0.0010322046000510454,
        "seek": 42774,
        "start": 452.22,
        "temperature": 0,
        "text": " same computation, and then it gives us a prediction.",
        "tokens": [
          51588,
          912,
          24903,
          11,
          293,
          550,
          309,
          2709,
          505,
          257,
          17630,
          13,
          51718
        ]
      },
      {
        "avg_logprob": -0.24915627872242646,
        "compression_ratio": 1.7296511627906976,
        "end": 456.5,
        "id": 182,
        "no_speech_prob": 0.0010322046000510454,
        "seek": 42774,
        "start": 454.82,
        "temperature": 0,
        "text": " Oh, and actually, there's a bunch of questions.",
        "tokens": [
          51718,
          876,
          11,
          293,
          767,
          11,
          456,
          311,
          257,
          3840,
          295,
          1651,
          13,
          51802
        ]
      },
      {
        "avg_logprob": -0.2532764434814453,
        "compression_ratio": 1.6781609195402298,
        "end": 458.34,
        "id": 183,
        "no_speech_prob": 0.0035375053994357586,
        "seek": 45650,
        "start": 456.82,
        "temperature": 0,
        "text": " I might as well ask, can I interrupt you",
        "tokens": [
          50380,
          286,
          1062,
          382,
          731,
          1029,
          11,
          393,
          286,
          12729,
          291,
          50456
        ]
      },
      {
        "avg_logprob": -0.2532764434814453,
        "compression_ratio": 1.6781609195402298,
        "end": 459.18,
        "id": 184,
        "no_speech_prob": 0.0035375053994357586,
        "seek": 45650,
        "start": 458.34,
        "temperature": 0,
        "text": " and ask a question?",
        "tokens": [
          50456,
          293,
          1029,
          257,
          1168,
          30,
          50498
        ]
      },
      {
        "avg_logprob": -0.2532764434814453,
        "compression_ratio": 1.6781609195402298,
        "end": 460,
        "id": 185,
        "no_speech_prob": 0.0035375053994357586,
        "seek": 45650,
        "start": 459.18,
        "temperature": 0,
        "text": " Yeah, let's do it.",
        "tokens": [
          50498,
          865,
          11,
          718,
          311,
          360,
          309,
          13,
          50539
        ]
      },
      {
        "avg_logprob": -0.2532764434814453,
        "compression_ratio": 1.6781609195402298,
        "end": 463.54,
        "id": 186,
        "no_speech_prob": 0.0035375053994357586,
        "seek": 45650,
        "start": 460,
        "temperature": 0,
        "text": " All right, does the input have to be from zero to 255?",
        "tokens": [
          50539,
          1057,
          558,
          11,
          775,
          264,
          4846,
          362,
          281,
          312,
          490,
          4018,
          281,
          3552,
          20,
          30,
          50716
        ]
      },
      {
        "avg_logprob": -0.2532764434814453,
        "compression_ratio": 1.6781609195402298,
        "end": 465.78,
        "id": 187,
        "no_speech_prob": 0.0035375053994357586,
        "seek": 45650,
        "start": 463.54,
        "temperature": 0,
        "text": " Should inputs have to be normalized?",
        "tokens": [
          50716,
          6454,
          15743,
          362,
          281,
          312,
          48704,
          30,
          50828
        ]
      },
      {
        "avg_logprob": -0.2532764434814453,
        "compression_ratio": 1.6781609195402298,
        "end": 467.38,
        "id": 188,
        "no_speech_prob": 0.0035375053994357586,
        "seek": 45650,
        "start": 465.78,
        "temperature": 0,
        "text": " What's from zero to one?",
        "tokens": [
          50828,
          708,
          311,
          490,
          4018,
          281,
          472,
          30,
          50908
        ]
      },
      {
        "avg_logprob": -0.2532764434814453,
        "compression_ratio": 1.6781609195402298,
        "end": 469.58,
        "id": 189,
        "no_speech_prob": 0.0035375053994357586,
        "seek": 45650,
        "start": 467.38,
        "temperature": 0,
        "text": " Yes, great question, great question.",
        "tokens": [
          50908,
          1079,
          11,
          869,
          1168,
          11,
          869,
          1168,
          13,
          51018
        ]
      },
      {
        "avg_logprob": -0.2532764434814453,
        "compression_ratio": 1.6781609195402298,
        "end": 471.54,
        "id": 190,
        "no_speech_prob": 0.0035375053994357586,
        "seek": 45650,
        "start": 469.58,
        "temperature": 0,
        "text": " So again, I just glossed over this,",
        "tokens": [
          51018,
          407,
          797,
          11,
          286,
          445,
          19574,
          292,
          670,
          341,
          11,
          51116
        ]
      },
      {
        "avg_logprob": -0.2532764434814453,
        "compression_ratio": 1.6781609195402298,
        "end": 474.86,
        "id": 191,
        "no_speech_prob": 0.0035375053994357586,
        "seek": 45650,
        "start": 471.54,
        "temperature": 0,
        "text": " but so normalizing inputs for colors",
        "tokens": [
          51116,
          457,
          370,
          2710,
          3319,
          15743,
          337,
          4577,
          51282
        ]
      },
      {
        "avg_logprob": -0.2532764434814453,
        "compression_ratio": 1.6781609195402298,
        "end": 475.9,
        "id": 192,
        "no_speech_prob": 0.0035375053994357586,
        "seek": 45650,
        "start": 474.86,
        "temperature": 0,
        "text": " is actually really simple,",
        "tokens": [
          51282,
          307,
          767,
          534,
          2199,
          11,
          51334
        ]
      },
      {
        "avg_logprob": -0.2532764434814453,
        "compression_ratio": 1.6781609195402298,
        "end": 480.9,
        "id": 193,
        "no_speech_prob": 0.0035375053994357586,
        "seek": 45650,
        "start": 475.9,
        "temperature": 0,
        "text": " and it is always best to normalize your input data.",
        "tokens": [
          51334,
          293,
          309,
          307,
          1009,
          1151,
          281,
          2710,
          1125,
          428,
          4846,
          1412,
          13,
          51584
        ]
      },
      {
        "avg_logprob": -0.2532764434814453,
        "compression_ratio": 1.6781609195402298,
        "end": 486.14,
        "id": 194,
        "no_speech_prob": 0.0035375053994357586,
        "seek": 45650,
        "start": 481.42,
        "temperature": 0,
        "text": " So because we know that the domain for a color value",
        "tokens": [
          51610,
          407,
          570,
          321,
          458,
          300,
          264,
          9274,
          337,
          257,
          2017,
          2158,
          51846
        ]
      },
      {
        "avg_logprob": -0.2402908874280525,
        "compression_ratio": 1.6321428571428571,
        "end": 488.65999999999997,
        "id": 195,
        "no_speech_prob": 0.0014103265712037683,
        "seek": 48614,
        "start": 486.82,
        "temperature": 0,
        "text": " is always gonna be one to 256,",
        "tokens": [
          50398,
          307,
          1009,
          799,
          312,
          472,
          281,
          38882,
          11,
          50490
        ]
      },
      {
        "avg_logprob": -0.2402908874280525,
        "compression_ratio": 1.6321428571428571,
        "end": 492.21999999999997,
        "id": 196,
        "no_speech_prob": 0.0014103265712037683,
        "seek": 48614,
        "start": 488.65999999999997,
        "temperature": 0,
        "text": " or in computer language, we shift that by one, zero to 255,",
        "tokens": [
          50490,
          420,
          294,
          3820,
          2856,
          11,
          321,
          5513,
          300,
          538,
          472,
          11,
          4018,
          281,
          3552,
          20,
          11,
          50668
        ]
      },
      {
        "avg_logprob": -0.2402908874280525,
        "compression_ratio": 1.6321428571428571,
        "end": 495.74,
        "id": 197,
        "no_speech_prob": 0.0014103265712037683,
        "seek": 48614,
        "start": 492.21999999999997,
        "temperature": 0,
        "text": " we can simply just divide whatever this value is over 255,",
        "tokens": [
          50668,
          321,
          393,
          2935,
          445,
          9845,
          2035,
          341,
          2158,
          307,
          670,
          3552,
          20,
          11,
          50844
        ]
      },
      {
        "avg_logprob": -0.2402908874280525,
        "compression_ratio": 1.6321428571428571,
        "end": 498.18,
        "id": 198,
        "no_speech_prob": 0.0014103265712037683,
        "seek": 48614,
        "start": 495.74,
        "temperature": 0,
        "text": " and that will remap this between zero and one.",
        "tokens": [
          50844,
          293,
          300,
          486,
          890,
          569,
          341,
          1296,
          4018,
          293,
          472,
          13,
          50966
        ]
      },
      {
        "avg_logprob": -0.2402908874280525,
        "compression_ratio": 1.6321428571428571,
        "end": 501.18,
        "id": 199,
        "no_speech_prob": 0.0014103265712037683,
        "seek": 48614,
        "start": 498.18,
        "temperature": 0,
        "text": " And so essentially, when you're writing your program,",
        "tokens": [
          50966,
          400,
          370,
          4476,
          11,
          562,
          291,
          434,
          3579,
          428,
          1461,
          11,
          51116
        ]
      },
      {
        "avg_logprob": -0.2402908874280525,
        "compression_ratio": 1.6321428571428571,
        "end": 504.78,
        "id": 200,
        "no_speech_prob": 0.0014103265712037683,
        "seek": 48614,
        "start": 501.18,
        "temperature": 0,
        "text": " you would just pass the input through a function",
        "tokens": [
          51116,
          291,
          576,
          445,
          1320,
          264,
          4846,
          807,
          257,
          2445,
          51296
        ]
      },
      {
        "avg_logprob": -0.2402908874280525,
        "compression_ratio": 1.6321428571428571,
        "end": 506.58,
        "id": 201,
        "no_speech_prob": 0.0014103265712037683,
        "seek": 48614,
        "start": 504.78,
        "temperature": 0,
        "text": " that would just divide it by 255.",
        "tokens": [
          51296,
          300,
          576,
          445,
          9845,
          309,
          538,
          3552,
          20,
          13,
          51386
        ]
      },
      {
        "avg_logprob": -0.2402908874280525,
        "compression_ratio": 1.6321428571428571,
        "end": 508.58,
        "id": 202,
        "no_speech_prob": 0.0014103265712037683,
        "seek": 48614,
        "start": 506.58,
        "temperature": 0,
        "text": " So yes, great question.",
        "tokens": [
          51386,
          407,
          2086,
          11,
          869,
          1168,
          13,
          51486
        ]
      },
      {
        "avg_logprob": -0.2402908874280525,
        "compression_ratio": 1.6321428571428571,
        "end": 509.9,
        "id": 203,
        "no_speech_prob": 0.0014103265712037683,
        "seek": 48614,
        "start": 508.58,
        "temperature": 0,
        "text": " All right, one more question.",
        "tokens": [
          51486,
          1057,
          558,
          11,
          472,
          544,
          1168,
          13,
          51552
        ]
      },
      {
        "avg_logprob": -0.2402908874280525,
        "compression_ratio": 1.6321428571428571,
        "end": 511.9,
        "id": 204,
        "no_speech_prob": 0.0014103265712037683,
        "seek": 48614,
        "start": 509.9,
        "temperature": 0,
        "text": " So I'm kind of curious about this too.",
        "tokens": [
          51552,
          407,
          286,
          478,
          733,
          295,
          6369,
          466,
          341,
          886,
          13,
          51652
        ]
      },
      {
        "avg_logprob": -0.2402908874280525,
        "compression_ratio": 1.6321428571428571,
        "end": 513.66,
        "id": 205,
        "no_speech_prob": 0.0014103265712037683,
        "seek": 48614,
        "start": 511.9,
        "temperature": 0,
        "text": " I sort of think it's probably,",
        "tokens": [
          51652,
          286,
          1333,
          295,
          519,
          309,
          311,
          1391,
          11,
          51740
        ]
      },
      {
        "avg_logprob": -0.30878505875579026,
        "compression_ratio": 1.7659574468085106,
        "end": 516.38,
        "id": 206,
        "no_speech_prob": 0.00023781837080605328,
        "seek": 51366,
        "start": 513.8199999999999,
        "temperature": 0,
        "text": " I'm answering the question before I ask it.",
        "tokens": [
          50372,
          286,
          478,
          13430,
          264,
          1168,
          949,
          286,
          1029,
          309,
          13,
          50500
        ]
      },
      {
        "avg_logprob": -0.30878505875579026,
        "compression_ratio": 1.7659574468085106,
        "end": 520.6999999999999,
        "id": 207,
        "no_speech_prob": 0.00023781837080605328,
        "seek": 51366,
        "start": 516.38,
        "temperature": 0,
        "text": " But is there a benefit to having two output nodes",
        "tokens": [
          50500,
          583,
          307,
          456,
          257,
          5121,
          281,
          1419,
          732,
          5598,
          13891,
          50716
        ]
      },
      {
        "avg_logprob": -0.30878505875579026,
        "compression_ratio": 1.7659574468085106,
        "end": 524.02,
        "id": 208,
        "no_speech_prob": 0.00023781837080605328,
        "seek": 51366,
        "start": 522.62,
        "temperature": 0,
        "text": " rather than just have one,",
        "tokens": [
          50812,
          2831,
          813,
          445,
          362,
          472,
          11,
          50882
        ]
      },
      {
        "avg_logprob": -0.30878505875579026,
        "compression_ratio": 1.7659574468085106,
        "end": 525.5799999999999,
        "id": 209,
        "no_speech_prob": 0.00023781837080605328,
        "seek": 51366,
        "start": 524.02,
        "temperature": 0,
        "text": " since there's only, that's like a range",
        "tokens": [
          50882,
          1670,
          456,
          311,
          787,
          11,
          300,
          311,
          411,
          257,
          3613,
          50960
        ]
      },
      {
        "avg_logprob": -0.30878505875579026,
        "compression_ratio": 1.7659574468085106,
        "end": 527.1,
        "id": 210,
        "no_speech_prob": 0.00023781837080605328,
        "seek": 51366,
        "start": 525.5799999999999,
        "temperature": 0,
        "text": " between negative one and one or something like that?",
        "tokens": [
          50960,
          1296,
          3671,
          472,
          293,
          472,
          420,
          746,
          411,
          300,
          30,
          51036
        ]
      },
      {
        "avg_logprob": -0.30878505875579026,
        "compression_ratio": 1.7659574468085106,
        "end": 530.5,
        "id": 211,
        "no_speech_prob": 0.00023781837080605328,
        "seek": 51366,
        "start": 527.1,
        "temperature": 0,
        "text": " Yeah, so there's a lot of debate on this,",
        "tokens": [
          51036,
          865,
          11,
          370,
          456,
          311,
          257,
          688,
          295,
          7958,
          322,
          341,
          11,
          51206
        ]
      },
      {
        "avg_logprob": -0.30878505875579026,
        "compression_ratio": 1.7659574468085106,
        "end": 535.06,
        "id": 212,
        "no_speech_prob": 0.00023781837080605328,
        "seek": 51366,
        "start": 530.5,
        "temperature": 0,
        "text": " and I agree with the side that it's easier",
        "tokens": [
          51206,
          293,
          286,
          3986,
          365,
          264,
          1252,
          300,
          309,
          311,
          3571,
          51434
        ]
      },
      {
        "avg_logprob": -0.30878505875579026,
        "compression_ratio": 1.7659574468085106,
        "end": 539.1,
        "id": 213,
        "no_speech_prob": 0.00023781837080605328,
        "seek": 51366,
        "start": 535.06,
        "temperature": 0,
        "text": " when you have classifiers versus like,",
        "tokens": [
          51434,
          562,
          291,
          362,
          1508,
          23463,
          5717,
          411,
          11,
          51636
        ]
      },
      {
        "avg_logprob": -0.30878505875579026,
        "compression_ratio": 1.7659574468085106,
        "end": 540.86,
        "id": 214,
        "no_speech_prob": 0.00023781837080605328,
        "seek": 51366,
        "start": 539.1,
        "temperature": 0,
        "text": " if you have just one output node",
        "tokens": [
          51636,
          498,
          291,
          362,
          445,
          472,
          5598,
          9984,
          51724
        ]
      },
      {
        "avg_logprob": -0.30878505875579026,
        "compression_ratio": 1.7659574468085106,
        "end": 543.18,
        "id": 215,
        "no_speech_prob": 0.00023781837080605328,
        "seek": 51366,
        "start": 540.86,
        "temperature": 0,
        "text": " that is mapped between negative one and one,",
        "tokens": [
          51724,
          300,
          307,
          33318,
          1296,
          3671,
          472,
          293,
          472,
          11,
          51840
        ]
      },
      {
        "avg_logprob": -0.2046778889025672,
        "compression_ratio": 1.7056277056277056,
        "end": 548.4599999999999,
        "id": 216,
        "no_speech_prob": 0.000014510298569803126,
        "seek": 54366,
        "start": 544.54,
        "temperature": 0,
        "text": " and then if it's above zero, then it's gonna be white.",
        "tokens": [
          50408,
          293,
          550,
          498,
          309,
          311,
          3673,
          4018,
          11,
          550,
          309,
          311,
          799,
          312,
          2418,
          13,
          50604
        ]
      },
      {
        "avg_logprob": -0.2046778889025672,
        "compression_ratio": 1.7056277056277056,
        "end": 551.62,
        "id": 217,
        "no_speech_prob": 0.000014510298569803126,
        "seek": 54366,
        "start": 548.4599999999999,
        "temperature": 0,
        "text": " If it's below zero, then it's gonna be black.",
        "tokens": [
          50604,
          759,
          309,
          311,
          2507,
          4018,
          11,
          550,
          309,
          311,
          799,
          312,
          2211,
          13,
          50762
        ]
      },
      {
        "avg_logprob": -0.2046778889025672,
        "compression_ratio": 1.7056277056277056,
        "end": 556.38,
        "id": 218,
        "no_speech_prob": 0.000014510298569803126,
        "seek": 54366,
        "start": 554.14,
        "temperature": 0,
        "text": " Yeah, based on the research that I've read,",
        "tokens": [
          50888,
          865,
          11,
          2361,
          322,
          264,
          2132,
          300,
          286,
          600,
          1401,
          11,
          51000
        ]
      },
      {
        "avg_logprob": -0.2046778889025672,
        "compression_ratio": 1.7056277056277056,
        "end": 558.6999999999999,
        "id": 219,
        "no_speech_prob": 0.000014510298569803126,
        "seek": 54366,
        "start": 556.38,
        "temperature": 0,
        "text": " it's always best to go on a classifier.",
        "tokens": [
          51000,
          309,
          311,
          1009,
          1151,
          281,
          352,
          322,
          257,
          1508,
          9902,
          13,
          51116
        ]
      },
      {
        "avg_logprob": -0.2046778889025672,
        "compression_ratio": 1.7056277056277056,
        "end": 562.18,
        "id": 220,
        "no_speech_prob": 0.000014510298569803126,
        "seek": 54366,
        "start": 558.6999999999999,
        "temperature": 0,
        "text": " Yeah, and it seems to me like maybe this would be fine",
        "tokens": [
          51116,
          865,
          11,
          293,
          309,
          2544,
          281,
          385,
          411,
          1310,
          341,
          576,
          312,
          2489,
          51290
        ]
      },
      {
        "avg_logprob": -0.2046778889025672,
        "compression_ratio": 1.7056277056277056,
        "end": 565.74,
        "id": 221,
        "no_speech_prob": 0.000014510298569803126,
        "seek": 54366,
        "start": 562.18,
        "temperature": 0,
        "text": " in the case of there's only two labels or two classes,",
        "tokens": [
          51290,
          294,
          264,
          1389,
          295,
          456,
          311,
          787,
          732,
          16949,
          420,
          732,
          5359,
          11,
          51468
        ]
      },
      {
        "avg_logprob": -0.2046778889025672,
        "compression_ratio": 1.7056277056277056,
        "end": 567.74,
        "id": 222,
        "no_speech_prob": 0.000014510298569803126,
        "seek": 54366,
        "start": 565.74,
        "temperature": 0,
        "text": " but once you have more than two,",
        "tokens": [
          51468,
          457,
          1564,
          291,
          362,
          544,
          813,
          732,
          11,
          51568
        ]
      },
      {
        "avg_logprob": -0.2046778889025672,
        "compression_ratio": 1.7056277056277056,
        "end": 569.1,
        "id": 223,
        "no_speech_prob": 0.000014510298569803126,
        "seek": 54366,
        "start": 567.74,
        "temperature": 0,
        "text": " it's gonna be problematic.",
        "tokens": [
          51568,
          309,
          311,
          799,
          312,
          19011,
          13,
          51636
        ]
      },
      {
        "avg_logprob": -0.2046778889025672,
        "compression_ratio": 1.7056277056277056,
        "end": 571.42,
        "id": 224,
        "no_speech_prob": 0.000014510298569803126,
        "seek": 54366,
        "start": 569.1,
        "temperature": 0,
        "text": " And so as a demonstration and learning,",
        "tokens": [
          51636,
          400,
          370,
          382,
          257,
          16520,
          293,
          2539,
          11,
          51752
        ]
      },
      {
        "avg_logprob": -0.2082879086758228,
        "compression_ratio": 1.729641693811075,
        "end": 573.86,
        "id": 225,
        "no_speech_prob": 0.0002737143950071186,
        "seek": 57142,
        "start": 571.42,
        "temperature": 0,
        "text": " even though this might be a very basic scenario,",
        "tokens": [
          50364,
          754,
          1673,
          341,
          1062,
          312,
          257,
          588,
          3875,
          9005,
          11,
          50486
        ]
      },
      {
        "avg_logprob": -0.2082879086758228,
        "compression_ratio": 1.729641693811075,
        "end": 576.4599999999999,
        "id": 226,
        "no_speech_prob": 0.0002737143950071186,
        "seek": 57142,
        "start": 573.86,
        "temperature": 0,
        "text": " it's useful to demonstrate the multiple outputs",
        "tokens": [
          50486,
          309,
          311,
          4420,
          281,
          11698,
          264,
          3866,
          23930,
          50616
        ]
      },
      {
        "avg_logprob": -0.2082879086758228,
        "compression_ratio": 1.729641693811075,
        "end": 577.66,
        "id": 227,
        "no_speech_prob": 0.0002737143950071186,
        "seek": 57142,
        "start": 576.4599999999999,
        "temperature": 0,
        "text": " because you're gonna need to do that",
        "tokens": [
          50616,
          570,
          291,
          434,
          799,
          643,
          281,
          360,
          300,
          50676
        ]
      },
      {
        "avg_logprob": -0.2082879086758228,
        "compression_ratio": 1.729641693811075,
        "end": 578.5799999999999,
        "id": 228,
        "no_speech_prob": 0.0002737143950071186,
        "seek": 57142,
        "start": 577.66,
        "temperature": 0,
        "text": " if you were to expand this further.",
        "tokens": [
          50676,
          498,
          291,
          645,
          281,
          5268,
          341,
          3052,
          13,
          50722
        ]
      },
      {
        "avg_logprob": -0.2082879086758228,
        "compression_ratio": 1.729641693811075,
        "end": 581.9,
        "id": 229,
        "no_speech_prob": 0.0002737143950071186,
        "seek": 57142,
        "start": 578.5799999999999,
        "temperature": 0,
        "text": " Correct, and the whole reason for that",
        "tokens": [
          50722,
          12753,
          11,
          293,
          264,
          1379,
          1778,
          337,
          300,
          50888
        ]
      },
      {
        "avg_logprob": -0.2082879086758228,
        "compression_ratio": 1.729641693811075,
        "end": 584.5799999999999,
        "id": 230,
        "no_speech_prob": 0.0002737143950071186,
        "seek": 57142,
        "start": 581.9,
        "temperature": 0,
        "text": " is because what happens when you separate them",
        "tokens": [
          50888,
          307,
          570,
          437,
          2314,
          562,
          291,
          4994,
          552,
          51022
        ]
      },
      {
        "avg_logprob": -0.2082879086758228,
        "compression_ratio": 1.729641693811075,
        "end": 586.06,
        "id": 231,
        "no_speech_prob": 0.0002737143950071186,
        "seek": 57142,
        "start": 584.5799999999999,
        "temperature": 0,
        "text": " is you get probabilities",
        "tokens": [
          51022,
          307,
          291,
          483,
          33783,
          51096
        ]
      },
      {
        "avg_logprob": -0.2082879086758228,
        "compression_ratio": 1.729641693811075,
        "end": 588.78,
        "id": 232,
        "no_speech_prob": 0.0002737143950071186,
        "seek": 57142,
        "start": 586.06,
        "temperature": 0,
        "text": " versus you get a map of between zero and one,",
        "tokens": [
          51096,
          5717,
          291,
          483,
          257,
          4471,
          295,
          1296,
          4018,
          293,
          472,
          11,
          51232
        ]
      },
      {
        "avg_logprob": -0.2082879086758228,
        "compression_ratio": 1.729641693811075,
        "end": 591.42,
        "id": 233,
        "no_speech_prob": 0.0002737143950071186,
        "seek": 57142,
        "start": 588.78,
        "temperature": 0,
        "text": " which again, if it's one output, you can get away with that,",
        "tokens": [
          51232,
          597,
          797,
          11,
          498,
          309,
          311,
          472,
          5598,
          11,
          291,
          393,
          483,
          1314,
          365,
          300,
          11,
          51364
        ]
      },
      {
        "avg_logprob": -0.2082879086758228,
        "compression_ratio": 1.729641693811075,
        "end": 594.02,
        "id": 234,
        "no_speech_prob": 0.0002737143950071186,
        "seek": 57142,
        "start": 591.42,
        "temperature": 0,
        "text": " but if you try and encode your outputs",
        "tokens": [
          51364,
          457,
          498,
          291,
          853,
          293,
          2058,
          1429,
          428,
          23930,
          51494
        ]
      },
      {
        "avg_logprob": -0.2082879086758228,
        "compression_ratio": 1.729641693811075,
        "end": 597.02,
        "id": 235,
        "no_speech_prob": 0.0002737143950071186,
        "seek": 57142,
        "start": 594.02,
        "temperature": 0,
        "text": " using this for like 30 different,",
        "tokens": [
          51494,
          1228,
          341,
          337,
          411,
          2217,
          819,
          11,
          51644
        ]
      },
      {
        "avg_logprob": -0.2082879086758228,
        "compression_ratio": 1.729641693811075,
        "end": 599.98,
        "id": 236,
        "no_speech_prob": 0.0002737143950071186,
        "seek": 57142,
        "start": 597.02,
        "temperature": 0,
        "text": " the neural network might not make good sense of that.",
        "tokens": [
          51644,
          264,
          18161,
          3209,
          1062,
          406,
          652,
          665,
          2020,
          295,
          300,
          13,
          51792
        ]
      },
      {
        "avg_logprob": -0.2082879086758228,
        "compression_ratio": 1.729641693811075,
        "end": 600.98,
        "id": 237,
        "no_speech_prob": 0.0002737143950071186,
        "seek": 57142,
        "start": 599.98,
        "temperature": 0,
        "text": " Cool, all right.",
        "tokens": [
          51792,
          8561,
          11,
          439,
          558,
          13,
          51842
        ]
      },
      {
        "avg_logprob": -0.231009934165261,
        "compression_ratio": 1.5895196506550218,
        "end": 603.5,
        "id": 238,
        "no_speech_prob": 0.0002824010734912008,
        "seek": 60098,
        "start": 601.38,
        "temperature": 0,
        "text": " Cool, so let's continue on.",
        "tokens": [
          50384,
          8561,
          11,
          370,
          718,
          311,
          2354,
          322,
          13,
          50490
        ]
      },
      {
        "avg_logprob": -0.231009934165261,
        "compression_ratio": 1.5895196506550218,
        "end": 605.66,
        "id": 239,
        "no_speech_prob": 0.0002824010734912008,
        "seek": 60098,
        "start": 603.5,
        "temperature": 0,
        "text": " Let's look at some of the code",
        "tokens": [
          50490,
          961,
          311,
          574,
          412,
          512,
          295,
          264,
          3089,
          50598
        ]
      },
      {
        "avg_logprob": -0.231009934165261,
        "compression_ratio": 1.5895196506550218,
        "end": 608.1,
        "id": 240,
        "no_speech_prob": 0.0002824010734912008,
        "seek": 60098,
        "start": 605.66,
        "temperature": 0,
        "text": " as to how we went about writing",
        "tokens": [
          50598,
          382,
          281,
          577,
          321,
          1437,
          466,
          3579,
          50720
        ]
      },
      {
        "avg_logprob": -0.231009934165261,
        "compression_ratio": 1.5895196506550218,
        "end": 610.94,
        "id": 241,
        "no_speech_prob": 0.0002824010734912008,
        "seek": 60098,
        "start": 608.1,
        "temperature": 0,
        "text": " that part of our neural network.",
        "tokens": [
          50720,
          300,
          644,
          295,
          527,
          18161,
          3209,
          13,
          50862
        ]
      },
      {
        "avg_logprob": -0.231009934165261,
        "compression_ratio": 1.5895196506550218,
        "end": 612.66,
        "id": 242,
        "no_speech_prob": 0.0002824010734912008,
        "seek": 60098,
        "start": 610.94,
        "temperature": 0,
        "text": " Sweet, so we set up our variables.",
        "tokens": [
          50862,
          14653,
          11,
          370,
          321,
          992,
          493,
          527,
          9102,
          13,
          50948
        ]
      },
      {
        "avg_logprob": -0.231009934165261,
        "compression_ratio": 1.5895196506550218,
        "end": 614.34,
        "id": 243,
        "no_speech_prob": 0.0002824010734912008,
        "seek": 60098,
        "start": 612.66,
        "temperature": 0,
        "text": " RGB is our input data.",
        "tokens": [
          50948,
          31231,
          307,
          527,
          4846,
          1412,
          13,
          51032
        ]
      },
      {
        "avg_logprob": -0.231009934165261,
        "compression_ratio": 1.5895196506550218,
        "end": 618.38,
        "id": 244,
        "no_speech_prob": 0.0002824010734912008,
        "seek": 60098,
        "start": 615.82,
        "temperature": 0,
        "text": " And then so one thing that's really important",
        "tokens": [
          51106,
          400,
          550,
          370,
          472,
          551,
          300,
          311,
          534,
          1021,
          51234
        ]
      },
      {
        "avg_logprob": -0.231009934165261,
        "compression_ratio": 1.5895196506550218,
        "end": 622.38,
        "id": 245,
        "no_speech_prob": 0.0002824010734912008,
        "seek": 60098,
        "start": 618.38,
        "temperature": 0,
        "text": " that I should go over just to make sense",
        "tokens": [
          51234,
          300,
          286,
          820,
          352,
          670,
          445,
          281,
          652,
          2020,
          51434
        ]
      },
      {
        "avg_logprob": -0.231009934165261,
        "compression_ratio": 1.5895196506550218,
        "end": 625.46,
        "id": 246,
        "no_speech_prob": 0.0002824010734912008,
        "seek": 60098,
        "start": 622.38,
        "temperature": 0,
        "text": " of what's going on here is,",
        "tokens": [
          51434,
          295,
          437,
          311,
          516,
          322,
          510,
          307,
          11,
          51588
        ]
      },
      {
        "avg_logprob": -0.231009934165261,
        "compression_ratio": 1.5895196506550218,
        "end": 628.1,
        "id": 247,
        "no_speech_prob": 0.0002824010734912008,
        "seek": 60098,
        "start": 625.46,
        "temperature": 0,
        "text": " so it's really important,",
        "tokens": [
          51588,
          370,
          309,
          311,
          534,
          1021,
          11,
          51720
        ]
      },
      {
        "avg_logprob": -0.231009934165261,
        "compression_ratio": 1.5895196506550218,
        "end": 629.82,
        "id": 248,
        "no_speech_prob": 0.0002824010734912008,
        "seek": 60098,
        "start": 628.1,
        "temperature": 0,
        "text": " in order for you to write your algorithm,",
        "tokens": [
          51720,
          294,
          1668,
          337,
          291,
          281,
          2464,
          428,
          9284,
          11,
          51806
        ]
      },
      {
        "avg_logprob": -0.1944370939020525,
        "compression_ratio": 1.8784530386740332,
        "end": 633.4200000000001,
        "id": 249,
        "no_speech_prob": 0.00029136944795027375,
        "seek": 62982,
        "start": 630.34,
        "temperature": 0,
        "text": " you need to know how to compute this,",
        "tokens": [
          50390,
          291,
          643,
          281,
          458,
          577,
          281,
          14722,
          341,
          11,
          50544
        ]
      },
      {
        "avg_logprob": -0.1944370939020525,
        "compression_ratio": 1.8784530386740332,
        "end": 634.6600000000001,
        "id": 250,
        "no_speech_prob": 0.00029136944795027375,
        "seek": 62982,
        "start": 633.4200000000001,
        "temperature": 0,
        "text": " compute both of these.",
        "tokens": [
          50544,
          14722,
          1293,
          295,
          613,
          13,
          50606
        ]
      },
      {
        "avg_logprob": -0.1944370939020525,
        "compression_ratio": 1.8784530386740332,
        "end": 639.6600000000001,
        "id": 251,
        "no_speech_prob": 0.00029136944795027375,
        "seek": 62982,
        "start": 634.6600000000001,
        "temperature": 0,
        "text": " So this is really just an array of values.",
        "tokens": [
          50606,
          407,
          341,
          307,
          534,
          445,
          364,
          10225,
          295,
          4190,
          13,
          50856
        ]
      },
      {
        "avg_logprob": -0.1944370939020525,
        "compression_ratio": 1.8784530386740332,
        "end": 643.9000000000001,
        "id": 252,
        "no_speech_prob": 0.00029136944795027375,
        "seek": 62982,
        "start": 639.86,
        "temperature": 0,
        "text": " So we can call this array G of I, right?",
        "tokens": [
          50866,
          407,
          321,
          393,
          818,
          341,
          10225,
          460,
          295,
          286,
          11,
          558,
          30,
          51068
        ]
      },
      {
        "avg_logprob": -0.1944370939020525,
        "compression_ratio": 1.8784530386740332,
        "end": 647.38,
        "id": 253,
        "no_speech_prob": 0.00029136944795027375,
        "seek": 62982,
        "start": 643.9000000000001,
        "temperature": 0,
        "text": " So this is G of zero, and this is G of one.",
        "tokens": [
          51068,
          407,
          341,
          307,
          460,
          295,
          4018,
          11,
          293,
          341,
          307,
          460,
          295,
          472,
          13,
          51242
        ]
      },
      {
        "avg_logprob": -0.1944370939020525,
        "compression_ratio": 1.8784530386740332,
        "end": 648.9000000000001,
        "id": 254,
        "no_speech_prob": 0.00029136944795027375,
        "seek": 62982,
        "start": 647.38,
        "temperature": 0,
        "text": " And G just stands for a guess.",
        "tokens": [
          51242,
          400,
          460,
          445,
          7382,
          337,
          257,
          2041,
          13,
          51318
        ]
      },
      {
        "avg_logprob": -0.1944370939020525,
        "compression_ratio": 1.8784530386740332,
        "end": 651.94,
        "id": 255,
        "no_speech_prob": 0.00029136944795027375,
        "seek": 62982,
        "start": 648.9000000000001,
        "temperature": 0,
        "text": " I put zero, G of one, right?",
        "tokens": [
          51318,
          286,
          829,
          4018,
          11,
          460,
          295,
          472,
          11,
          558,
          30,
          51470
        ]
      },
      {
        "avg_logprob": -0.1944370939020525,
        "compression_ratio": 1.8784530386740332,
        "end": 655.1400000000001,
        "id": 256,
        "no_speech_prob": 0.00029136944795027375,
        "seek": 62982,
        "start": 651.94,
        "temperature": 0,
        "text": " And so we wanna know what does G of I,",
        "tokens": [
          51470,
          400,
          370,
          321,
          1948,
          458,
          437,
          775,
          460,
          295,
          286,
          11,
          51630
        ]
      },
      {
        "avg_logprob": -0.1944370939020525,
        "compression_ratio": 1.8784530386740332,
        "end": 658.0200000000001,
        "id": 257,
        "no_speech_prob": 0.00029136944795027375,
        "seek": 62982,
        "start": 655.1400000000001,
        "temperature": 0,
        "text": " or what does G of zero, or what does G of one equal?",
        "tokens": [
          51630,
          420,
          437,
          775,
          460,
          295,
          4018,
          11,
          420,
          437,
          775,
          460,
          295,
          472,
          2681,
          30,
          51774
        ]
      },
      {
        "avg_logprob": -0.19535243872440222,
        "compression_ratio": 1.5857142857142856,
        "end": 660.5,
        "id": 258,
        "no_speech_prob": 0.00006605203088838607,
        "seek": 65802,
        "start": 658.02,
        "temperature": 0,
        "text": " How can we get that equation?",
        "tokens": [
          50364,
          1012,
          393,
          321,
          483,
          300,
          5367,
          30,
          50488
        ]
      },
      {
        "avg_logprob": -0.19535243872440222,
        "compression_ratio": 1.5857142857142856,
        "end": 662.6999999999999,
        "id": 259,
        "no_speech_prob": 0.00006605203088838607,
        "seek": 65802,
        "start": 660.5,
        "temperature": 0,
        "text": " Well, if we look at our diagram for our neural network,",
        "tokens": [
          50488,
          1042,
          11,
          498,
          321,
          574,
          412,
          527,
          10686,
          337,
          527,
          18161,
          3209,
          11,
          50598
        ]
      },
      {
        "avg_logprob": -0.19535243872440222,
        "compression_ratio": 1.5857142857142856,
        "end": 664.1,
        "id": 260,
        "no_speech_prob": 0.00006605203088838607,
        "seek": 65802,
        "start": 662.6999999999999,
        "temperature": 0,
        "text": " it's actually quite simple.",
        "tokens": [
          50598,
          309,
          311,
          767,
          1596,
          2199,
          13,
          50668
        ]
      },
      {
        "avg_logprob": -0.19535243872440222,
        "compression_ratio": 1.5857142857142856,
        "end": 668.74,
        "id": 261,
        "no_speech_prob": 0.00006605203088838607,
        "seek": 65802,
        "start": 664.1,
        "temperature": 0,
        "text": " So G of I, which again is this array, this output layer,",
        "tokens": [
          50668,
          407,
          460,
          295,
          286,
          11,
          597,
          797,
          307,
          341,
          10225,
          11,
          341,
          5598,
          4583,
          11,
          50900
        ]
      },
      {
        "avg_logprob": -0.19535243872440222,
        "compression_ratio": 1.5857142857142856,
        "end": 673.74,
        "id": 262,
        "no_speech_prob": 0.00006605203088838607,
        "seek": 65802,
        "start": 668.74,
        "temperature": 0,
        "text": " G of I equals the summation of a hidden layer.",
        "tokens": [
          50900,
          460,
          295,
          286,
          6915,
          264,
          28811,
          295,
          257,
          7633,
          4583,
          13,
          51150
        ]
      },
      {
        "avg_logprob": -0.19535243872440222,
        "compression_ratio": 1.5857142857142856,
        "end": 679.3,
        "id": 263,
        "no_speech_prob": 0.00006605203088838607,
        "seek": 65802,
        "start": 677.54,
        "temperature": 0,
        "text": " This is gonna be a hidden layer of I,",
        "tokens": [
          51340,
          639,
          307,
          799,
          312,
          257,
          7633,
          4583,
          295,
          286,
          11,
          51428
        ]
      },
      {
        "avg_logprob": -0.19535243872440222,
        "compression_ratio": 1.5857142857142856,
        "end": 681.9,
        "id": 264,
        "no_speech_prob": 0.00006605203088838607,
        "seek": 65802,
        "start": 679.3,
        "temperature": 0,
        "text": " and this is gonna be inputs of I.",
        "tokens": [
          51428,
          293,
          341,
          307,
          799,
          312,
          15743,
          295,
          286,
          13,
          51558
        ]
      },
      {
        "avg_logprob": -0.19535243872440222,
        "compression_ratio": 1.5857142857142856,
        "end": 685.66,
        "id": 265,
        "no_speech_prob": 0.00006605203088838607,
        "seek": 65802,
        "start": 683.14,
        "temperature": 0,
        "text": " That's how we define each of these vectors.",
        "tokens": [
          51620,
          663,
          311,
          577,
          321,
          6964,
          1184,
          295,
          613,
          18875,
          13,
          51746
        ]
      },
      {
        "avg_logprob": -0.22849369049072266,
        "compression_ratio": 1.5222222222222221,
        "end": 690.66,
        "id": 266,
        "no_speech_prob": 0.00009170167322736233,
        "seek": 68566,
        "start": 685.66,
        "temperature": 0,
        "text": " So G of I equals a summation of HL, hidden layer,",
        "tokens": [
          50364,
          407,
          460,
          295,
          286,
          6915,
          257,
          28811,
          295,
          389,
          43,
          11,
          7633,
          4583,
          11,
          50614
        ]
      },
      {
        "avg_logprob": -0.22849369049072266,
        "compression_ratio": 1.5222222222222221,
        "end": 695.88,
        "id": 267,
        "no_speech_prob": 0.00009170167322736233,
        "seek": 68566,
        "start": 692.1,
        "temperature": 0,
        "text": " and then we have to go into another loop,",
        "tokens": [
          50686,
          293,
          550,
          321,
          362,
          281,
          352,
          666,
          1071,
          6367,
          11,
          50875
        ]
      },
      {
        "avg_logprob": -0.22849369049072266,
        "compression_ratio": 1.5222222222222221,
        "end": 698.54,
        "id": 268,
        "no_speech_prob": 0.00009170167322736233,
        "seek": 68566,
        "start": 695.88,
        "temperature": 0,
        "text": " because we can't use the same in this C of I and J,",
        "tokens": [
          50875,
          570,
          321,
          393,
          380,
          764,
          264,
          912,
          294,
          341,
          383,
          295,
          286,
          293,
          508,
          11,
          51008
        ]
      },
      {
        "avg_logprob": -0.22849369049072266,
        "compression_ratio": 1.5222222222222221,
        "end": 702.3,
        "id": 269,
        "no_speech_prob": 0.00009170167322736233,
        "seek": 68566,
        "start": 698.54,
        "temperature": 0,
        "text": " because it won't return the right value.",
        "tokens": [
          51008,
          570,
          309,
          1582,
          380,
          2736,
          264,
          558,
          2158,
          13,
          51196
        ]
      },
      {
        "avg_logprob": -0.22849369049072266,
        "compression_ratio": 1.5222222222222221,
        "end": 707.3,
        "id": 270,
        "no_speech_prob": 0.00009170167322736233,
        "seek": 68566,
        "start": 702.3,
        "temperature": 0,
        "text": " So a hidden layer of J, which is just gonna be zero, one, two",
        "tokens": [
          51196,
          407,
          257,
          7633,
          4583,
          295,
          508,
          11,
          597,
          307,
          445,
          799,
          312,
          4018,
          11,
          472,
          11,
          732,
          51446
        ]
      },
      {
        "avg_logprob": -0.22849369049072266,
        "compression_ratio": 1.5222222222222221,
        "end": 712.78,
        "id": 271,
        "no_speech_prob": 0.00009170167322736233,
        "seek": 68566,
        "start": 707.78,
        "temperature": 0,
        "text": " times the weight of G of I.",
        "tokens": [
          51470,
          1413,
          264,
          3364,
          295,
          460,
          295,
          286,
          13,
          51720
        ]
      },
      {
        "avg_logprob": -0.22228028822918328,
        "compression_ratio": 1.6325581395348838,
        "end": 720.26,
        "id": 272,
        "no_speech_prob": 0.00016346387565135956,
        "seek": 71566,
        "start": 716.54,
        "temperature": 0,
        "text": " Right, and then we simply just add our bias of G of I.",
        "tokens": [
          50408,
          1779,
          11,
          293,
          550,
          321,
          2935,
          445,
          909,
          527,
          12577,
          295,
          460,
          295,
          286,
          13,
          50594
        ]
      },
      {
        "avg_logprob": -0.22228028822918328,
        "compression_ratio": 1.6325581395348838,
        "end": 725.2199999999999,
        "id": 273,
        "no_speech_prob": 0.00016346387565135956,
        "seek": 71566,
        "start": 722.6999999999999,
        "temperature": 0,
        "text": " And so this is the equation that we can use",
        "tokens": [
          50716,
          400,
          370,
          341,
          307,
          264,
          5367,
          300,
          321,
          393,
          764,
          50842
        ]
      },
      {
        "avg_logprob": -0.22228028822918328,
        "compression_ratio": 1.6325581395348838,
        "end": 729.3,
        "id": 274,
        "no_speech_prob": 0.00016346387565135956,
        "seek": 71566,
        "start": 725.2199999999999,
        "temperature": 0,
        "text": " to compute each of our output nodes.",
        "tokens": [
          50842,
          281,
          14722,
          1184,
          295,
          527,
          5598,
          13891,
          13,
          51046
        ]
      },
      {
        "avg_logprob": -0.22228028822918328,
        "compression_ratio": 1.6325581395348838,
        "end": 731.2199999999999,
        "id": 275,
        "no_speech_prob": 0.00016346387565135956,
        "seek": 71566,
        "start": 729.3,
        "temperature": 0,
        "text": " And so just to clarify what's going on here,",
        "tokens": [
          51046,
          400,
          370,
          445,
          281,
          17594,
          437,
          311,
          516,
          322,
          510,
          11,
          51142
        ]
      },
      {
        "avg_logprob": -0.22228028822918328,
        "compression_ratio": 1.6325581395348838,
        "end": 733.8399999999999,
        "id": 276,
        "no_speech_prob": 0.00016346387565135956,
        "seek": 71566,
        "start": 731.2199999999999,
        "temperature": 0,
        "text": " this is summation symbol, which simply means",
        "tokens": [
          51142,
          341,
          307,
          28811,
          5986,
          11,
          597,
          2935,
          1355,
          51273
        ]
      },
      {
        "avg_logprob": -0.22228028822918328,
        "compression_ratio": 1.6325581395348838,
        "end": 737.8199999999999,
        "id": 277,
        "no_speech_prob": 0.00016346387565135956,
        "seek": 71566,
        "start": 733.8399999999999,
        "temperature": 0,
        "text": " to add up all within the array.",
        "tokens": [
          51273,
          281,
          909,
          493,
          439,
          1951,
          264,
          10225,
          13,
          51472
        ]
      },
      {
        "avg_logprob": -0.22228028822918328,
        "compression_ratio": 1.6325581395348838,
        "end": 741.18,
        "id": 278,
        "no_speech_prob": 0.00016346387565135956,
        "seek": 71566,
        "start": 737.8199999999999,
        "temperature": 0,
        "text": " So hidden layer of J times the weight.",
        "tokens": [
          51472,
          407,
          7633,
          4583,
          295,
          508,
          1413,
          264,
          3364,
          13,
          51640
        ]
      },
      {
        "avg_logprob": -0.22228028822918328,
        "compression_ratio": 1.6325581395348838,
        "end": 743.78,
        "id": 279,
        "no_speech_prob": 0.00016346387565135956,
        "seek": 71566,
        "start": 741.18,
        "temperature": 0,
        "text": " This is a function, which simply just grabs the weight",
        "tokens": [
          51640,
          639,
          307,
          257,
          2445,
          11,
          597,
          2935,
          445,
          30028,
          264,
          3364,
          51770
        ]
      },
      {
        "avg_logprob": -0.2494750447792582,
        "compression_ratio": 1.605,
        "end": 748.42,
        "id": 280,
        "no_speech_prob": 0.0004305534530431032,
        "seek": 74378,
        "start": 743.78,
        "temperature": 0,
        "text": " of whatever output node you're on.",
        "tokens": [
          50364,
          295,
          2035,
          5598,
          9984,
          291,
          434,
          322,
          13,
          50596
        ]
      },
      {
        "avg_logprob": -0.2494750447792582,
        "compression_ratio": 1.605,
        "end": 751.74,
        "id": 281,
        "no_speech_prob": 0.0004305534530431032,
        "seek": 74378,
        "start": 748.42,
        "temperature": 0,
        "text": " So if you pass G of zero, for example,",
        "tokens": [
          50596,
          407,
          498,
          291,
          1320,
          460,
          295,
          4018,
          11,
          337,
          1365,
          11,
          50762
        ]
      },
      {
        "avg_logprob": -0.2494750447792582,
        "compression_ratio": 1.605,
        "end": 753.5799999999999,
        "id": 282,
        "no_speech_prob": 0.0004305534530431032,
        "seek": 74378,
        "start": 751.74,
        "temperature": 0,
        "text": " to do this weight function, it will just grab",
        "tokens": [
          50762,
          281,
          360,
          341,
          3364,
          2445,
          11,
          309,
          486,
          445,
          4444,
          50854
        ]
      },
      {
        "avg_logprob": -0.2494750447792582,
        "compression_ratio": 1.605,
        "end": 756.26,
        "id": 283,
        "no_speech_prob": 0.0004305534530431032,
        "seek": 74378,
        "start": 753.5799999999999,
        "temperature": 0,
        "text": " whatever bias, or I'm sorry, whatever weight",
        "tokens": [
          50854,
          2035,
          12577,
          11,
          420,
          286,
          478,
          2597,
          11,
          2035,
          3364,
          50988
        ]
      },
      {
        "avg_logprob": -0.2494750447792582,
        "compression_ratio": 1.605,
        "end": 758.14,
        "id": 284,
        "no_speech_prob": 0.0004305534530431032,
        "seek": 74378,
        "start": 756.26,
        "temperature": 0,
        "text": " of G of I is there.",
        "tokens": [
          50988,
          295,
          460,
          295,
          286,
          307,
          456,
          13,
          51082
        ]
      },
      {
        "avg_logprob": -0.2494750447792582,
        "compression_ratio": 1.605,
        "end": 764.54,
        "id": 285,
        "no_speech_prob": 0.0004305534530431032,
        "seek": 74378,
        "start": 759.54,
        "temperature": 0,
        "text": " So that's actually G of I of J, actually.",
        "tokens": [
          51152,
          407,
          300,
          311,
          767,
          460,
          295,
          286,
          295,
          508,
          11,
          767,
          13,
          51402
        ]
      },
      {
        "avg_logprob": -0.2494750447792582,
        "compression_ratio": 1.605,
        "end": 766.76,
        "id": 286,
        "no_speech_prob": 0.0004305534530431032,
        "seek": 74378,
        "start": 764.86,
        "temperature": 0,
        "text": " Oops, G of I of J.",
        "tokens": [
          51418,
          21726,
          11,
          460,
          295,
          286,
          295,
          508,
          13,
          51513
        ]
      },
      {
        "avg_logprob": -0.2494750447792582,
        "compression_ratio": 1.605,
        "end": 770.8399999999999,
        "id": 287,
        "no_speech_prob": 0.0004305534530431032,
        "seek": 74378,
        "start": 768.72,
        "temperature": 0,
        "text": " So now we have this equation that tells us",
        "tokens": [
          51611,
          407,
          586,
          321,
          362,
          341,
          5367,
          300,
          5112,
          505,
          51717
        ]
      },
      {
        "avg_logprob": -0.2494750447792582,
        "compression_ratio": 1.605,
        "end": 773.12,
        "id": 288,
        "no_speech_prob": 0.0004305534530431032,
        "seek": 74378,
        "start": 770.8399999999999,
        "temperature": 0,
        "text": " exactly what these values equal.",
        "tokens": [
          51717,
          2293,
          437,
          613,
          4190,
          2681,
          13,
          51831
        ]
      },
      {
        "avg_logprob": -0.28272523030196084,
        "compression_ratio": 1.6096256684491979,
        "end": 776.84,
        "id": 289,
        "no_speech_prob": 0.00006709204171784222,
        "seek": 77312,
        "start": 773.44,
        "temperature": 0,
        "text": " So now we don't know what HL of J equals.",
        "tokens": [
          50380,
          407,
          586,
          321,
          500,
          380,
          458,
          437,
          389,
          43,
          295,
          508,
          6915,
          13,
          50550
        ]
      },
      {
        "avg_logprob": -0.28272523030196084,
        "compression_ratio": 1.6096256684491979,
        "end": 779.36,
        "id": 290,
        "no_speech_prob": 0.00006709204171784222,
        "seek": 77312,
        "start": 776.84,
        "temperature": 0,
        "text": " So we also have to define HL of J.",
        "tokens": [
          50550,
          407,
          321,
          611,
          362,
          281,
          6964,
          389,
          43,
          295,
          508,
          13,
          50676
        ]
      },
      {
        "avg_logprob": -0.28272523030196084,
        "compression_ratio": 1.6096256684491979,
        "end": 783.84,
        "id": 291,
        "no_speech_prob": 0.00006709204171784222,
        "seek": 77312,
        "start": 779.36,
        "temperature": 0,
        "text": " And we go about doing that by doing the same exact process.",
        "tokens": [
          50676,
          400,
          321,
          352,
          466,
          884,
          300,
          538,
          884,
          264,
          912,
          1900,
          1399,
          13,
          50900
        ]
      },
      {
        "avg_logprob": -0.28272523030196084,
        "compression_ratio": 1.6096256684491979,
        "end": 787.36,
        "id": 292,
        "no_speech_prob": 0.00006709204171784222,
        "seek": 77312,
        "start": 783.84,
        "temperature": 0,
        "text": " Actually, this would be HL of I for indices.",
        "tokens": [
          50900,
          5135,
          11,
          341,
          576,
          312,
          389,
          43,
          295,
          286,
          337,
          43840,
          13,
          51076
        ]
      },
      {
        "avg_logprob": -0.28272523030196084,
        "compression_ratio": 1.6096256684491979,
        "end": 792.36,
        "id": 293,
        "no_speech_prob": 0.00006709204171784222,
        "seek": 77312,
        "start": 787.36,
        "temperature": 0,
        "text": " Same exact process, summation of our inputs, right?",
        "tokens": [
          51076,
          10635,
          1900,
          1399,
          11,
          28811,
          295,
          527,
          15743,
          11,
          558,
          30,
          51326
        ]
      },
      {
        "avg_logprob": -0.28272523030196084,
        "compression_ratio": 1.6096256684491979,
        "end": 795.44,
        "id": 294,
        "no_speech_prob": 0.00006709204171784222,
        "seek": 77312,
        "start": 793.08,
        "temperature": 0,
        "text": " Inputs, what did I use, I and P.",
        "tokens": [
          51362,
          682,
          2582,
          82,
          11,
          437,
          630,
          286,
          764,
          11,
          286,
          293,
          430,
          13,
          51480
        ]
      },
      {
        "avg_logprob": -0.28272523030196084,
        "compression_ratio": 1.6096256684491979,
        "end": 801.64,
        "id": 295,
        "no_speech_prob": 0.00006709204171784222,
        "seek": 77312,
        "start": 796.64,
        "temperature": 0,
        "text": " Inputs, J times weight of HL of J.",
        "tokens": [
          51540,
          682,
          2582,
          82,
          11,
          508,
          1413,
          3364,
          295,
          389,
          43,
          295,
          508,
          13,
          51790
        ]
      },
      {
        "avg_logprob": -0.2487311771937779,
        "compression_ratio": 1.5931372549019607,
        "end": 806.12,
        "id": 296,
        "no_speech_prob": 0.0001253370865015313,
        "seek": 80312,
        "start": 803.12,
        "temperature": 0,
        "text": " HL of I, so the same exact input.",
        "tokens": [
          50364,
          389,
          43,
          295,
          286,
          11,
          370,
          264,
          912,
          1900,
          4846,
          13,
          50514
        ]
      },
      {
        "avg_logprob": -0.2487311771937779,
        "compression_ratio": 1.5931372549019607,
        "end": 807.88,
        "id": 297,
        "no_speech_prob": 0.0001253370865015313,
        "seek": 80312,
        "start": 806.12,
        "temperature": 0,
        "text": " We need HL of I.",
        "tokens": [
          50514,
          492,
          643,
          389,
          43,
          295,
          286,
          13,
          50602
        ]
      },
      {
        "avg_logprob": -0.2487311771937779,
        "compression_ratio": 1.5931372549019607,
        "end": 812.96,
        "id": 298,
        "no_speech_prob": 0.0001253370865015313,
        "seek": 80312,
        "start": 811.08,
        "temperature": 0,
        "text": " And then we simply just pass our bias.",
        "tokens": [
          50762,
          400,
          550,
          321,
          2935,
          445,
          1320,
          527,
          12577,
          13,
          50856
        ]
      },
      {
        "avg_logprob": -0.2487311771937779,
        "compression_ratio": 1.5931372549019607,
        "end": 815.72,
        "id": 299,
        "no_speech_prob": 0.0001253370865015313,
        "seek": 80312,
        "start": 812.96,
        "temperature": 0,
        "text": " And again, this right here is a function.",
        "tokens": [
          50856,
          400,
          797,
          11,
          341,
          558,
          510,
          307,
          257,
          2445,
          13,
          50994
        ]
      },
      {
        "avg_logprob": -0.2487311771937779,
        "compression_ratio": 1.5931372549019607,
        "end": 819,
        "id": 300,
        "no_speech_prob": 0.0001253370865015313,
        "seek": 80312,
        "start": 815.72,
        "temperature": 0,
        "text": " All it does is it grabs the bias for whatever node",
        "tokens": [
          50994,
          1057,
          309,
          775,
          307,
          309,
          30028,
          264,
          12577,
          337,
          2035,
          9984,
          51158
        ]
      },
      {
        "avg_logprob": -0.2487311771937779,
        "compression_ratio": 1.5931372549019607,
        "end": 820.96,
        "id": 301,
        "no_speech_prob": 0.0001253370865015313,
        "seek": 80312,
        "start": 819,
        "temperature": 0,
        "text": " that we pass through it.",
        "tokens": [
          51158,
          300,
          321,
          1320,
          807,
          309,
          13,
          51256
        ]
      },
      {
        "avg_logprob": -0.2487311771937779,
        "compression_ratio": 1.5931372549019607,
        "end": 822.92,
        "id": 302,
        "no_speech_prob": 0.0001253370865015313,
        "seek": 80312,
        "start": 820.96,
        "temperature": 0,
        "text": " So bias of HL of I.",
        "tokens": [
          51256,
          407,
          12577,
          295,
          389,
          43,
          295,
          286,
          13,
          51354
        ]
      },
      {
        "avg_logprob": -0.2487311771937779,
        "compression_ratio": 1.5931372549019607,
        "end": 825.86,
        "id": 303,
        "no_speech_prob": 0.0001253370865015313,
        "seek": 80312,
        "start": 825.04,
        "temperature": 0,
        "text": " And there we have it.",
        "tokens": [
          51460,
          400,
          456,
          321,
          362,
          309,
          13,
          51501
        ]
      },
      {
        "avg_logprob": -0.2487311771937779,
        "compression_ratio": 1.5931372549019607,
        "end": 828.12,
        "id": 304,
        "no_speech_prob": 0.0001253370865015313,
        "seek": 80312,
        "start": 825.86,
        "temperature": 0,
        "text": " We have our entire equation because we know exactly",
        "tokens": [
          51501,
          492,
          362,
          527,
          2302,
          5367,
          570,
          321,
          458,
          2293,
          51614
        ]
      },
      {
        "avg_logprob": -0.2487311771937779,
        "compression_ratio": 1.5931372549019607,
        "end": 830.26,
        "id": 305,
        "no_speech_prob": 0.0001253370865015313,
        "seek": 80312,
        "start": 828.12,
        "temperature": 0,
        "text": " what input of J equals.",
        "tokens": [
          51614,
          437,
          4846,
          295,
          508,
          6915,
          13,
          51721
        ]
      },
      {
        "avg_logprob": -0.18738467283923216,
        "compression_ratio": 1.6650717703349283,
        "end": 833.34,
        "id": 306,
        "no_speech_prob": 0.0005884058773517609,
        "seek": 83026,
        "start": 830.26,
        "temperature": 0,
        "text": " It's gonna be simply the random value of our color.",
        "tokens": [
          50364,
          467,
          311,
          799,
          312,
          2935,
          264,
          4974,
          2158,
          295,
          527,
          2017,
          13,
          50518
        ]
      },
      {
        "avg_logprob": -0.18738467283923216,
        "compression_ratio": 1.6650717703349283,
        "end": 840.02,
        "id": 307,
        "no_speech_prob": 0.0005884058773517609,
        "seek": 83026,
        "start": 835.02,
        "temperature": 0,
        "text": " And so this is what we need to write in our software.",
        "tokens": [
          50602,
          400,
          370,
          341,
          307,
          437,
          321,
          643,
          281,
          2464,
          294,
          527,
          4722,
          13,
          50852
        ]
      },
      {
        "avg_logprob": -0.18738467283923216,
        "compression_ratio": 1.6650717703349283,
        "end": 845.14,
        "id": 308,
        "no_speech_prob": 0.0005884058773517609,
        "seek": 83026,
        "start": 841.18,
        "temperature": 0,
        "text": " Okay, so same exact thing that you see on the board",
        "tokens": [
          50910,
          1033,
          11,
          370,
          912,
          1900,
          551,
          300,
          291,
          536,
          322,
          264,
          3150,
          51108
        ]
      },
      {
        "avg_logprob": -0.18738467283923216,
        "compression_ratio": 1.6650717703349283,
        "end": 847.66,
        "id": 309,
        "no_speech_prob": 0.0005884058773517609,
        "seek": 83026,
        "start": 845.14,
        "temperature": 0,
        "text": " is what we write here in our code.",
        "tokens": [
          51108,
          307,
          437,
          321,
          2464,
          510,
          294,
          527,
          3089,
          13,
          51234
        ]
      },
      {
        "avg_logprob": -0.18738467283923216,
        "compression_ratio": 1.6650717703349283,
        "end": 851.86,
        "id": 310,
        "no_speech_prob": 0.0005884058773517609,
        "seek": 83026,
        "start": 847.66,
        "temperature": 0,
        "text": " So first, before we can get what the guess nodes equal,",
        "tokens": [
          51234,
          407,
          700,
          11,
          949,
          321,
          393,
          483,
          437,
          264,
          2041,
          13891,
          2681,
          11,
          51444
        ]
      },
      {
        "avg_logprob": -0.18738467283923216,
        "compression_ratio": 1.6650717703349283,
        "end": 854.54,
        "id": 311,
        "no_speech_prob": 0.0005884058773517609,
        "seek": 83026,
        "start": 851.86,
        "temperature": 0,
        "text": " we have to first get what the hidden layer nodes equal.",
        "tokens": [
          51444,
          321,
          362,
          281,
          700,
          483,
          437,
          264,
          7633,
          4583,
          13891,
          2681,
          13,
          51578
        ]
      },
      {
        "avg_logprob": -0.18738467283923216,
        "compression_ratio": 1.6650717703349283,
        "end": 858.9399999999999,
        "id": 312,
        "no_speech_prob": 0.0005884058773517609,
        "seek": 83026,
        "start": 854.54,
        "temperature": 0,
        "text": " So simply put, as we did on the whiteboard,",
        "tokens": [
          51578,
          407,
          2935,
          829,
          11,
          382,
          321,
          630,
          322,
          264,
          2418,
          3787,
          11,
          51798
        ]
      },
      {
        "avg_logprob": -0.20169681051503058,
        "compression_ratio": 1.6395939086294415,
        "end": 860.74,
        "id": 313,
        "no_speech_prob": 0.0007793609402142465,
        "seek": 85894,
        "start": 858.94,
        "temperature": 0,
        "text": " hidden layer zero equals,",
        "tokens": [
          50364,
          7633,
          4583,
          4018,
          6915,
          11,
          50454
        ]
      },
      {
        "avg_logprob": -0.20169681051503058,
        "compression_ratio": 1.6395939086294415,
        "end": 862.84,
        "id": 314,
        "no_speech_prob": 0.0007793609402142465,
        "seek": 85894,
        "start": 860.74,
        "temperature": 0,
        "text": " we'll get to what ReLU is in a second,",
        "tokens": [
          50454,
          321,
          603,
          483,
          281,
          437,
          1300,
          43,
          52,
          307,
          294,
          257,
          1150,
          11,
          50559
        ]
      },
      {
        "avg_logprob": -0.20169681051503058,
        "compression_ratio": 1.6395939086294415,
        "end": 867.34,
        "id": 315,
        "no_speech_prob": 0.0007793609402142465,
        "seek": 85894,
        "start": 862.84,
        "temperature": 0,
        "text": " but hidden layer zero equals, we did our input encoder,",
        "tokens": [
          50559,
          457,
          7633,
          4583,
          4018,
          6915,
          11,
          321,
          630,
          527,
          4846,
          2058,
          19866,
          11,
          50784
        ]
      },
      {
        "avg_logprob": -0.20169681051503058,
        "compression_ratio": 1.6395939086294415,
        "end": 869.5400000000001,
        "id": 316,
        "no_speech_prob": 0.0007793609402142465,
        "seek": 85894,
        "start": 867.34,
        "temperature": 0,
        "text": " which was a question that was asked earlier",
        "tokens": [
          50784,
          597,
          390,
          257,
          1168,
          300,
          390,
          2351,
          3071,
          50894
        ]
      },
      {
        "avg_logprob": -0.20169681051503058,
        "compression_ratio": 1.6395939086294415,
        "end": 872.98,
        "id": 317,
        "no_speech_prob": 0.0007793609402142465,
        "seek": 85894,
        "start": 869.5400000000001,
        "temperature": 0,
        "text": " about normalizing our input data.",
        "tokens": [
          50894,
          466,
          2710,
          3319,
          527,
          4846,
          1412,
          13,
          51066
        ]
      },
      {
        "avg_logprob": -0.20169681051503058,
        "compression_ratio": 1.6395939086294415,
        "end": 875.74,
        "id": 318,
        "no_speech_prob": 0.0007793609402142465,
        "seek": 85894,
        "start": 872.98,
        "temperature": 0,
        "text": " So this function simply just divides our input,",
        "tokens": [
          51066,
          407,
          341,
          2445,
          2935,
          445,
          41347,
          527,
          4846,
          11,
          51204
        ]
      },
      {
        "avg_logprob": -0.20169681051503058,
        "compression_ratio": 1.6395939086294415,
        "end": 878.7,
        "id": 319,
        "no_speech_prob": 0.0007793609402142465,
        "seek": 85894,
        "start": 875.74,
        "temperature": 0,
        "text": " divided by 255, and then we'll times that",
        "tokens": [
          51204,
          6666,
          538,
          3552,
          20,
          11,
          293,
          550,
          321,
          603,
          1413,
          300,
          51352
        ]
      },
      {
        "avg_logprob": -0.20169681051503058,
        "compression_ratio": 1.6395939086294415,
        "end": 883.7,
        "id": 320,
        "no_speech_prob": 0.0007793609402142465,
        "seek": 85894,
        "start": 878.7,
        "temperature": 0,
        "text": " by the weight of our hidden layer.",
        "tokens": [
          51352,
          538,
          264,
          3364,
          295,
          527,
          7633,
          4583,
          13,
          51602
        ]
      },
      {
        "avg_logprob": -0.523763708166174,
        "compression_ratio": 1.6736111111111112,
        "end": 887.0200000000001,
        "id": 321,
        "no_speech_prob": 0.0005112544749863446,
        "seek": 88370,
        "start": 884.38,
        "temperature": 0,
        "text": " So this is an array function",
        "tokens": [
          50398,
          407,
          341,
          307,
          364,
          10225,
          2445,
          50530
        ]
      },
      {
        "avg_logprob": -0.523763708166174,
        "compression_ratio": 1.6736111111111112,
        "end": 890.7,
        "id": 322,
        "no_speech_prob": 0.0005112544749863446,
        "seek": 88370,
        "start": 887.0200000000001,
        "temperature": 0,
        "text": " that I will go over really quickly",
        "tokens": [
          50530,
          300,
          286,
          486,
          352,
          670,
          534,
          2661,
          50714
        ]
      },
      {
        "avg_logprob": -0.523763708166174,
        "compression_ratio": 1.6736111111111112,
        "end": 895.7,
        "id": 323,
        "no_speech_prob": 0.0005112544749863446,
        "seek": 88370,
        "start": 890.7,
        "temperature": 0,
        "text": " that we instantiate to hold all of our weights.",
        "tokens": [
          50714,
          300,
          321,
          9836,
          13024,
          281,
          1797,
          439,
          295,
          527,
          17443,
          13,
          50964
        ]
      },
      {
        "avg_logprob": -0.523763708166174,
        "compression_ratio": 1.6736111111111112,
        "end": 902.7,
        "id": 324,
        "no_speech_prob": 0.0005112544749863446,
        "seek": 88370,
        "start": 899.7,
        "temperature": 0,
        "text": " So color predictor zero, zero, zero.",
        "tokens": [
          51164,
          407,
          2017,
          6069,
          284,
          4018,
          11,
          4018,
          11,
          4018,
          13,
          51314
        ]
      },
      {
        "avg_logprob": -0.523763708166174,
        "compression_ratio": 1.6736111111111112,
        "end": 904.0600000000001,
        "id": 325,
        "no_speech_prob": 0.0005112544749863446,
        "seek": 88370,
        "start": 902.7,
        "temperature": 0,
        "text": " I'll go over this.",
        "tokens": [
          51314,
          286,
          603,
          352,
          670,
          341,
          13,
          51382
        ]
      },
      {
        "avg_logprob": -0.523763708166174,
        "compression_ratio": 1.6736111111111112,
        "end": 905.1800000000001,
        "id": 326,
        "no_speech_prob": 0.0005112544749863446,
        "seek": 88370,
        "start": 904.0600000000001,
        "temperature": 0,
        "text": " I think it's important.",
        "tokens": [
          51382,
          286,
          519,
          309,
          311,
          1021,
          13,
          51438
        ]
      },
      {
        "avg_logprob": -0.523763708166174,
        "compression_ratio": 1.6736111111111112,
        "end": 911.62,
        "id": 327,
        "no_speech_prob": 0.0005112544749863446,
        "seek": 88370,
        "start": 906.62,
        "temperature": 0,
        "text": " So the function color predictor zero, zero, zero,",
        "tokens": [
          51510,
          407,
          264,
          2445,
          2017,
          6069,
          284,
          4018,
          11,
          4018,
          11,
          4018,
          11,
          51760
        ]
      },
      {
        "avg_logprob": -0.220338802144985,
        "compression_ratio": 1.6804123711340206,
        "end": 915.6,
        "id": 328,
        "no_speech_prob": 0.000037052628613309935,
        "seek": 91162,
        "start": 911.62,
        "temperature": 0,
        "text": " the function color predictor variable.",
        "tokens": [
          50364,
          264,
          2445,
          2017,
          6069,
          284,
          7006,
          13,
          50563
        ]
      },
      {
        "avg_logprob": -0.220338802144985,
        "compression_ratio": 1.6804123711340206,
        "end": 922.3,
        "id": 329,
        "no_speech_prob": 0.000037052628613309935,
        "seek": 91162,
        "start": 917.86,
        "temperature": 0,
        "text": " So there's all these different dimensions to it,",
        "tokens": [
          50676,
          407,
          456,
          311,
          439,
          613,
          819,
          12819,
          281,
          309,
          11,
          50898
        ]
      },
      {
        "avg_logprob": -0.220338802144985,
        "compression_ratio": 1.6804123711340206,
        "end": 924.0600000000001,
        "id": 330,
        "no_speech_prob": 0.000037052628613309935,
        "seek": 91162,
        "start": 922.3,
        "temperature": 0,
        "text": " and I think it's interesting,",
        "tokens": [
          50898,
          293,
          286,
          519,
          309,
          311,
          1880,
          11,
          50986
        ]
      },
      {
        "avg_logprob": -0.220338802144985,
        "compression_ratio": 1.6804123711340206,
        "end": 927.14,
        "id": 331,
        "no_speech_prob": 0.000037052628613309935,
        "seek": 91162,
        "start": 924.0600000000001,
        "temperature": 0,
        "text": " or it's important to go over what the dimensions mean.",
        "tokens": [
          50986,
          420,
          309,
          311,
          1021,
          281,
          352,
          670,
          437,
          264,
          12819,
          914,
          13,
          51140
        ]
      },
      {
        "avg_logprob": -0.220338802144985,
        "compression_ratio": 1.6804123711340206,
        "end": 929.82,
        "id": 332,
        "no_speech_prob": 0.000037052628613309935,
        "seek": 91162,
        "start": 927.14,
        "temperature": 0,
        "text": " So let's just get two, and then let's just do,",
        "tokens": [
          51140,
          407,
          718,
          311,
          445,
          483,
          732,
          11,
          293,
          550,
          718,
          311,
          445,
          360,
          11,
          51274
        ]
      },
      {
        "avg_logprob": -0.220338802144985,
        "compression_ratio": 1.6804123711340206,
        "end": 931.26,
        "id": 333,
        "no_speech_prob": 0.000037052628613309935,
        "seek": 91162,
        "start": 929.82,
        "temperature": 0,
        "text": " I don't know, one.",
        "tokens": [
          51274,
          286,
          500,
          380,
          458,
          11,
          472,
          13,
          51346
        ]
      },
      {
        "avg_logprob": -0.220338802144985,
        "compression_ratio": 1.6804123711340206,
        "end": 932.7,
        "id": 334,
        "no_speech_prob": 0.000037052628613309935,
        "seek": 91162,
        "start": 931.26,
        "temperature": 0,
        "text": " So what does this mean?",
        "tokens": [
          51346,
          407,
          437,
          775,
          341,
          914,
          30,
          51418
        ]
      },
      {
        "avg_logprob": -0.220338802144985,
        "compression_ratio": 1.6804123711340206,
        "end": 936.5,
        "id": 335,
        "no_speech_prob": 0.000037052628613309935,
        "seek": 91162,
        "start": 932.7,
        "temperature": 0,
        "text": " If you have color predictor I zero to one,",
        "tokens": [
          51418,
          759,
          291,
          362,
          2017,
          6069,
          284,
          286,
          4018,
          281,
          472,
          11,
          51608
        ]
      },
      {
        "avg_logprob": -0.220338802144985,
        "compression_ratio": 1.6804123711340206,
        "end": 938.1,
        "id": 336,
        "no_speech_prob": 0.000037052628613309935,
        "seek": 91162,
        "start": 936.5,
        "temperature": 0,
        "text": " what does that mean?",
        "tokens": [
          51608,
          437,
          775,
          300,
          914,
          30,
          51688
        ]
      },
      {
        "avg_logprob": -0.2738293497185958,
        "compression_ratio": 1.6464646464646464,
        "end": 943.1,
        "id": 337,
        "no_speech_prob": 0.00020988224423490465,
        "seek": 93810,
        "start": 938.1,
        "temperature": 0,
        "text": " Well, well, so we want to store these arrays",
        "tokens": [
          50364,
          1042,
          11,
          731,
          11,
          370,
          321,
          528,
          281,
          3531,
          613,
          41011,
          50614
        ]
      },
      {
        "avg_logprob": -0.2738293497185958,
        "compression_ratio": 1.6464646464646464,
        "end": 946.66,
        "id": 338,
        "no_speech_prob": 0.00020988224423490465,
        "seek": 93810,
        "start": 944.1800000000001,
        "temperature": 0,
        "text": " into our color predictor variable,",
        "tokens": [
          50668,
          666,
          527,
          2017,
          6069,
          284,
          7006,
          11,
          50792
        ]
      },
      {
        "avg_logprob": -0.2738293497185958,
        "compression_ratio": 1.6464646464646464,
        "end": 948.66,
        "id": 339,
        "no_speech_prob": 0.00020988224423490465,
        "seek": 93810,
        "start": 946.66,
        "temperature": 0,
        "text": " and we go about doing that by defining",
        "tokens": [
          50792,
          293,
          321,
          352,
          466,
          884,
          300,
          538,
          17827,
          50892
        ]
      },
      {
        "avg_logprob": -0.2738293497185958,
        "compression_ratio": 1.6464646464646464,
        "end": 951.46,
        "id": 340,
        "no_speech_prob": 0.00020988224423490465,
        "seek": 93810,
        "start": 949.7,
        "temperature": 0,
        "text": " the location of all of these.",
        "tokens": [
          50944,
          264,
          4914,
          295,
          439,
          295,
          613,
          13,
          51032
        ]
      },
      {
        "avg_logprob": -0.2738293497185958,
        "compression_ratio": 1.6464646464646464,
        "end": 954.46,
        "id": 341,
        "no_speech_prob": 0.00020988224423490465,
        "seek": 93810,
        "start": 951.46,
        "temperature": 0,
        "text": " So the hidden layer is going to be zero,",
        "tokens": [
          51032,
          407,
          264,
          7633,
          4583,
          307,
          516,
          281,
          312,
          4018,
          11,
          51182
        ]
      },
      {
        "avg_logprob": -0.2738293497185958,
        "compression_ratio": 1.6464646464646464,
        "end": 958.14,
        "id": 342,
        "no_speech_prob": 0.00020988224423490465,
        "seek": 93810,
        "start": 954.46,
        "temperature": 0,
        "text": " and then the guess is going to be one, right?",
        "tokens": [
          51182,
          293,
          550,
          264,
          2041,
          307,
          516,
          281,
          312,
          472,
          11,
          558,
          30,
          51366
        ]
      },
      {
        "avg_logprob": -0.2738293497185958,
        "compression_ratio": 1.6464646464646464,
        "end": 961.14,
        "id": 343,
        "no_speech_prob": 0.00020988224423490465,
        "seek": 93810,
        "start": 958.14,
        "temperature": 0,
        "text": " And so all the nodes are then gonna have",
        "tokens": [
          51366,
          400,
          370,
          439,
          264,
          13891,
          366,
          550,
          799,
          362,
          51516
        ]
      },
      {
        "avg_logprob": -0.2738293497185958,
        "compression_ratio": 1.6464646464646464,
        "end": 966.02,
        "id": 344,
        "no_speech_prob": 0.00020988224423490465,
        "seek": 93810,
        "start": 961.14,
        "temperature": 0,
        "text": " their own assignment, so zero, one, and then two.",
        "tokens": [
          51516,
          641,
          1065,
          15187,
          11,
          370,
          4018,
          11,
          472,
          11,
          293,
          550,
          732,
          13,
          51760
        ]
      },
      {
        "avg_logprob": -0.2276135111242775,
        "compression_ratio": 1.8078602620087336,
        "end": 970.66,
        "id": 345,
        "no_speech_prob": 0.0004583103582262993,
        "seek": 96602,
        "start": 966.02,
        "temperature": 0,
        "text": " Same here, this is gonna be zero and one,",
        "tokens": [
          50364,
          10635,
          510,
          11,
          341,
          307,
          799,
          312,
          4018,
          293,
          472,
          11,
          50596
        ]
      },
      {
        "avg_logprob": -0.2276135111242775,
        "compression_ratio": 1.8078602620087336,
        "end": 973.22,
        "id": 346,
        "no_speech_prob": 0.0004583103582262993,
        "seek": 96602,
        "start": 971.54,
        "temperature": 0,
        "text": " and then the weights are also gonna have",
        "tokens": [
          50640,
          293,
          550,
          264,
          17443,
          366,
          611,
          799,
          362,
          50724
        ]
      },
      {
        "avg_logprob": -0.2276135111242775,
        "compression_ratio": 1.8078602620087336,
        "end": 974.46,
        "id": 347,
        "no_speech_prob": 0.0004583103582262993,
        "seek": 96602,
        "start": 973.22,
        "temperature": 0,
        "text": " their own assignment as well.",
        "tokens": [
          50724,
          641,
          1065,
          15187,
          382,
          731,
          13,
          50786
        ]
      },
      {
        "avg_logprob": -0.2276135111242775,
        "compression_ratio": 1.8078602620087336,
        "end": 978.14,
        "id": 348,
        "no_speech_prob": 0.0004583103582262993,
        "seek": 96602,
        "start": 974.46,
        "temperature": 0,
        "text": " So zero, one, two, three, and the same deal,",
        "tokens": [
          50786,
          407,
          4018,
          11,
          472,
          11,
          732,
          11,
          1045,
          11,
          293,
          264,
          912,
          2028,
          11,
          50970
        ]
      },
      {
        "avg_logprob": -0.2276135111242775,
        "compression_ratio": 1.8078602620087336,
        "end": 981.34,
        "id": 349,
        "no_speech_prob": 0.0004583103582262993,
        "seek": 96602,
        "start": 978.14,
        "temperature": 0,
        "text": " zero, one, two, three, and we repeat that",
        "tokens": [
          50970,
          4018,
          11,
          472,
          11,
          732,
          11,
          1045,
          11,
          293,
          321,
          7149,
          300,
          51130
        ]
      },
      {
        "avg_logprob": -0.2276135111242775,
        "compression_ratio": 1.8078602620087336,
        "end": 984.54,
        "id": 350,
        "no_speech_prob": 0.0004583103582262993,
        "seek": 96602,
        "start": 981.34,
        "temperature": 0,
        "text": " for every single weight inside of the nodes, right?",
        "tokens": [
          51130,
          337,
          633,
          2167,
          3364,
          1854,
          295,
          264,
          13891,
          11,
          558,
          30,
          51290
        ]
      },
      {
        "avg_logprob": -0.2276135111242775,
        "compression_ratio": 1.8078602620087336,
        "end": 987.9,
        "id": 351,
        "no_speech_prob": 0.0004583103582262993,
        "seek": 96602,
        "start": 984.54,
        "temperature": 0,
        "text": " And so how this works is our color predictor",
        "tokens": [
          51290,
          400,
          370,
          577,
          341,
          1985,
          307,
          527,
          2017,
          6069,
          284,
          51458
        ]
      },
      {
        "avg_logprob": -0.2276135111242775,
        "compression_ratio": 1.8078602620087336,
        "end": 990.8199999999999,
        "id": 352,
        "no_speech_prob": 0.0004583103582262993,
        "seek": 96602,
        "start": 987.9,
        "temperature": 0,
        "text": " is if we want to grab reference to zero,",
        "tokens": [
          51458,
          307,
          498,
          321,
          528,
          281,
          4444,
          6408,
          281,
          4018,
          11,
          51604
        ]
      },
      {
        "avg_logprob": -0.2276135111242775,
        "compression_ratio": 1.8078602620087336,
        "end": 993.02,
        "id": 353,
        "no_speech_prob": 0.0004583103582262993,
        "seek": 96602,
        "start": 990.8199999999999,
        "temperature": 0,
        "text": " that is going to be hidden layer,",
        "tokens": [
          51604,
          300,
          307,
          516,
          281,
          312,
          7633,
          4583,
          11,
          51714
        ]
      },
      {
        "avg_logprob": -0.2276135111242775,
        "compression_ratio": 1.8078602620087336,
        "end": 995.9399999999999,
        "id": 354,
        "no_speech_prob": 0.0004583103582262993,
        "seek": 96602,
        "start": 993.02,
        "temperature": 0,
        "text": " and then two is going to be the last node,",
        "tokens": [
          51714,
          293,
          550,
          732,
          307,
          516,
          281,
          312,
          264,
          1036,
          9984,
          11,
          51860
        ]
      },
      {
        "avg_logprob": -0.23903609566066575,
        "compression_ratio": 1.7552742616033756,
        "end": 999.1400000000001,
        "id": 355,
        "no_speech_prob": 0.000698665389791131,
        "seek": 99594,
        "start": 996.82,
        "temperature": 0,
        "text": " and then one is going to be the second weight,",
        "tokens": [
          50408,
          293,
          550,
          472,
          307,
          516,
          281,
          312,
          264,
          1150,
          3364,
          11,
          50524
        ]
      },
      {
        "avg_logprob": -0.23903609566066575,
        "compression_ratio": 1.7552742616033756,
        "end": 1001.1800000000001,
        "id": 356,
        "no_speech_prob": 0.000698665389791131,
        "seek": 99594,
        "start": 999.1400000000001,
        "temperature": 0,
        "text": " because start zero, one, second weight.",
        "tokens": [
          50524,
          570,
          722,
          4018,
          11,
          472,
          11,
          1150,
          3364,
          13,
          50626
        ]
      },
      {
        "avg_logprob": -0.23903609566066575,
        "compression_ratio": 1.7552742616033756,
        "end": 1005.1,
        "id": 357,
        "no_speech_prob": 0.000698665389791131,
        "seek": 99594,
        "start": 1001.1800000000001,
        "temperature": 0,
        "text": " So this variable is grabbing a reference to this weight.",
        "tokens": [
          50626,
          407,
          341,
          7006,
          307,
          23771,
          257,
          6408,
          281,
          341,
          3364,
          13,
          50822
        ]
      },
      {
        "avg_logprob": -0.23903609566066575,
        "compression_ratio": 1.7552742616033756,
        "end": 1009.5400000000001,
        "id": 358,
        "no_speech_prob": 0.000698665389791131,
        "seek": 99594,
        "start": 1005.1,
        "temperature": 0,
        "text": " That's exactly what color predictor zero, two, one is doing.",
        "tokens": [
          50822,
          663,
          311,
          2293,
          437,
          2017,
          6069,
          284,
          4018,
          11,
          732,
          11,
          472,
          307,
          884,
          13,
          51044
        ]
      },
      {
        "avg_logprob": -0.23903609566066575,
        "compression_ratio": 1.7552742616033756,
        "end": 1011.94,
        "id": 359,
        "no_speech_prob": 0.000698665389791131,
        "seek": 99594,
        "start": 1009.5400000000001,
        "temperature": 0,
        "text": " And so you see this I, that's an extra dimension",
        "tokens": [
          51044,
          400,
          370,
          291,
          536,
          341,
          286,
          11,
          300,
          311,
          364,
          2857,
          10139,
          51164
        ]
      },
      {
        "avg_logprob": -0.23903609566066575,
        "compression_ratio": 1.7552742616033756,
        "end": 1013.5,
        "id": 360,
        "no_speech_prob": 0.000698665389791131,
        "seek": 99594,
        "start": 1011.94,
        "temperature": 0,
        "text": " that you might be confused about.",
        "tokens": [
          51164,
          300,
          291,
          1062,
          312,
          9019,
          466,
          13,
          51242
        ]
      },
      {
        "avg_logprob": -0.23903609566066575,
        "compression_ratio": 1.7552742616033756,
        "end": 1015.74,
        "id": 361,
        "no_speech_prob": 0.000698665389791131,
        "seek": 99594,
        "start": 1013.5,
        "temperature": 0,
        "text": " So let's talk about that real briefly.",
        "tokens": [
          51242,
          407,
          718,
          311,
          751,
          466,
          300,
          957,
          10515,
          13,
          51354
        ]
      },
      {
        "avg_logprob": -0.23903609566066575,
        "compression_ratio": 1.7552742616033756,
        "end": 1018.6800000000001,
        "id": 362,
        "no_speech_prob": 0.000698665389791131,
        "seek": 99594,
        "start": 1015.74,
        "temperature": 0,
        "text": " So you see this extra I, and what does that mean?",
        "tokens": [
          51354,
          407,
          291,
          536,
          341,
          2857,
          286,
          11,
          293,
          437,
          775,
          300,
          914,
          30,
          51501
        ]
      },
      {
        "avg_logprob": -0.23903609566066575,
        "compression_ratio": 1.7552742616033756,
        "end": 1024.1000000000001,
        "id": 363,
        "no_speech_prob": 0.000698665389791131,
        "seek": 99594,
        "start": 1019.82,
        "temperature": 0,
        "text": " So traditionally, with such an example,",
        "tokens": [
          51558,
          407,
          19067,
          11,
          365,
          1270,
          364,
          1365,
          11,
          51772
        ]
      },
      {
        "avg_logprob": -0.25887713065514195,
        "compression_ratio": 1.6597510373443984,
        "end": 1028.6599999999999,
        "id": 364,
        "no_speech_prob": 0.0006878438871353865,
        "seek": 102410,
        "start": 1024.1,
        "temperature": 0,
        "text": " you would use back propagation to train this neural network.",
        "tokens": [
          50364,
          291,
          576,
          764,
          646,
          38377,
          281,
          3847,
          341,
          18161,
          3209,
          13,
          50592
        ]
      },
      {
        "avg_logprob": -0.25887713065514195,
        "compression_ratio": 1.6597510373443984,
        "end": 1032.62,
        "id": 365,
        "no_speech_prob": 0.0006878438871353865,
        "seek": 102410,
        "start": 1028.6599999999999,
        "temperature": 0,
        "text": " However, time was a factor, as well as,",
        "tokens": [
          50592,
          2908,
          11,
          565,
          390,
          257,
          5952,
          11,
          382,
          731,
          382,
          11,
          50790
        ]
      },
      {
        "avg_logprob": -0.25887713065514195,
        "compression_ratio": 1.6597510373443984,
        "end": 1036.04,
        "id": 366,
        "no_speech_prob": 0.0006878438871353865,
        "seek": 102410,
        "start": 1032.62,
        "temperature": 0,
        "text": " we wanted to go over a lesson of both neural networks",
        "tokens": [
          50790,
          321,
          1415,
          281,
          352,
          670,
          257,
          6898,
          295,
          1293,
          18161,
          9590,
          50961
        ]
      },
      {
        "avg_logprob": -0.25887713065514195,
        "compression_ratio": 1.6597510373443984,
        "end": 1040.26,
        "id": 367,
        "no_speech_prob": 0.0006878438871353865,
        "seek": 102410,
        "start": 1036.04,
        "temperature": 0,
        "text": " and genetic algorithms, so why not combine them together,",
        "tokens": [
          50961,
          293,
          12462,
          14642,
          11,
          370,
          983,
          406,
          10432,
          552,
          1214,
          11,
          51172
        ]
      },
      {
        "avg_logprob": -0.25887713065514195,
        "compression_ratio": 1.6597510373443984,
        "end": 1041.4199999999998,
        "id": 368,
        "no_speech_prob": 0.0006878438871353865,
        "seek": 102410,
        "start": 1040.26,
        "temperature": 0,
        "text": " is what we did for this example.",
        "tokens": [
          51172,
          307,
          437,
          321,
          630,
          337,
          341,
          1365,
          13,
          51230
        ]
      },
      {
        "avg_logprob": -0.25887713065514195,
        "compression_ratio": 1.6597510373443984,
        "end": 1045.06,
        "id": 369,
        "no_speech_prob": 0.0006878438871353865,
        "seek": 102410,
        "start": 1041.4199999999998,
        "temperature": 0,
        "text": " So the I is actually just grabbing a reference",
        "tokens": [
          51230,
          407,
          264,
          286,
          307,
          767,
          445,
          23771,
          257,
          6408,
          51412
        ]
      },
      {
        "avg_logprob": -0.25887713065514195,
        "compression_ratio": 1.6597510373443984,
        "end": 1049.26,
        "id": 370,
        "no_speech_prob": 0.0006878438871353865,
        "seek": 102410,
        "start": 1045.06,
        "temperature": 0,
        "text": " to what predictor we are currently using.",
        "tokens": [
          51412,
          281,
          437,
          6069,
          284,
          321,
          366,
          4362,
          1228,
          13,
          51622
        ]
      },
      {
        "avg_logprob": -0.25887713065514195,
        "compression_ratio": 1.6597510373443984,
        "end": 1052.34,
        "id": 371,
        "no_speech_prob": 0.0006878438871353865,
        "seek": 102410,
        "start": 1049.26,
        "temperature": 0,
        "text": " So genetic algorithms, real quick,",
        "tokens": [
          51622,
          407,
          12462,
          14642,
          11,
          957,
          1702,
          11,
          51776
        ]
      },
      {
        "avg_logprob": -0.25887713065514195,
        "compression_ratio": 1.6597510373443984,
        "end": 1053.86,
        "id": 372,
        "no_speech_prob": 0.0006878438871353865,
        "seek": 102410,
        "start": 1052.34,
        "temperature": 0,
        "text": " you have to have a population,",
        "tokens": [
          51776,
          291,
          362,
          281,
          362,
          257,
          4415,
          11,
          51852
        ]
      },
      {
        "avg_logprob": -0.2670952848685804,
        "compression_ratio": 1.8775510204081634,
        "end": 1057.3,
        "id": 373,
        "no_speech_prob": 0.015904739499092102,
        "seek": 105386,
        "start": 1054.62,
        "temperature": 0,
        "text": " you have to assign fitness scores to every single,",
        "tokens": [
          50402,
          291,
          362,
          281,
          6269,
          15303,
          13444,
          281,
          633,
          2167,
          11,
          50536
        ]
      },
      {
        "avg_logprob": -0.2670952848685804,
        "compression_ratio": 1.8775510204081634,
        "end": 1061.62,
        "id": 374,
        "no_speech_prob": 0.015904739499092102,
        "seek": 105386,
        "start": 1060.1399999999999,
        "temperature": 0,
        "text": " what's the word I'm looking for,",
        "tokens": [
          50678,
          437,
          311,
          264,
          1349,
          286,
          478,
          1237,
          337,
          11,
          50752
        ]
      },
      {
        "avg_logprob": -0.2670952848685804,
        "compression_ratio": 1.8775510204081634,
        "end": 1063.54,
        "id": 375,
        "no_speech_prob": 0.015904739499092102,
        "seek": 105386,
        "start": 1061.62,
        "temperature": 0,
        "text": " creature within the population,",
        "tokens": [
          50752,
          12797,
          1951,
          264,
          4415,
          11,
          50848
        ]
      },
      {
        "avg_logprob": -0.2670952848685804,
        "compression_ratio": 1.8775510204081634,
        "end": 1066.6599999999999,
        "id": 376,
        "no_speech_prob": 0.015904739499092102,
        "seek": 105386,
        "start": 1063.54,
        "temperature": 0,
        "text": " and then you have to mutate them and breed them in X, Y, and Z.",
        "tokens": [
          50848,
          293,
          550,
          291,
          362,
          281,
          5839,
          473,
          552,
          293,
          18971,
          552,
          294,
          1783,
          11,
          398,
          11,
          293,
          1176,
          13,
          51004
        ]
      },
      {
        "avg_logprob": -0.2670952848685804,
        "compression_ratio": 1.8775510204081634,
        "end": 1071.06,
        "id": 377,
        "no_speech_prob": 0.015904739499092102,
        "seek": 105386,
        "start": 1066.6599999999999,
        "temperature": 0,
        "text": " So we have a population of 100 predictors at start,",
        "tokens": [
          51004,
          407,
          321,
          362,
          257,
          4415,
          295,
          2319,
          6069,
          830,
          412,
          722,
          11,
          51224
        ]
      },
      {
        "avg_logprob": -0.2670952848685804,
        "compression_ratio": 1.8775510204081634,
        "end": 1074.9799999999998,
        "id": 378,
        "no_speech_prob": 0.015904739499092102,
        "seek": 105386,
        "start": 1071.06,
        "temperature": 0,
        "text": " and then they all have randomly initialized weights",
        "tokens": [
          51224,
          293,
          550,
          436,
          439,
          362,
          16979,
          5883,
          1602,
          17443,
          51420
        ]
      },
      {
        "avg_logprob": -0.2670952848685804,
        "compression_ratio": 1.8775510204081634,
        "end": 1077.8,
        "id": 379,
        "no_speech_prob": 0.015904739499092102,
        "seek": 105386,
        "start": 1074.9799999999998,
        "temperature": 0,
        "text": " and biases, which again, just to make sure we're clear,",
        "tokens": [
          51420,
          293,
          32152,
          11,
          597,
          797,
          11,
          445,
          281,
          652,
          988,
          321,
          434,
          1850,
          11,
          51561
        ]
      },
      {
        "avg_logprob": -0.2670952848685804,
        "compression_ratio": 1.8775510204081634,
        "end": 1080.74,
        "id": 380,
        "no_speech_prob": 0.015904739499092102,
        "seek": 105386,
        "start": 1077.8,
        "temperature": 0,
        "text": " are all of these values, weight, weight, weight, bias,",
        "tokens": [
          51561,
          366,
          439,
          295,
          613,
          4190,
          11,
          3364,
          11,
          3364,
          11,
          3364,
          11,
          12577,
          11,
          51708
        ]
      },
      {
        "avg_logprob": -0.2670952848685804,
        "compression_ratio": 1.8775510204081634,
        "end": 1081.58,
        "id": 381,
        "no_speech_prob": 0.015904739499092102,
        "seek": 105386,
        "start": 1080.74,
        "temperature": 0,
        "text": " weight, weight, weight, bias.",
        "tokens": [
          51708,
          3364,
          11,
          3364,
          11,
          3364,
          11,
          12577,
          13,
          51750
        ]
      },
      {
        "avg_logprob": -0.2670952848685804,
        "compression_ratio": 1.8775510204081634,
        "end": 1083.4599999999998,
        "id": 382,
        "no_speech_prob": 0.015904739499092102,
        "seek": 105386,
        "start": 1081.58,
        "temperature": 0,
        "text": " These are all randomly initialized.",
        "tokens": [
          51750,
          1981,
          366,
          439,
          16979,
          5883,
          1602,
          13,
          51844
        ]
      },
      {
        "avg_logprob": -0.32267377617653836,
        "compression_ratio": 1.726829268292683,
        "end": 1086.42,
        "id": 383,
        "no_speech_prob": 0.0006771782645955682,
        "seek": 108346,
        "start": 1084.06,
        "temperature": 0,
        "text": " So the function that we use for this program",
        "tokens": [
          50394,
          407,
          264,
          2445,
          300,
          321,
          764,
          337,
          341,
          1461,
          50512
        ]
      },
      {
        "avg_logprob": -0.32267377617653836,
        "compression_ratio": 1.726829268292683,
        "end": 1090.02,
        "id": 384,
        "no_speech_prob": 0.0006771782645955682,
        "seek": 108346,
        "start": 1086.42,
        "temperature": 0,
        "text": " is randomized between zero and one,",
        "tokens": [
          50512,
          307,
          38513,
          1296,
          4018,
          293,
          472,
          11,
          50692
        ]
      },
      {
        "avg_logprob": -0.32267377617653836,
        "compression_ratio": 1.726829268292683,
        "end": 1092.3,
        "id": 385,
        "no_speech_prob": 0.0006771782645955682,
        "seek": 108346,
        "start": 1090.02,
        "temperature": 0,
        "text": " and then they all,",
        "tokens": [
          50692,
          293,
          550,
          436,
          439,
          11,
          50806
        ]
      },
      {
        "avg_logprob": -0.32267377617653836,
        "compression_ratio": 1.726829268292683,
        "end": 1094.22,
        "id": 386,
        "no_speech_prob": 0.0006771782645955682,
        "seek": 108346,
        "start": 1092.3,
        "temperature": 0,
        "text": " based on their randomly initialized weights,",
        "tokens": [
          50806,
          2361,
          322,
          641,
          16979,
          5883,
          1602,
          17443,
          11,
          50902
        ]
      },
      {
        "avg_logprob": -0.32267377617653836,
        "compression_ratio": 1.726829268292683,
        "end": 1097.58,
        "id": 387,
        "no_speech_prob": 0.0006771782645955682,
        "seek": 108346,
        "start": 1094.22,
        "temperature": 0,
        "text": " will make a guess on which one they think is correct,",
        "tokens": [
          50902,
          486,
          652,
          257,
          2041,
          322,
          597,
          472,
          436,
          519,
          307,
          3006,
          11,
          51070
        ]
      },
      {
        "avg_logprob": -0.32267377617653836,
        "compression_ratio": 1.726829268292683,
        "end": 1102.58,
        "id": 388,
        "no_speech_prob": 0.0006771782645955682,
        "seek": 108346,
        "start": 1097.58,
        "temperature": 0,
        "text": " and so most of them said that black is the correct color",
        "tokens": [
          51070,
          293,
          370,
          881,
          295,
          552,
          848,
          300,
          2211,
          307,
          264,
          3006,
          2017,
          51320
        ]
      },
      {
        "avg_logprob": -0.32267377617653836,
        "compression_ratio": 1.726829268292683,
        "end": 1107.3400000000001,
        "id": 389,
        "no_speech_prob": 0.0006771782645955682,
        "seek": 108346,
        "start": 1103.24,
        "temperature": 0,
        "text": " that looks best over this randomized color,",
        "tokens": [
          51353,
          300,
          1542,
          1151,
          670,
          341,
          38513,
          2017,
          11,
          51558
        ]
      },
      {
        "avg_logprob": -0.32267377617653836,
        "compression_ratio": 1.726829268292683,
        "end": 1112.3400000000001,
        "id": 390,
        "no_speech_prob": 0.0006771782645955682,
        "seek": 108346,
        "start": 1107.3400000000001,
        "temperature": 0,
        "text": " and then we simply just use a generalization function,",
        "tokens": [
          51558,
          293,
          550,
          321,
          2935,
          445,
          764,
          257,
          2674,
          2144,
          2445,
          11,
          51808
        ]
      },
      {
        "avg_logprob": -0.20657068140366497,
        "compression_ratio": 1.437125748502994,
        "end": 1117.6200000000001,
        "id": 391,
        "no_speech_prob": 0.0002492304192855954,
        "seek": 111346,
        "start": 1113.46,
        "temperature": 0,
        "text": " a genetic algorithm to train this predictor",
        "tokens": [
          50364,
          257,
          12462,
          9284,
          281,
          3847,
          341,
          6069,
          284,
          50572
        ]
      },
      {
        "avg_logprob": -0.20657068140366497,
        "compression_ratio": 1.437125748502994,
        "end": 1120.76,
        "id": 392,
        "no_speech_prob": 0.0002492304192855954,
        "seek": 111346,
        "start": 1117.6200000000001,
        "temperature": 0,
        "text": " to converge on the best possible predictor.",
        "tokens": [
          50572,
          281,
          41881,
          322,
          264,
          1151,
          1944,
          6069,
          284,
          13,
          50729
        ]
      },
      {
        "avg_logprob": -0.20657068140366497,
        "compression_ratio": 1.437125748502994,
        "end": 1128.26,
        "id": 393,
        "no_speech_prob": 0.0002492304192855954,
        "seek": 111346,
        "start": 1123.92,
        "temperature": 0,
        "text": " And yeah, that's a general overview on this.",
        "tokens": [
          50887,
          400,
          1338,
          11,
          300,
          311,
          257,
          2674,
          12492,
          322,
          341,
          13,
          51104
        ]
      },
      {
        "avg_logprob": -0.20657068140366497,
        "compression_ratio": 1.437125748502994,
        "end": 1132.7,
        "id": 394,
        "no_speech_prob": 0.0002492304192855954,
        "seek": 111346,
        "start": 1128.26,
        "temperature": 0,
        "text": " The code is available on GitHub,",
        "tokens": [
          51104,
          440,
          3089,
          307,
          2435,
          322,
          23331,
          11,
          51326
        ]
      },
      {
        "avg_logprob": -0.20657068140366497,
        "compression_ratio": 1.437125748502994,
        "end": 1135.58,
        "id": 395,
        "no_speech_prob": 0.0002492304192855954,
        "seek": 111346,
        "start": 1132.7,
        "temperature": 0,
        "text": " and I left a lot of comments.",
        "tokens": [
          51326,
          293,
          286,
          1411,
          257,
          688,
          295,
          3053,
          13,
          51470
        ]
      },
      {
        "avg_logprob": -0.20657068140366497,
        "compression_ratio": 1.437125748502994,
        "end": 1140.14,
        "id": 396,
        "no_speech_prob": 0.0002492304192855954,
        "seek": 111346,
        "start": 1135.58,
        "temperature": 0,
        "text": " However, it will require a bit more in-depth",
        "tokens": [
          51470,
          2908,
          11,
          309,
          486,
          3651,
          257,
          857,
          544,
          294,
          12,
          25478,
          51698
        ]
      },
      {
        "avg_logprob": -0.2411897908086362,
        "compression_ratio": 1.762962962962963,
        "end": 1143.7800000000002,
        "id": 397,
        "no_speech_prob": 0.002672914182767272,
        "seek": 114014,
        "start": 1140.3400000000001,
        "temperature": 0,
        "text": " if you really want to get a full grasp on this",
        "tokens": [
          50374,
          498,
          291,
          534,
          528,
          281,
          483,
          257,
          1577,
          21743,
          322,
          341,
          50546
        ]
      },
      {
        "avg_logprob": -0.2411897908086362,
        "compression_ratio": 1.762962962962963,
        "end": 1146.0600000000002,
        "id": 398,
        "no_speech_prob": 0.002672914182767272,
        "seek": 114014,
        "start": 1143.7800000000002,
        "temperature": 0,
        "text": " from knowing absolutely nothing.",
        "tokens": [
          50546,
          490,
          5276,
          3122,
          1825,
          13,
          50660
        ]
      },
      {
        "avg_logprob": -0.2411897908086362,
        "compression_ratio": 1.762962962962963,
        "end": 1147.16,
        "id": 399,
        "no_speech_prob": 0.002672914182767272,
        "seek": 114014,
        "start": 1146.0600000000002,
        "temperature": 0,
        "text": " If you already know some stuff",
        "tokens": [
          50660,
          759,
          291,
          1217,
          458,
          512,
          1507,
          50715
        ]
      },
      {
        "avg_logprob": -0.2411897908086362,
        "compression_ratio": 1.762962962962963,
        "end": 1148.8200000000002,
        "id": 400,
        "no_speech_prob": 0.002672914182767272,
        "seek": 114014,
        "start": 1147.16,
        "temperature": 0,
        "text": " about neural networks and machine learning,",
        "tokens": [
          50715,
          466,
          18161,
          9590,
          293,
          3479,
          2539,
          11,
          50798
        ]
      },
      {
        "avg_logprob": -0.2411897908086362,
        "compression_ratio": 1.762962962962963,
        "end": 1149.94,
        "id": 401,
        "no_speech_prob": 0.002672914182767272,
        "seek": 114014,
        "start": 1148.8200000000002,
        "temperature": 0,
        "text": " I'm pretty sure this example",
        "tokens": [
          50798,
          286,
          478,
          1238,
          988,
          341,
          1365,
          50854
        ]
      },
      {
        "avg_logprob": -0.2411897908086362,
        "compression_ratio": 1.762962962962963,
        "end": 1151.26,
        "id": 402,
        "no_speech_prob": 0.002672914182767272,
        "seek": 114014,
        "start": 1149.94,
        "temperature": 0,
        "text": " is pretty straightforward for you,",
        "tokens": [
          50854,
          307,
          1238,
          15325,
          337,
          291,
          11,
          50920
        ]
      },
      {
        "avg_logprob": -0.2411897908086362,
        "compression_ratio": 1.762962962962963,
        "end": 1155.5200000000002,
        "id": 403,
        "no_speech_prob": 0.002672914182767272,
        "seek": 114014,
        "start": 1151.26,
        "temperature": 0,
        "text": " but the benefit of writing neural networks from scratch",
        "tokens": [
          50920,
          457,
          264,
          5121,
          295,
          3579,
          18161,
          9590,
          490,
          8459,
          51133
        ]
      },
      {
        "avg_logprob": -0.2411897908086362,
        "compression_ratio": 1.762962962962963,
        "end": 1157.3000000000002,
        "id": 404,
        "no_speech_prob": 0.002672914182767272,
        "seek": 114014,
        "start": 1155.5200000000002,
        "temperature": 0,
        "text": " is that you really have a good grasp",
        "tokens": [
          51133,
          307,
          300,
          291,
          534,
          362,
          257,
          665,
          21743,
          51222
        ]
      },
      {
        "avg_logprob": -0.2411897908086362,
        "compression_ratio": 1.762962962962963,
        "end": 1159.0200000000002,
        "id": 405,
        "no_speech_prob": 0.002672914182767272,
        "seek": 114014,
        "start": 1157.3000000000002,
        "temperature": 0,
        "text": " on what's going on behind the scenes",
        "tokens": [
          51222,
          322,
          437,
          311,
          516,
          322,
          2261,
          264,
          8026,
          51308
        ]
      },
      {
        "avg_logprob": -0.2411897908086362,
        "compression_ratio": 1.762962962962963,
        "end": 1160.8400000000001,
        "id": 406,
        "no_speech_prob": 0.002672914182767272,
        "seek": 114014,
        "start": 1159.0200000000002,
        "temperature": 0,
        "text": " versus using libraries,",
        "tokens": [
          51308,
          5717,
          1228,
          15148,
          11,
          51399
        ]
      },
      {
        "avg_logprob": -0.2411897908086362,
        "compression_ratio": 1.762962962962963,
        "end": 1165.8200000000002,
        "id": 407,
        "no_speech_prob": 0.002672914182767272,
        "seek": 114014,
        "start": 1162.16,
        "temperature": 0,
        "text": " and you're able to debug different problems",
        "tokens": [
          51465,
          293,
          291,
          434,
          1075,
          281,
          24083,
          819,
          2740,
          51648
        ]
      },
      {
        "avg_logprob": -0.2411897908086362,
        "compression_ratio": 1.762962962962963,
        "end": 1169.94,
        "id": 408,
        "no_speech_prob": 0.002672914182767272,
        "seek": 114014,
        "start": 1165.8200000000002,
        "temperature": 0,
        "text": " that might arise when you are writing your neural networks,",
        "tokens": [
          51648,
          300,
          1062,
          20288,
          562,
          291,
          366,
          3579,
          428,
          18161,
          9590,
          11,
          51854
        ]
      },
      {
        "avg_logprob": -0.30814410568377293,
        "compression_ratio": 1.491304347826087,
        "end": 1172.78,
        "id": 409,
        "no_speech_prob": 0.0002913621428888291,
        "seek": 116994,
        "start": 1170.74,
        "temperature": 0,
        "text": " be it from scratch or using libraries.",
        "tokens": [
          50404,
          312,
          309,
          490,
          8459,
          420,
          1228,
          15148,
          13,
          50506
        ]
      },
      {
        "avg_logprob": -0.30814410568377293,
        "compression_ratio": 1.491304347826087,
        "end": 1173.8200000000002,
        "id": 410,
        "no_speech_prob": 0.0002913621428888291,
        "seek": 116994,
        "start": 1172.78,
        "temperature": 0,
        "text": " That is pretty much it.",
        "tokens": [
          50506,
          663,
          307,
          1238,
          709,
          309,
          13,
          50558
        ]
      },
      {
        "avg_logprob": -0.30814410568377293,
        "compression_ratio": 1.491304347826087,
        "end": 1175.54,
        "id": 411,
        "no_speech_prob": 0.0002913621428888291,
        "seek": 116994,
        "start": 1173.8200000000002,
        "temperature": 0,
        "text": " We do have plans to update it",
        "tokens": [
          50558,
          492,
          360,
          362,
          5482,
          281,
          5623,
          309,
          50644
        ]
      },
      {
        "avg_logprob": -0.30814410568377293,
        "compression_ratio": 1.491304347826087,
        "end": 1180.02,
        "id": 412,
        "no_speech_prob": 0.0002913621428888291,
        "seek": 116994,
        "start": 1175.54,
        "temperature": 0,
        "text": " with the actual backpropagation algorithm in there",
        "tokens": [
          50644,
          365,
          264,
          3539,
          646,
          79,
          1513,
          559,
          399,
          9284,
          294,
          456,
          50868
        ]
      },
      {
        "avg_logprob": -0.30814410568377293,
        "compression_ratio": 1.491304347826087,
        "end": 1183.78,
        "id": 413,
        "no_speech_prob": 0.0002913621428888291,
        "seek": 116994,
        "start": 1180.02,
        "temperature": 0,
        "text": " so that you can learn from that as well.",
        "tokens": [
          50868,
          370,
          300,
          291,
          393,
          1466,
          490,
          300,
          382,
          731,
          13,
          51056
        ]
      },
      {
        "avg_logprob": -0.30814410568377293,
        "compression_ratio": 1.491304347826087,
        "end": 1185.38,
        "id": 414,
        "no_speech_prob": 0.0002913621428888291,
        "seek": 116994,
        "start": 1183.78,
        "temperature": 0,
        "text": " So let me come over here.",
        "tokens": [
          51056,
          407,
          718,
          385,
          808,
          670,
          510,
          13,
          51136
        ]
      },
      {
        "avg_logprob": -0.30814410568377293,
        "compression_ratio": 1.491304347826087,
        "end": 1187.06,
        "id": 415,
        "no_speech_prob": 0.0002913621428888291,
        "seek": 116994,
        "start": 1185.38,
        "temperature": 0,
        "text": " There are a few more questions.",
        "tokens": [
          51136,
          821,
          366,
          257,
          1326,
          544,
          1651,
          13,
          51220
        ]
      },
      {
        "avg_logprob": -0.30814410568377293,
        "compression_ratio": 1.491304347826087,
        "end": 1187.9,
        "id": 416,
        "no_speech_prob": 0.0002913621428888291,
        "seek": 116994,
        "start": 1187.06,
        "temperature": 0,
        "text": " Okay.",
        "tokens": [
          51220,
          1033,
          13,
          51262
        ]
      },
      {
        "avg_logprob": -0.30814410568377293,
        "compression_ratio": 1.491304347826087,
        "end": 1190.66,
        "id": 417,
        "no_speech_prob": 0.0002913621428888291,
        "seek": 116994,
        "start": 1188.78,
        "temperature": 0,
        "text": " Let's see what I can find here.",
        "tokens": [
          51306,
          961,
          311,
          536,
          437,
          286,
          393,
          915,
          510,
          13,
          51400
        ]
      },
      {
        "avg_logprob": -0.30814410568377293,
        "compression_ratio": 1.491304347826087,
        "end": 1191.48,
        "id": 418,
        "no_speech_prob": 0.0002913621428888291,
        "seek": 116994,
        "start": 1190.66,
        "temperature": 0,
        "text": " Oh.",
        "tokens": [
          51400,
          876,
          13,
          51441
        ]
      },
      {
        "avg_logprob": -0.30814410568377293,
        "compression_ratio": 1.491304347826087,
        "end": 1196.5,
        "id": 419,
        "no_speech_prob": 0.0002913621428888291,
        "seek": 116994,
        "start": 1194.38,
        "temperature": 0,
        "text": " So first, people had asked,",
        "tokens": [
          51586,
          407,
          700,
          11,
          561,
          632,
          2351,
          11,
          51692
        ]
      },
      {
        "avg_logprob": -0.30814410568377293,
        "compression_ratio": 1.491304347826087,
        "end": 1198.06,
        "id": 420,
        "no_speech_prob": 0.0002913621428888291,
        "seek": 116994,
        "start": 1196.5,
        "temperature": 0,
        "text": " is the code already at GitHub,",
        "tokens": [
          51692,
          307,
          264,
          3089,
          1217,
          412,
          23331,
          11,
          51770
        ]
      },
      {
        "avg_logprob": -0.2974560312981153,
        "compression_ratio": 1.6328125,
        "end": 1199.82,
        "id": 421,
        "no_speech_prob": 0.0023595746606588364,
        "seek": 119806,
        "start": 1198.58,
        "temperature": 0,
        "text": " on GitHub, or are you going to update it on GitHub later?",
        "tokens": [
          50390,
          322,
          23331,
          11,
          420,
          366,
          291,
          516,
          281,
          5623,
          309,
          322,
          23331,
          1780,
          30,
          50452
        ]
      },
      {
        "avg_logprob": -0.2974560312981153,
        "compression_ratio": 1.6328125,
        "end": 1202.06,
        "id": 422,
        "no_speech_prob": 0.0023595746606588364,
        "seek": 119806,
        "start": 1199.82,
        "temperature": 0,
        "text": " Yeah, I still have to upload it to GitHub.",
        "tokens": [
          50452,
          865,
          11,
          286,
          920,
          362,
          281,
          6580,
          309,
          281,
          23331,
          13,
          50564
        ]
      },
      {
        "avg_logprob": -0.2974560312981153,
        "compression_ratio": 1.6328125,
        "end": 1203.98,
        "id": 423,
        "no_speech_prob": 0.0023595746606588364,
        "seek": 119806,
        "start": 1202.06,
        "temperature": 0,
        "text": " So stay tuned.",
        "tokens": [
          50564,
          407,
          1754,
          10870,
          13,
          50660
        ]
      },
      {
        "avg_logprob": -0.2974560312981153,
        "compression_ratio": 1.6328125,
        "end": 1206.4199999999998,
        "id": 424,
        "no_speech_prob": 0.0023595746606588364,
        "seek": 119806,
        "start": 1203.98,
        "temperature": 0,
        "text": " Whenever it's on GitHub,",
        "tokens": [
          50660,
          14159,
          309,
          311,
          322,
          23331,
          11,
          50782
        ]
      },
      {
        "avg_logprob": -0.2974560312981153,
        "compression_ratio": 1.6328125,
        "end": 1208.5,
        "id": 425,
        "no_speech_prob": 0.0023595746606588364,
        "seek": 119806,
        "start": 1206.4199999999998,
        "temperature": 0,
        "text": " I will come back and edit the description",
        "tokens": [
          50782,
          286,
          486,
          808,
          646,
          293,
          8129,
          264,
          3855,
          50886
        ]
      },
      {
        "avg_logprob": -0.2974560312981153,
        "compression_ratio": 1.6328125,
        "end": 1210.62,
        "id": 426,
        "no_speech_prob": 0.0023595746606588364,
        "seek": 119806,
        "start": 1208.5,
        "temperature": 0,
        "text": " for this livestream and put a link to the code there.",
        "tokens": [
          50886,
          337,
          341,
          29782,
          293,
          829,
          257,
          2113,
          281,
          264,
          3089,
          456,
          13,
          50992
        ]
      },
      {
        "avg_logprob": -0.2974560312981153,
        "compression_ratio": 1.6328125,
        "end": 1215.62,
        "id": 427,
        "no_speech_prob": 0.0023595746606588364,
        "seek": 119806,
        "start": 1210.62,
        "temperature": 0,
        "text": " But they can currently go to cephstuff.com slash color.",
        "tokens": [
          50992,
          583,
          436,
          393,
          4362,
          352,
          281,
          269,
          595,
          71,
          372,
          1245,
          13,
          1112,
          17330,
          2017,
          13,
          51242
        ]
      },
      {
        "avg_logprob": -0.2974560312981153,
        "compression_ratio": 1.6328125,
        "end": 1217.22,
        "id": 428,
        "no_speech_prob": 0.0023595746606588364,
        "seek": 119806,
        "start": 1215.94,
        "temperature": 0,
        "text": " Oh, that link that I use.",
        "tokens": [
          51258,
          876,
          11,
          300,
          2113,
          300,
          286,
          764,
          13,
          51322
        ]
      },
      {
        "avg_logprob": -0.2974560312981153,
        "compression_ratio": 1.6328125,
        "end": 1218.06,
        "id": 429,
        "no_speech_prob": 0.0023595746606588364,
        "seek": 119806,
        "start": 1217.22,
        "temperature": 0,
        "text": " Right.",
        "tokens": [
          51322,
          1779,
          13,
          51364
        ]
      },
      {
        "avg_logprob": -0.2974560312981153,
        "compression_ratio": 1.6328125,
        "end": 1219.26,
        "id": 430,
        "no_speech_prob": 0.0023595746606588364,
        "seek": 119806,
        "start": 1218.06,
        "temperature": 0,
        "text": " Yeah, so you can go to,",
        "tokens": [
          51364,
          865,
          11,
          370,
          291,
          393,
          352,
          281,
          11,
          51424
        ]
      },
      {
        "avg_logprob": -0.2974560312981153,
        "compression_ratio": 1.6328125,
        "end": 1221.1,
        "id": 431,
        "no_speech_prob": 0.0023595746606588364,
        "seek": 119806,
        "start": 1219.26,
        "temperature": 0,
        "text": " let me zoom in here and show you.",
        "tokens": [
          51424,
          718,
          385,
          8863,
          294,
          510,
          293,
          855,
          291,
          13,
          51516
        ]
      },
      {
        "avg_logprob": -0.2974560312981153,
        "compression_ratio": 1.6328125,
        "end": 1225.5,
        "id": 432,
        "no_speech_prob": 0.0023595746606588364,
        "seek": 119806,
        "start": 1221.1,
        "temperature": 0,
        "text": " So cephstuff.com, this one, right?",
        "tokens": [
          51516,
          407,
          45026,
          71,
          372,
          1245,
          13,
          1112,
          11,
          341,
          472,
          11,
          558,
          30,
          51736
        ]
      },
      {
        "avg_logprob": -0.26157407891260437,
        "compression_ratio": 1.7321428571428572,
        "end": 1228.66,
        "id": 433,
        "no_speech_prob": 0.0038241890724748373,
        "seek": 122550,
        "start": 1225.54,
        "temperature": 0,
        "text": " So if you want to grab the code right now,",
        "tokens": [
          50366,
          407,
          498,
          291,
          528,
          281,
          4444,
          264,
          3089,
          558,
          586,
          11,
          50522
        ]
      },
      {
        "avg_logprob": -0.26157407891260437,
        "compression_ratio": 1.7321428571428572,
        "end": 1230.18,
        "id": 434,
        "no_speech_prob": 0.0038241890724748373,
        "seek": 122550,
        "start": 1228.66,
        "temperature": 0,
        "text": " and I don't know why,",
        "tokens": [
          50522,
          293,
          286,
          500,
          380,
          458,
          983,
          11,
          50598
        ]
      },
      {
        "avg_logprob": -0.26157407891260437,
        "compression_ratio": 1.7321428571428572,
        "end": 1232.7,
        "id": 435,
        "no_speech_prob": 0.0038241890724748373,
        "seek": 122550,
        "start": 1230.18,
        "temperature": 0,
        "text": " whenever I paste links into the YouTube chat,",
        "tokens": [
          50598,
          5699,
          286,
          9163,
          6123,
          666,
          264,
          3088,
          5081,
          11,
          50724
        ]
      },
      {
        "avg_logprob": -0.26157407891260437,
        "compression_ratio": 1.7321428571428572,
        "end": 1234.62,
        "id": 436,
        "no_speech_prob": 0.0038241890724748373,
        "seek": 122550,
        "start": 1232.7,
        "temperature": 0,
        "text": " they don't seem to work for people.",
        "tokens": [
          50724,
          436,
          500,
          380,
          1643,
          281,
          589,
          337,
          561,
          13,
          50820
        ]
      },
      {
        "avg_logprob": -0.26157407891260437,
        "compression_ratio": 1.7321428571428572,
        "end": 1236.46,
        "id": 437,
        "no_speech_prob": 0.0038241890724748373,
        "seek": 122550,
        "start": 1234.62,
        "temperature": 0,
        "text": " So but anyway, so I would paste this into the chat,",
        "tokens": [
          50820,
          407,
          457,
          4033,
          11,
          370,
          286,
          576,
          9163,
          341,
          666,
          264,
          5081,
          11,
          50912
        ]
      },
      {
        "avg_logprob": -0.26157407891260437,
        "compression_ratio": 1.7321428571428572,
        "end": 1237.5,
        "id": 438,
        "no_speech_prob": 0.0038241890724748373,
        "seek": 122550,
        "start": 1236.46,
        "temperature": 0,
        "text": " but that wouldn't even work.",
        "tokens": [
          50912,
          457,
          300,
          2759,
          380,
          754,
          589,
          13,
          50964
        ]
      },
      {
        "avg_logprob": -0.26157407891260437,
        "compression_ratio": 1.7321428571428572,
        "end": 1241.18,
        "id": 439,
        "no_speech_prob": 0.0038241890724748373,
        "seek": 122550,
        "start": 1237.5,
        "temperature": 0,
        "text": " So you can see it up there,",
        "tokens": [
          50964,
          407,
          291,
          393,
          536,
          309,
          493,
          456,
          11,
          51148
        ]
      },
      {
        "avg_logprob": -0.26157407891260437,
        "compression_ratio": 1.7321428571428572,
        "end": 1242.26,
        "id": 440,
        "no_speech_prob": 0.0038241890724748373,
        "seek": 122550,
        "start": 1241.18,
        "temperature": 0,
        "text": " that you can grab the code now.",
        "tokens": [
          51148,
          300,
          291,
          393,
          4444,
          264,
          3089,
          586,
          13,
          51202
        ]
      },
      {
        "avg_logprob": -0.26157407891260437,
        "compression_ratio": 1.7321428571428572,
        "end": 1245.5,
        "id": 441,
        "no_speech_prob": 0.0038241890724748373,
        "seek": 122550,
        "start": 1242.26,
        "temperature": 0,
        "text": " But I will also include a link",
        "tokens": [
          51202,
          583,
          286,
          486,
          611,
          4090,
          257,
          2113,
          51364
        ]
      },
      {
        "avg_logprob": -0.26157407891260437,
        "compression_ratio": 1.7321428571428572,
        "end": 1247.58,
        "id": 442,
        "no_speech_prob": 0.0038241890724748373,
        "seek": 122550,
        "start": 1245.5,
        "temperature": 0,
        "text": " to GitHub repository, whatever that's.",
        "tokens": [
          51364,
          281,
          23331,
          25841,
          11,
          2035,
          300,
          311,
          13,
          51468
        ]
      },
      {
        "avg_logprob": -0.26157407891260437,
        "compression_ratio": 1.7321428571428572,
        "end": 1249.1,
        "id": 443,
        "no_speech_prob": 0.0038241890724748373,
        "seek": 122550,
        "start": 1247.58,
        "temperature": 0,
        "text": " And it is my intention,",
        "tokens": [
          51468,
          400,
          309,
          307,
          452,
          7789,
          11,
          51544
        ]
      },
      {
        "avg_logprob": -0.26157407891260437,
        "compression_ratio": 1.7321428571428572,
        "end": 1252.14,
        "id": 444,
        "no_speech_prob": 0.0038241890724748373,
        "seek": 122550,
        "start": 1249.1,
        "temperature": 0,
        "text": " I think one of the reasons why I love this demo,",
        "tokens": [
          51544,
          286,
          519,
          472,
          295,
          264,
          4112,
          983,
          286,
          959,
          341,
          10723,
          11,
          51696
        ]
      },
      {
        "avg_logprob": -0.26157407891260437,
        "compression_ratio": 1.7321428571428572,
        "end": 1254.14,
        "id": 445,
        "no_speech_prob": 0.0038241890724748373,
        "seek": 122550,
        "start": 1252.14,
        "temperature": 0,
        "text": " and people were kind of asking this a bit in the chat,",
        "tokens": [
          51696,
          293,
          561,
          645,
          733,
          295,
          3365,
          341,
          257,
          857,
          294,
          264,
          5081,
          11,
          51796
        ]
      },
      {
        "avg_logprob": -0.2728323553600451,
        "compression_ratio": 1.7247386759581882,
        "end": 1256.8600000000001,
        "id": 446,
        "no_speech_prob": 0.003707184921950102,
        "seek": 125414,
        "start": 1254.14,
        "temperature": 0,
        "text": " was like, oh, do you really need a neural network for this?",
        "tokens": [
          50364,
          390,
          411,
          11,
          1954,
          11,
          360,
          291,
          534,
          643,
          257,
          18161,
          3209,
          337,
          341,
          30,
          50500
        ]
      },
      {
        "avg_logprob": -0.2728323553600451,
        "compression_ratio": 1.7247386759581882,
        "end": 1257.7,
        "id": 447,
        "no_speech_prob": 0.003707184921950102,
        "seek": 125414,
        "start": 1256.8600000000001,
        "temperature": 0,
        "text": " Right.",
        "tokens": [
          50500,
          1779,
          13,
          50542
        ]
      },
      {
        "avg_logprob": -0.2728323553600451,
        "compression_ratio": 1.7247386759581882,
        "end": 1258.7,
        "id": 448,
        "no_speech_prob": 0.003707184921950102,
        "seek": 125414,
        "start": 1257.7,
        "temperature": 0,
        "text": " And I think to me,",
        "tokens": [
          50542,
          400,
          286,
          519,
          281,
          385,
          11,
          50592
        ]
      },
      {
        "avg_logprob": -0.2728323553600451,
        "compression_ratio": 1.7247386759581882,
        "end": 1260.94,
        "id": 449,
        "no_speech_prob": 0.003707184921950102,
        "seek": 125414,
        "start": 1258.7,
        "temperature": 0,
        "text": " I mean, that's a perfectly valid and interesting question.",
        "tokens": [
          50592,
          286,
          914,
          11,
          300,
          311,
          257,
          6239,
          7363,
          293,
          1880,
          1168,
          13,
          50704
        ]
      },
      {
        "avg_logprob": -0.2728323553600451,
        "compression_ratio": 1.7247386759581882,
        "end": 1262.26,
        "id": 450,
        "no_speech_prob": 0.003707184921950102,
        "seek": 125414,
        "start": 1260.94,
        "temperature": 0,
        "text": " And probably the answer is no,",
        "tokens": [
          50704,
          400,
          1391,
          264,
          1867,
          307,
          572,
          11,
          50770
        ]
      },
      {
        "avg_logprob": -0.2728323553600451,
        "compression_ratio": 1.7247386759581882,
        "end": 1264.3400000000001,
        "id": 451,
        "no_speech_prob": 0.003707184921950102,
        "seek": 125414,
        "start": 1262.26,
        "temperature": 0,
        "text": " you don't need a neural network for this.",
        "tokens": [
          50770,
          291,
          500,
          380,
          643,
          257,
          18161,
          3209,
          337,
          341,
          13,
          50874
        ]
      },
      {
        "avg_logprob": -0.2728323553600451,
        "compression_ratio": 1.7247386759581882,
        "end": 1266.3400000000001,
        "id": 452,
        "no_speech_prob": 0.003707184921950102,
        "seek": 125414,
        "start": 1264.3400000000001,
        "temperature": 0,
        "text": " But when learning about neural networks,",
        "tokens": [
          50874,
          583,
          562,
          2539,
          466,
          18161,
          9590,
          11,
          50974
        ]
      },
      {
        "avg_logprob": -0.2728323553600451,
        "compression_ratio": 1.7247386759581882,
        "end": 1268.6200000000001,
        "id": 453,
        "no_speech_prob": 0.003707184921950102,
        "seek": 125414,
        "start": 1266.3400000000001,
        "temperature": 0,
        "text": " when trying to build your own machine learning project,",
        "tokens": [
          50974,
          562,
          1382,
          281,
          1322,
          428,
          1065,
          3479,
          2539,
          1716,
          11,
          51088
        ]
      },
      {
        "avg_logprob": -0.2728323553600451,
        "compression_ratio": 1.7247386759581882,
        "end": 1271.42,
        "id": 454,
        "no_speech_prob": 0.003707184921950102,
        "seek": 125414,
        "start": 1268.6200000000001,
        "temperature": 0,
        "text": " if you can start with a well-defined,",
        "tokens": [
          51088,
          498,
          291,
          393,
          722,
          365,
          257,
          731,
          12,
          37716,
          11,
          51228
        ]
      },
      {
        "avg_logprob": -0.2728323553600451,
        "compression_ratio": 1.7247386759581882,
        "end": 1273.5,
        "id": 455,
        "no_speech_prob": 0.003707184921950102,
        "seek": 125414,
        "start": 1271.42,
        "temperature": 0,
        "text": " small in scope problem,",
        "tokens": [
          51228,
          1359,
          294,
          11923,
          1154,
          11,
          51332
        ]
      },
      {
        "avg_logprob": -0.2728323553600451,
        "compression_ratio": 1.7247386759581882,
        "end": 1274.94,
        "id": 456,
        "no_speech_prob": 0.003707184921950102,
        "seek": 125414,
        "start": 1273.5,
        "temperature": 0,
        "text": " then you can really figure out,",
        "tokens": [
          51332,
          550,
          291,
          393,
          534,
          2573,
          484,
          11,
          51404
        ]
      },
      {
        "avg_logprob": -0.2728323553600451,
        "compression_ratio": 1.7247386759581882,
        "end": 1278.0600000000002,
        "id": 457,
        "no_speech_prob": 0.003707184921950102,
        "seek": 125414,
        "start": 1274.94,
        "temperature": 0,
        "text": " and because in some ways I do this similarly",
        "tokens": [
          51404,
          293,
          570,
          294,
          512,
          2098,
          286,
          360,
          341,
          14138,
          51560
        ]
      },
      {
        "avg_logprob": -0.2728323553600451,
        "compression_ratio": 1.7247386759581882,
        "end": 1281.38,
        "id": 458,
        "no_speech_prob": 0.003707184921950102,
        "seek": 125414,
        "start": 1278.0600000000002,
        "temperature": 0,
        "text": " with the semi-genetic algorithm projects.",
        "tokens": [
          51560,
          365,
          264,
          12909,
          12,
          1766,
          3532,
          9284,
          4455,
          13,
          51726
        ]
      },
      {
        "avg_logprob": -0.24135108399235344,
        "compression_ratio": 1.806020066889632,
        "end": 1285.7,
        "id": 459,
        "no_speech_prob": 0.0005274630966596305,
        "seek": 128138,
        "start": 1281.38,
        "temperature": 0,
        "text": " I take a example where I know the answer.",
        "tokens": [
          50364,
          286,
          747,
          257,
          1365,
          689,
          286,
          458,
          264,
          1867,
          13,
          50580
        ]
      },
      {
        "avg_logprob": -0.24135108399235344,
        "compression_ratio": 1.806020066889632,
        "end": 1288.3000000000002,
        "id": 460,
        "no_speech_prob": 0.0005274630966596305,
        "seek": 128138,
        "start": 1285.7,
        "temperature": 0,
        "text": " So I can see if the genetic algorithm worked.",
        "tokens": [
          50580,
          407,
          286,
          393,
          536,
          498,
          264,
          12462,
          9284,
          2732,
          13,
          50710
        ]
      },
      {
        "avg_logprob": -0.24135108399235344,
        "compression_ratio": 1.806020066889632,
        "end": 1289.66,
        "id": 461,
        "no_speech_prob": 0.0005274630966596305,
        "seek": 128138,
        "start": 1288.3000000000002,
        "temperature": 0,
        "text": " Because ultimately what I want to do",
        "tokens": [
          50710,
          1436,
          6284,
          437,
          286,
          528,
          281,
          360,
          50778
        ]
      },
      {
        "avg_logprob": -0.24135108399235344,
        "compression_ratio": 1.806020066889632,
        "end": 1291.7800000000002,
        "id": 462,
        "no_speech_prob": 0.0005274630966596305,
        "seek": 128138,
        "start": 1289.66,
        "temperature": 0,
        "text": " is use a neural network or a genetic algorithm",
        "tokens": [
          50778,
          307,
          764,
          257,
          18161,
          3209,
          420,
          257,
          12462,
          9284,
          50884
        ]
      },
      {
        "avg_logprob": -0.24135108399235344,
        "compression_ratio": 1.806020066889632,
        "end": 1294.0600000000002,
        "id": 463,
        "no_speech_prob": 0.0005274630966596305,
        "seek": 128138,
        "start": 1291.7800000000002,
        "temperature": 0,
        "text": " in some domain where maybe I don't know the answer,",
        "tokens": [
          50884,
          294,
          512,
          9274,
          689,
          1310,
          286,
          500,
          380,
          458,
          264,
          1867,
          11,
          50998
        ]
      },
      {
        "avg_logprob": -0.24135108399235344,
        "compression_ratio": 1.806020066889632,
        "end": 1295.8600000000001,
        "id": 464,
        "no_speech_prob": 0.0005274630966596305,
        "seek": 128138,
        "start": 1294.0600000000002,
        "temperature": 0,
        "text": " or couldn't solve the answer easily.",
        "tokens": [
          50998,
          420,
          2809,
          380,
          5039,
          264,
          1867,
          3612,
          13,
          51088
        ]
      },
      {
        "avg_logprob": -0.24135108399235344,
        "compression_ratio": 1.806020066889632,
        "end": 1297.3000000000002,
        "id": 465,
        "no_speech_prob": 0.0005274630966596305,
        "seek": 128138,
        "start": 1295.8600000000001,
        "temperature": 0,
        "text": " But to figure out how those things work,",
        "tokens": [
          51088,
          583,
          281,
          2573,
          484,
          577,
          729,
          721,
          589,
          11,
          51160
        ]
      },
      {
        "avg_logprob": -0.24135108399235344,
        "compression_ratio": 1.806020066889632,
        "end": 1298.3400000000001,
        "id": 466,
        "no_speech_prob": 0.0005274630966596305,
        "seek": 128138,
        "start": 1297.3000000000002,
        "temperature": 0,
        "text": " I've got to come up with,",
        "tokens": [
          51160,
          286,
          600,
          658,
          281,
          808,
          493,
          365,
          11,
          51212
        ]
      },
      {
        "avg_logprob": -0.24135108399235344,
        "compression_ratio": 1.806020066889632,
        "end": 1300.6200000000001,
        "id": 467,
        "no_speech_prob": 0.0005274630966596305,
        "seek": 128138,
        "start": 1298.3400000000001,
        "temperature": 0,
        "text": " and so this is a really nice problem for that,",
        "tokens": [
          51212,
          293,
          370,
          341,
          307,
          257,
          534,
          1481,
          1154,
          337,
          300,
          11,
          51326
        ]
      },
      {
        "avg_logprob": -0.24135108399235344,
        "compression_ratio": 1.806020066889632,
        "end": 1302.14,
        "id": 468,
        "no_speech_prob": 0.0005274630966596305,
        "seek": 128138,
        "start": 1300.6200000000001,
        "temperature": 0,
        "text": " because it's simple, small in scope,",
        "tokens": [
          51326,
          570,
          309,
          311,
          2199,
          11,
          1359,
          294,
          11923,
          11,
          51402
        ]
      },
      {
        "avg_logprob": -0.24135108399235344,
        "compression_ratio": 1.806020066889632,
        "end": 1304.7800000000002,
        "id": 469,
        "no_speech_prob": 0.0005274630966596305,
        "seek": 128138,
        "start": 1302.14,
        "temperature": 0,
        "text": " and for people who want to do creative coding,",
        "tokens": [
          51402,
          293,
          337,
          561,
          567,
          528,
          281,
          360,
          5880,
          17720,
          11,
          51534
        ]
      },
      {
        "avg_logprob": -0.24135108399235344,
        "compression_ratio": 1.806020066889632,
        "end": 1306.0600000000002,
        "id": 470,
        "no_speech_prob": 0.0005274630966596305,
        "seek": 128138,
        "start": 1304.7800000000002,
        "temperature": 0,
        "text": " and graphics, and design stuff,",
        "tokens": [
          51534,
          293,
          11837,
          11,
          293,
          1715,
          1507,
          11,
          51598
        ]
      },
      {
        "avg_logprob": -0.24135108399235344,
        "compression_ratio": 1.806020066889632,
        "end": 1308.2600000000002,
        "id": 471,
        "no_speech_prob": 0.0005274630966596305,
        "seek": 128138,
        "start": 1306.0600000000002,
        "temperature": 0,
        "text": " it's got color, and graphics.",
        "tokens": [
          51598,
          309,
          311,
          658,
          2017,
          11,
          293,
          11837,
          13,
          51708
        ]
      },
      {
        "avg_logprob": -0.24135108399235344,
        "compression_ratio": 1.806020066889632,
        "end": 1309.22,
        "id": 472,
        "no_speech_prob": 0.0005274630966596305,
        "seek": 128138,
        "start": 1308.2600000000002,
        "temperature": 0,
        "text": " And not to mention,",
        "tokens": [
          51708,
          400,
          406,
          281,
          2152,
          11,
          51756
        ]
      },
      {
        "avg_logprob": -0.21864303588867187,
        "compression_ratio": 1.7147766323024054,
        "end": 1311.82,
        "id": 473,
        "no_speech_prob": 0.004006405360996723,
        "seek": 130922,
        "start": 1309.22,
        "temperature": 0,
        "text": " once you learn how to do this stuff from scratch,",
        "tokens": [
          50364,
          1564,
          291,
          1466,
          577,
          281,
          360,
          341,
          1507,
          490,
          8459,
          11,
          50494
        ]
      },
      {
        "avg_logprob": -0.21864303588867187,
        "compression_ratio": 1.7147766323024054,
        "end": 1315.26,
        "id": 474,
        "no_speech_prob": 0.004006405360996723,
        "seek": 130922,
        "start": 1311.82,
        "temperature": 0,
        "text": " this is easily scalable for the most part.",
        "tokens": [
          50494,
          341,
          307,
          3612,
          38481,
          337,
          264,
          881,
          644,
          13,
          50666
        ]
      },
      {
        "avg_logprob": -0.21864303588867187,
        "compression_ratio": 1.7147766323024054,
        "end": 1316.6200000000001,
        "id": 475,
        "no_speech_prob": 0.004006405360996723,
        "seek": 130922,
        "start": 1315.26,
        "temperature": 0,
        "text": " You still have to worry about some other stuff",
        "tokens": [
          50666,
          509,
          920,
          362,
          281,
          3292,
          466,
          512,
          661,
          1507,
          50734
        ]
      },
      {
        "avg_logprob": -0.21864303588867187,
        "compression_ratio": 1.7147766323024054,
        "end": 1318.22,
        "id": 476,
        "no_speech_prob": 0.004006405360996723,
        "seek": 130922,
        "start": 1316.6200000000001,
        "temperature": 0,
        "text": " like vanishing gradients and stuff like that,",
        "tokens": [
          50734,
          411,
          3161,
          3807,
          2771,
          2448,
          293,
          1507,
          411,
          300,
          11,
          50814
        ]
      },
      {
        "avg_logprob": -0.21864303588867187,
        "compression_ratio": 1.7147766323024054,
        "end": 1320.38,
        "id": 477,
        "no_speech_prob": 0.004006405360996723,
        "seek": 130922,
        "start": 1318.22,
        "temperature": 0,
        "text": " but for the most part, you can take this and scale this up",
        "tokens": [
          50814,
          457,
          337,
          264,
          881,
          644,
          11,
          291,
          393,
          747,
          341,
          293,
          4373,
          341,
          493,
          50922
        ]
      },
      {
        "avg_logprob": -0.21864303588867187,
        "compression_ratio": 1.7147766323024054,
        "end": 1322.26,
        "id": 478,
        "no_speech_prob": 0.004006405360996723,
        "seek": 130922,
        "start": 1320.38,
        "temperature": 0,
        "text": " and it'll work just about the same.",
        "tokens": [
          50922,
          293,
          309,
          603,
          589,
          445,
          466,
          264,
          912,
          13,
          51016
        ]
      },
      {
        "avg_logprob": -0.21864303588867187,
        "compression_ratio": 1.7147766323024054,
        "end": 1324.54,
        "id": 479,
        "no_speech_prob": 0.004006405360996723,
        "seek": 130922,
        "start": 1322.26,
        "temperature": 0,
        "text": " So there's also that benefit as well.",
        "tokens": [
          51016,
          407,
          456,
          311,
          611,
          300,
          5121,
          382,
          731,
          13,
          51130
        ]
      },
      {
        "avg_logprob": -0.21864303588867187,
        "compression_ratio": 1.7147766323024054,
        "end": 1327.94,
        "id": 480,
        "no_speech_prob": 0.004006405360996723,
        "seek": 130922,
        "start": 1324.54,
        "temperature": 0,
        "text": " So here's a question from IamRoshan on Twitter.",
        "tokens": [
          51130,
          407,
          510,
          311,
          257,
          1168,
          490,
          286,
          335,
          49,
          3019,
          282,
          322,
          5794,
          13,
          51300
        ]
      },
      {
        "avg_logprob": -0.21864303588867187,
        "compression_ratio": 1.7147766323024054,
        "end": 1328.98,
        "id": 481,
        "no_speech_prob": 0.004006405360996723,
        "seek": 130922,
        "start": 1327.94,
        "temperature": 0,
        "text": " This is a big question.",
        "tokens": [
          51300,
          639,
          307,
          257,
          955,
          1168,
          13,
          51352
        ]
      },
      {
        "avg_logprob": -0.21864303588867187,
        "compression_ratio": 1.7147766323024054,
        "end": 1329.82,
        "id": 482,
        "no_speech_prob": 0.004006405360996723,
        "seek": 130922,
        "start": 1328.98,
        "temperature": 0,
        "text": " Okay, let's do it.",
        "tokens": [
          51352,
          1033,
          11,
          718,
          311,
          360,
          309,
          13,
          51394
        ]
      },
      {
        "avg_logprob": -0.21864303588867187,
        "compression_ratio": 1.7147766323024054,
        "end": 1330.94,
        "id": 483,
        "no_speech_prob": 0.004006405360996723,
        "seek": 130922,
        "start": 1329.82,
        "temperature": 0,
        "text": " I don't know if,",
        "tokens": [
          51394,
          286,
          500,
          380,
          458,
          498,
          11,
          51450
        ]
      },
      {
        "avg_logprob": -0.21864303588867187,
        "compression_ratio": 1.7147766323024054,
        "end": 1333.74,
        "id": 484,
        "no_speech_prob": 0.004006405360996723,
        "seek": 130922,
        "start": 1330.94,
        "temperature": 0,
        "text": " and I think this is a question I've certainly touched on,",
        "tokens": [
          51450,
          293,
          286,
          519,
          341,
          307,
          257,
          1168,
          286,
          600,
          3297,
          9828,
          322,
          11,
          51590
        ]
      },
      {
        "avg_logprob": -0.21864303588867187,
        "compression_ratio": 1.7147766323024054,
        "end": 1336.42,
        "id": 485,
        "no_speech_prob": 0.004006405360996723,
        "seek": 130922,
        "start": 1333.74,
        "temperature": 0,
        "text": " but how is AI,",
        "tokens": [
          51590,
          457,
          577,
          307,
          7318,
          11,
          51724
        ]
      },
      {
        "avg_logprob": -0.22643688226201733,
        "compression_ratio": 1.7962962962962963,
        "end": 1339.22,
        "id": 486,
        "no_speech_prob": 0.000503180839587003,
        "seek": 133642,
        "start": 1336.42,
        "temperature": 0,
        "text": " let me read the question how it actually is written.",
        "tokens": [
          50364,
          718,
          385,
          1401,
          264,
          1168,
          577,
          309,
          767,
          307,
          3720,
          13,
          50504
        ]
      },
      {
        "avg_logprob": -0.22643688226201733,
        "compression_ratio": 1.7962962962962963,
        "end": 1341.3000000000002,
        "id": 487,
        "no_speech_prob": 0.000503180839587003,
        "seek": 133642,
        "start": 1339.22,
        "temperature": 0,
        "text": " How is AI different from a neural network,",
        "tokens": [
          50504,
          1012,
          307,
          7318,
          819,
          490,
          257,
          18161,
          3209,
          11,
          50608
        ]
      },
      {
        "avg_logprob": -0.22643688226201733,
        "compression_ratio": 1.7962962962962963,
        "end": 1343.26,
        "id": 488,
        "no_speech_prob": 0.000503180839587003,
        "seek": 133642,
        "start": 1341.3000000000002,
        "temperature": 0,
        "text": " or deep learning, or machine learning?",
        "tokens": [
          50608,
          420,
          2452,
          2539,
          11,
          420,
          3479,
          2539,
          30,
          50706
        ]
      },
      {
        "avg_logprob": -0.22643688226201733,
        "compression_ratio": 1.7962962962962963,
        "end": 1345.02,
        "id": 489,
        "no_speech_prob": 0.000503180839587003,
        "seek": 133642,
        "start": 1343.26,
        "temperature": 0,
        "text": " They often seem to be used interchangeably",
        "tokens": [
          50706,
          814,
          2049,
          1643,
          281,
          312,
          1143,
          30358,
          1188,
          50794
        ]
      },
      {
        "avg_logprob": -0.22643688226201733,
        "compression_ratio": 1.7962962962962963,
        "end": 1345.8600000000001,
        "id": 490,
        "no_speech_prob": 0.000503180839587003,
        "seek": 133642,
        "start": 1345.02,
        "temperature": 0,
        "text": " and cause confusion.",
        "tokens": [
          50794,
          293,
          3082,
          15075,
          13,
          50836
        ]
      },
      {
        "avg_logprob": -0.22643688226201733,
        "compression_ratio": 1.7962962962962963,
        "end": 1346.8200000000002,
        "id": 491,
        "no_speech_prob": 0.000503180839587003,
        "seek": 133642,
        "start": 1345.8600000000001,
        "temperature": 0,
        "text": " So first of all, I want to say,",
        "tokens": [
          50836,
          407,
          700,
          295,
          439,
          11,
          286,
          528,
          281,
          584,
          11,
          50884
        ]
      },
      {
        "avg_logprob": -0.22643688226201733,
        "compression_ratio": 1.7962962962962963,
        "end": 1348.8600000000001,
        "id": 492,
        "no_speech_prob": 0.000503180839587003,
        "seek": 133642,
        "start": 1346.8200000000002,
        "temperature": 0,
        "text": " like this is a really good question,",
        "tokens": [
          50884,
          411,
          341,
          307,
          257,
          534,
          665,
          1168,
          11,
          50986
        ]
      },
      {
        "avg_logprob": -0.22643688226201733,
        "compression_ratio": 1.7962962962962963,
        "end": 1350.46,
        "id": 493,
        "no_speech_prob": 0.000503180839587003,
        "seek": 133642,
        "start": 1348.8600000000001,
        "temperature": 0,
        "text": " and I struggle with this question all the time,",
        "tokens": [
          50986,
          293,
          286,
          7799,
          365,
          341,
          1168,
          439,
          264,
          565,
          11,
          51066
        ]
      },
      {
        "avg_logprob": -0.22643688226201733,
        "compression_ratio": 1.7962962962962963,
        "end": 1352.18,
        "id": 494,
        "no_speech_prob": 0.000503180839587003,
        "seek": 133642,
        "start": 1350.46,
        "temperature": 0,
        "text": " because there's all these different terms.",
        "tokens": [
          51066,
          570,
          456,
          311,
          439,
          613,
          819,
          2115,
          13,
          51152
        ]
      },
      {
        "avg_logprob": -0.22643688226201733,
        "compression_ratio": 1.7962962962962963,
        "end": 1354.14,
        "id": 495,
        "no_speech_prob": 0.000503180839587003,
        "seek": 133642,
        "start": 1352.18,
        "temperature": 0,
        "text": " So let me list those terms.",
        "tokens": [
          51152,
          407,
          718,
          385,
          1329,
          729,
          2115,
          13,
          51250
        ]
      },
      {
        "avg_logprob": -0.22643688226201733,
        "compression_ratio": 1.7962962962962963,
        "end": 1357.42,
        "id": 496,
        "no_speech_prob": 0.000503180839587003,
        "seek": 133642,
        "start": 1354.14,
        "temperature": 0,
        "text": " Artificial intelligence, deep learning, machine learning.",
        "tokens": [
          51250,
          5735,
          10371,
          7599,
          11,
          2452,
          2539,
          11,
          3479,
          2539,
          13,
          51414
        ]
      },
      {
        "avg_logprob": -0.22643688226201733,
        "compression_ratio": 1.7962962962962963,
        "end": 1358.94,
        "id": 497,
        "no_speech_prob": 0.000503180839587003,
        "seek": 133642,
        "start": 1357.42,
        "temperature": 0,
        "text": " And then I might put neural network",
        "tokens": [
          51414,
          400,
          550,
          286,
          1062,
          829,
          18161,
          3209,
          51490
        ]
      },
      {
        "avg_logprob": -0.22643688226201733,
        "compression_ratio": 1.7962962962962963,
        "end": 1360.66,
        "id": 498,
        "no_speech_prob": 0.000503180839587003,
        "seek": 133642,
        "start": 1358.94,
        "temperature": 0,
        "text": " in a different category,",
        "tokens": [
          51490,
          294,
          257,
          819,
          7719,
          11,
          51576
        ]
      },
      {
        "avg_logprob": -0.22643688226201733,
        "compression_ratio": 1.7962962962962963,
        "end": 1362.22,
        "id": 499,
        "no_speech_prob": 0.000503180839587003,
        "seek": 133642,
        "start": 1360.66,
        "temperature": 0,
        "text": " but that's another term as well.",
        "tokens": [
          51576,
          457,
          300,
          311,
          1071,
          1433,
          382,
          731,
          13,
          51654
        ]
      },
      {
        "avg_logprob": -0.22643688226201733,
        "compression_ratio": 1.7962962962962963,
        "end": 1364.6200000000001,
        "id": 500,
        "no_speech_prob": 0.000503180839587003,
        "seek": 133642,
        "start": 1362.22,
        "temperature": 0,
        "text": " I don't know if you have a kind of like way",
        "tokens": [
          51654,
          286,
          500,
          380,
          458,
          498,
          291,
          362,
          257,
          733,
          295,
          411,
          636,
          51774
        ]
      },
      {
        "avg_logprob": -0.24163213353248159,
        "compression_ratio": 1.7945205479452055,
        "end": 1366.34,
        "id": 501,
        "no_speech_prob": 0.0022871391847729683,
        "seek": 136462,
        "start": 1364.62,
        "temperature": 0,
        "text": " that you describe these,",
        "tokens": [
          50364,
          300,
          291,
          6786,
          613,
          11,
          50450
        ]
      },
      {
        "avg_logprob": -0.24163213353248159,
        "compression_ratio": 1.7945205479452055,
        "end": 1367.26,
        "id": 502,
        "no_speech_prob": 0.0022871391847729683,
        "seek": 136462,
        "start": 1366.34,
        "temperature": 0,
        "text": " the terminology to people",
        "tokens": [
          50450,
          264,
          27575,
          281,
          561,
          50496
        ]
      },
      {
        "avg_logprob": -0.24163213353248159,
        "compression_ratio": 1.7945205479452055,
        "end": 1368.1,
        "id": 503,
        "no_speech_prob": 0.0022871391847729683,
        "seek": 136462,
        "start": 1367.26,
        "temperature": 0,
        "text": " when they ask this kind of question.",
        "tokens": [
          50496,
          562,
          436,
          1029,
          341,
          733,
          295,
          1168,
          13,
          50538
        ]
      },
      {
        "avg_logprob": -0.24163213353248159,
        "compression_ratio": 1.7945205479452055,
        "end": 1371.34,
        "id": 504,
        "no_speech_prob": 0.0022871391847729683,
        "seek": 136462,
        "start": 1368.1,
        "temperature": 0,
        "text": " Yeah, I like to start with English is difficult.",
        "tokens": [
          50538,
          865,
          11,
          286,
          411,
          281,
          722,
          365,
          3669,
          307,
          2252,
          13,
          50700
        ]
      },
      {
        "avg_logprob": -0.24163213353248159,
        "compression_ratio": 1.7945205479452055,
        "end": 1373.1799999999998,
        "id": 505,
        "no_speech_prob": 0.0022871391847729683,
        "seek": 136462,
        "start": 1371.34,
        "temperature": 0,
        "text": " I like to start there.",
        "tokens": [
          50700,
          286,
          411,
          281,
          722,
          456,
          13,
          50792
        ]
      },
      {
        "avg_logprob": -0.24163213353248159,
        "compression_ratio": 1.7945205479452055,
        "end": 1374.6999999999998,
        "id": 506,
        "no_speech_prob": 0.0022871391847729683,
        "seek": 136462,
        "start": 1373.1799999999998,
        "temperature": 0,
        "text": " But for the most part, AI is,",
        "tokens": [
          50792,
          583,
          337,
          264,
          881,
          644,
          11,
          7318,
          307,
          11,
          50868
        ]
      },
      {
        "avg_logprob": -0.24163213353248159,
        "compression_ratio": 1.7945205479452055,
        "end": 1377.2199999999998,
        "id": 507,
        "no_speech_prob": 0.0022871391847729683,
        "seek": 136462,
        "start": 1374.6999999999998,
        "temperature": 0,
        "text": " it's like a grab all for everything AI.",
        "tokens": [
          50868,
          309,
          311,
          411,
          257,
          4444,
          439,
          337,
          1203,
          7318,
          13,
          50994
        ]
      },
      {
        "avg_logprob": -0.24163213353248159,
        "compression_ratio": 1.7945205479452055,
        "end": 1379.4199999999998,
        "id": 508,
        "no_speech_prob": 0.0022871391847729683,
        "seek": 136462,
        "start": 1377.2199999999998,
        "temperature": 0,
        "text": " Like you can hard code AI.",
        "tokens": [
          50994,
          1743,
          291,
          393,
          1152,
          3089,
          7318,
          13,
          51104
        ]
      },
      {
        "avg_logprob": -0.24163213353248159,
        "compression_ratio": 1.7945205479452055,
        "end": 1381.6599999999999,
        "id": 509,
        "no_speech_prob": 0.0022871391847729683,
        "seek": 136462,
        "start": 1379.4199999999998,
        "temperature": 0,
        "text": " You don't have to use machine learning.",
        "tokens": [
          51104,
          509,
          500,
          380,
          362,
          281,
          764,
          3479,
          2539,
          13,
          51216
        ]
      },
      {
        "avg_logprob": -0.24163213353248159,
        "compression_ratio": 1.7945205479452055,
        "end": 1384.1,
        "id": 510,
        "no_speech_prob": 0.0022871391847729683,
        "seek": 136462,
        "start": 1381.6599999999999,
        "temperature": 0,
        "text": " So that's kind of like the grab all for it all.",
        "tokens": [
          51216,
          407,
          300,
          311,
          733,
          295,
          411,
          264,
          4444,
          439,
          337,
          309,
          439,
          13,
          51338
        ]
      },
      {
        "avg_logprob": -0.24163213353248159,
        "compression_ratio": 1.7945205479452055,
        "end": 1385.1399999999999,
        "id": 511,
        "no_speech_prob": 0.0022871391847729683,
        "seek": 136462,
        "start": 1384.1,
        "temperature": 0,
        "text": " And then what was the other,",
        "tokens": [
          51338,
          400,
          550,
          437,
          390,
          264,
          661,
          11,
          51390
        ]
      },
      {
        "avg_logprob": -0.24163213353248159,
        "compression_ratio": 1.7945205479452055,
        "end": 1386.2199999999998,
        "id": 512,
        "no_speech_prob": 0.0022871391847729683,
        "seek": 136462,
        "start": 1385.1399999999999,
        "temperature": 0,
        "text": " the code words for it?",
        "tokens": [
          51390,
          264,
          3089,
          2283,
          337,
          309,
          30,
          51444
        ]
      },
      {
        "avg_logprob": -0.24163213353248159,
        "compression_ratio": 1.7945205479452055,
        "end": 1388.1399999999999,
        "id": 513,
        "no_speech_prob": 0.0022871391847729683,
        "seek": 136462,
        "start": 1386.2199999999998,
        "temperature": 0,
        "text": " So, oh boy, all these things are coming in.",
        "tokens": [
          51444,
          407,
          11,
          1954,
          3237,
          11,
          439,
          613,
          721,
          366,
          1348,
          294,
          13,
          51540
        ]
      },
      {
        "avg_logprob": -0.24163213353248159,
        "compression_ratio": 1.7945205479452055,
        "end": 1391.58,
        "id": 514,
        "no_speech_prob": 0.0022871391847729683,
        "seek": 136462,
        "start": 1388.1399999999999,
        "temperature": 0,
        "text": " AI, machine learning, deep learning, neural network.",
        "tokens": [
          51540,
          7318,
          11,
          3479,
          2539,
          11,
          2452,
          2539,
          11,
          18161,
          3209,
          13,
          51712
        ]
      },
      {
        "avg_logprob": -0.24163213353248159,
        "compression_ratio": 1.7945205479452055,
        "end": 1393.1399999999999,
        "id": 515,
        "no_speech_prob": 0.0022871391847729683,
        "seek": 136462,
        "start": 1391.58,
        "temperature": 0,
        "text": " Yeah, and so machine learning,",
        "tokens": [
          51712,
          865,
          11,
          293,
          370,
          3479,
          2539,
          11,
          51790
        ]
      },
      {
        "avg_logprob": -0.20296980983527133,
        "compression_ratio": 1.8228782287822878,
        "end": 1395.74,
        "id": 516,
        "no_speech_prob": 0.0008691875846125185,
        "seek": 139314,
        "start": 1393.14,
        "temperature": 0,
        "text": " I think it's like the next level down.",
        "tokens": [
          50364,
          286,
          519,
          309,
          311,
          411,
          264,
          958,
          1496,
          760,
          13,
          50494
        ]
      },
      {
        "avg_logprob": -0.20296980983527133,
        "compression_ratio": 1.8228782287822878,
        "end": 1398.6200000000001,
        "id": 517,
        "no_speech_prob": 0.0008691875846125185,
        "seek": 139314,
        "start": 1395.74,
        "temperature": 0,
        "text": " So you don't need to use a neural network",
        "tokens": [
          50494,
          407,
          291,
          500,
          380,
          643,
          281,
          764,
          257,
          18161,
          3209,
          50638
        ]
      },
      {
        "avg_logprob": -0.20296980983527133,
        "compression_ratio": 1.8228782287822878,
        "end": 1399.66,
        "id": 518,
        "no_speech_prob": 0.0008691875846125185,
        "seek": 139314,
        "start": 1398.6200000000001,
        "temperature": 0,
        "text": " to do machine learning.",
        "tokens": [
          50638,
          281,
          360,
          3479,
          2539,
          13,
          50690
        ]
      },
      {
        "avg_logprob": -0.20296980983527133,
        "compression_ratio": 1.8228782287822878,
        "end": 1402.0600000000002,
        "id": 519,
        "no_speech_prob": 0.0008691875846125185,
        "seek": 139314,
        "start": 1399.66,
        "temperature": 0,
        "text": " There are different ways that you can go about",
        "tokens": [
          50690,
          821,
          366,
          819,
          2098,
          300,
          291,
          393,
          352,
          466,
          50810
        ]
      },
      {
        "avg_logprob": -0.20296980983527133,
        "compression_ratio": 1.8228782287822878,
        "end": 1403.5,
        "id": 520,
        "no_speech_prob": 0.0008691875846125185,
        "seek": 139314,
        "start": 1402.0600000000002,
        "temperature": 0,
        "text": " teaching a machine how to learn.",
        "tokens": [
          50810,
          4571,
          257,
          3479,
          577,
          281,
          1466,
          13,
          50882
        ]
      },
      {
        "avg_logprob": -0.20296980983527133,
        "compression_ratio": 1.8228782287822878,
        "end": 1406.46,
        "id": 521,
        "no_speech_prob": 0.0008691875846125185,
        "seek": 139314,
        "start": 1403.5,
        "temperature": 0,
        "text": " One really good example is decision trees.",
        "tokens": [
          50882,
          1485,
          534,
          665,
          1365,
          307,
          3537,
          5852,
          13,
          51030
        ]
      },
      {
        "avg_logprob": -0.20296980983527133,
        "compression_ratio": 1.8228782287822878,
        "end": 1408.66,
        "id": 522,
        "no_speech_prob": 0.0008691875846125185,
        "seek": 139314,
        "start": 1406.46,
        "temperature": 0,
        "text": " People have developed really complex decision trees",
        "tokens": [
          51030,
          3432,
          362,
          4743,
          534,
          3997,
          3537,
          5852,
          51140
        ]
      },
      {
        "avg_logprob": -0.20296980983527133,
        "compression_ratio": 1.8228782287822878,
        "end": 1411.5,
        "id": 523,
        "no_speech_prob": 0.0008691875846125185,
        "seek": 139314,
        "start": 1408.66,
        "temperature": 0,
        "text": " and the machine just kind of explores the space",
        "tokens": [
          51140,
          293,
          264,
          3479,
          445,
          733,
          295,
          45473,
          264,
          1901,
          51282
        ]
      },
      {
        "avg_logprob": -0.20296980983527133,
        "compression_ratio": 1.8228782287822878,
        "end": 1414.46,
        "id": 524,
        "no_speech_prob": 0.0008691875846125185,
        "seek": 139314,
        "start": 1411.5,
        "temperature": 0,
        "text": " and learns the best way to use this decision tree.",
        "tokens": [
          51282,
          293,
          27152,
          264,
          1151,
          636,
          281,
          764,
          341,
          3537,
          4230,
          13,
          51430
        ]
      },
      {
        "avg_logprob": -0.20296980983527133,
        "compression_ratio": 1.8228782287822878,
        "end": 1416.74,
        "id": 525,
        "no_speech_prob": 0.0008691875846125185,
        "seek": 139314,
        "start": 1414.46,
        "temperature": 0,
        "text": " So that's another way of applying machine learning.",
        "tokens": [
          51430,
          407,
          300,
          311,
          1071,
          636,
          295,
          9275,
          3479,
          2539,
          13,
          51544
        ]
      },
      {
        "avg_logprob": -0.20296980983527133,
        "compression_ratio": 1.8228782287822878,
        "end": 1418.5800000000002,
        "id": 526,
        "no_speech_prob": 0.0008691875846125185,
        "seek": 139314,
        "start": 1416.74,
        "temperature": 0,
        "text": " And then what'd you say,",
        "tokens": [
          51544,
          400,
          550,
          437,
          1116,
          291,
          584,
          11,
          51636
        ]
      },
      {
        "avg_logprob": -0.20296980983527133,
        "compression_ratio": 1.8228782287822878,
        "end": 1419.74,
        "id": 527,
        "no_speech_prob": 0.0008691875846125185,
        "seek": 139314,
        "start": 1418.5800000000002,
        "temperature": 0,
        "text": " neural network and deep learning?",
        "tokens": [
          51636,
          18161,
          3209,
          293,
          2452,
          2539,
          30,
          51694
        ]
      },
      {
        "avg_logprob": -0.20296980983527133,
        "compression_ratio": 1.8228782287822878,
        "end": 1420.5800000000002,
        "id": 528,
        "no_speech_prob": 0.0008691875846125185,
        "seek": 139314,
        "start": 1419.74,
        "temperature": 0,
        "text": " Yes.",
        "tokens": [
          51694,
          1079,
          13,
          51736
        ]
      },
      {
        "avg_logprob": -0.26822569238857963,
        "compression_ratio": 1.7605042016806722,
        "end": 1423.1799999999998,
        "id": 529,
        "no_speech_prob": 0.0015487181954085827,
        "seek": 142058,
        "start": 1420.58,
        "temperature": 0,
        "text": " So neural networks is essentially what we showed.",
        "tokens": [
          50364,
          407,
          18161,
          9590,
          307,
          4476,
          437,
          321,
          4712,
          13,
          50494
        ]
      },
      {
        "avg_logprob": -0.26822569238857963,
        "compression_ratio": 1.7605042016806722,
        "end": 1427.54,
        "id": 530,
        "no_speech_prob": 0.0015487181954085827,
        "seek": 142058,
        "start": 1424.54,
        "temperature": 0,
        "text": " And even that's kind of up to debate",
        "tokens": [
          50562,
          400,
          754,
          300,
          311,
          733,
          295,
          493,
          281,
          7958,
          50712
        ]
      },
      {
        "avg_logprob": -0.26822569238857963,
        "compression_ratio": 1.7605042016806722,
        "end": 1429.26,
        "id": 531,
        "no_speech_prob": 0.0015487181954085827,
        "seek": 142058,
        "start": 1427.54,
        "temperature": 0,
        "text": " because like recurrent neural networks",
        "tokens": [
          50712,
          570,
          411,
          18680,
          1753,
          18161,
          9590,
          50798
        ]
      },
      {
        "avg_logprob": -0.26822569238857963,
        "compression_ratio": 1.7605042016806722,
        "end": 1430.46,
        "id": 532,
        "no_speech_prob": 0.0015487181954085827,
        "seek": 142058,
        "start": 1429.26,
        "temperature": 0,
        "text": " are neural networks,",
        "tokens": [
          50798,
          366,
          18161,
          9590,
          11,
          50858
        ]
      },
      {
        "avg_logprob": -0.26822569238857963,
        "compression_ratio": 1.7605042016806722,
        "end": 1435.46,
        "id": 533,
        "no_speech_prob": 0.0015487181954085827,
        "seek": 142058,
        "start": 1430.46,
        "temperature": 0,
        "text": " but they're not really, you know, neural networked,",
        "tokens": [
          50858,
          457,
          436,
          434,
          406,
          534,
          11,
          291,
          458,
          11,
          18161,
          3209,
          292,
          11,
          51108
        ]
      },
      {
        "avg_logprob": -0.26822569238857963,
        "compression_ratio": 1.7605042016806722,
        "end": 1436.78,
        "id": 534,
        "no_speech_prob": 0.0015487181954085827,
        "seek": 142058,
        "start": 1435.62,
        "temperature": 0,
        "text": " you know, if that makes sense.",
        "tokens": [
          51116,
          291,
          458,
          11,
          498,
          300,
          1669,
          2020,
          13,
          51174
        ]
      },
      {
        "avg_logprob": -0.26822569238857963,
        "compression_ratio": 1.7605042016806722,
        "end": 1438.22,
        "id": 535,
        "no_speech_prob": 0.0015487181954085827,
        "seek": 142058,
        "start": 1436.78,
        "temperature": 0,
        "text": " So I don't know.",
        "tokens": [
          51174,
          407,
          286,
          500,
          380,
          458,
          13,
          51246
        ]
      },
      {
        "avg_logprob": -0.26822569238857963,
        "compression_ratio": 1.7605042016806722,
        "end": 1440.82,
        "id": 536,
        "no_speech_prob": 0.0015487181954085827,
        "seek": 142058,
        "start": 1439.62,
        "temperature": 0,
        "text": " If you say neural network,",
        "tokens": [
          51316,
          759,
          291,
          584,
          18161,
          3209,
          11,
          51376
        ]
      },
      {
        "avg_logprob": -0.26822569238857963,
        "compression_ratio": 1.7605042016806722,
        "end": 1443.1799999999998,
        "id": 537,
        "no_speech_prob": 0.0015487181954085827,
        "seek": 142058,
        "start": 1440.82,
        "temperature": 0,
        "text": " people know what you're talking about for the most part.",
        "tokens": [
          51376,
          561,
          458,
          437,
          291,
          434,
          1417,
          466,
          337,
          264,
          881,
          644,
          13,
          51494
        ]
      },
      {
        "avg_logprob": -0.26822569238857963,
        "compression_ratio": 1.7605042016806722,
        "end": 1445.82,
        "id": 538,
        "no_speech_prob": 0.0015487181954085827,
        "seek": 142058,
        "start": 1443.1799999999998,
        "temperature": 0,
        "text": " All right, I'm gonna try to give my take on this.",
        "tokens": [
          51494,
          1057,
          558,
          11,
          286,
          478,
          799,
          853,
          281,
          976,
          452,
          747,
          322,
          341,
          13,
          51626
        ]
      },
      {
        "avg_logprob": -0.26822569238857963,
        "compression_ratio": 1.7605042016806722,
        "end": 1446.6599999999999,
        "id": 539,
        "no_speech_prob": 0.0015487181954085827,
        "seek": 142058,
        "start": 1445.82,
        "temperature": 0,
        "text": " Let's see if I can do it.",
        "tokens": [
          51626,
          961,
          311,
          536,
          498,
          286,
          393,
          360,
          309,
          13,
          51668
        ]
      },
      {
        "avg_logprob": -0.26822569238857963,
        "compression_ratio": 1.7605042016806722,
        "end": 1447.5,
        "id": 540,
        "no_speech_prob": 0.0015487181954085827,
        "seek": 142058,
        "start": 1446.6599999999999,
        "temperature": 0,
        "text": " Let's do it.",
        "tokens": [
          51668,
          961,
          311,
          360,
          309,
          13,
          51710
        ]
      },
      {
        "avg_logprob": -0.23165618248705594,
        "compression_ratio": 1.7951807228915662,
        "end": 1452.5,
        "id": 541,
        "no_speech_prob": 0.00000966598054219503,
        "seek": 144750,
        "start": 1447.5,
        "temperature": 0,
        "text": " So neural network to me is a particular algorithm",
        "tokens": [
          50364,
          407,
          18161,
          3209,
          281,
          385,
          307,
          257,
          1729,
          9284,
          50614
        ]
      },
      {
        "avg_logprob": -0.23165618248705594,
        "compression_ratio": 1.7951807228915662,
        "end": 1456.98,
        "id": 542,
        "no_speech_prob": 0.00000966598054219503,
        "seek": 144750,
        "start": 1453.42,
        "temperature": 0,
        "text": " that involves connected nodes",
        "tokens": [
          50660,
          300,
          11626,
          4582,
          13891,
          50838
        ]
      },
      {
        "avg_logprob": -0.23165618248705594,
        "compression_ratio": 1.7951807228915662,
        "end": 1459.38,
        "id": 543,
        "no_speech_prob": 0.00000966598054219503,
        "seek": 144750,
        "start": 1456.98,
        "temperature": 0,
        "text": " and data flows from one node to the other.",
        "tokens": [
          50838,
          293,
          1412,
          12867,
          490,
          472,
          9984,
          281,
          264,
          661,
          13,
          50958
        ]
      },
      {
        "avg_logprob": -0.23165618248705594,
        "compression_ratio": 1.7951807228915662,
        "end": 1461.14,
        "id": 544,
        "no_speech_prob": 0.00000966598054219503,
        "seek": 144750,
        "start": 1459.38,
        "temperature": 0,
        "text": " There could be different architectures and styles.",
        "tokens": [
          50958,
          821,
          727,
          312,
          819,
          6331,
          1303,
          293,
          13273,
          13,
          51046
        ]
      },
      {
        "avg_logprob": -0.23165618248705594,
        "compression_ratio": 1.7951807228915662,
        "end": 1465.06,
        "id": 545,
        "no_speech_prob": 0.00000966598054219503,
        "seek": 144750,
        "start": 1461.14,
        "temperature": 0,
        "text": " And so that neural network data structure algorithm",
        "tokens": [
          51046,
          400,
          370,
          300,
          18161,
          3209,
          1412,
          3877,
          9284,
          51242
        ]
      },
      {
        "avg_logprob": -0.23165618248705594,
        "compression_ratio": 1.7951807228915662,
        "end": 1469.3,
        "id": 546,
        "no_speech_prob": 0.00000966598054219503,
        "seek": 144750,
        "start": 1465.06,
        "temperature": 0,
        "text": " can be applied in the fields of artificial intelligence,",
        "tokens": [
          51242,
          393,
          312,
          6456,
          294,
          264,
          7909,
          295,
          11677,
          7599,
          11,
          51454
        ]
      },
      {
        "avg_logprob": -0.23165618248705594,
        "compression_ratio": 1.7951807228915662,
        "end": 1470.3,
        "id": 547,
        "no_speech_prob": 0.00000966598054219503,
        "seek": 144750,
        "start": 1469.3,
        "temperature": 0,
        "text": " machine learning and deep learning.",
        "tokens": [
          51454,
          3479,
          2539,
          293,
          2452,
          2539,
          13,
          51504
        ]
      },
      {
        "avg_logprob": -0.23165618248705594,
        "compression_ratio": 1.7951807228915662,
        "end": 1471.78,
        "id": 548,
        "no_speech_prob": 0.00000966598054219503,
        "seek": 144750,
        "start": 1470.3,
        "temperature": 0,
        "text": " But neural network is an example",
        "tokens": [
          51504,
          583,
          18161,
          3209,
          307,
          364,
          1365,
          51578
        ]
      },
      {
        "avg_logprob": -0.23165618248705594,
        "compression_ratio": 1.7951807228915662,
        "end": 1474.78,
        "id": 549,
        "no_speech_prob": 0.00000966598054219503,
        "seek": 144750,
        "start": 1471.78,
        "temperature": 0,
        "text": " of a particular algorithm, I would say.",
        "tokens": [
          51578,
          295,
          257,
          1729,
          9284,
          11,
          286,
          576,
          584,
          13,
          51728
        ]
      },
      {
        "avg_logprob": -0.23165618248705594,
        "compression_ratio": 1.7951807228915662,
        "end": 1476.9,
        "id": 550,
        "no_speech_prob": 0.00000966598054219503,
        "seek": 144750,
        "start": 1474.78,
        "temperature": 0,
        "text": " You could sort of think of it also as a data structure,",
        "tokens": [
          51728,
          509,
          727,
          1333,
          295,
          519,
          295,
          309,
          611,
          382,
          257,
          1412,
          3877,
          11,
          51834
        ]
      },
      {
        "avg_logprob": -0.21690504286024306,
        "compression_ratio": 1.8736462093862816,
        "end": 1477.98,
        "id": 551,
        "no_speech_prob": 0.00007966504927026108,
        "seek": 147690,
        "start": 1476.9,
        "temperature": 0,
        "text": " but there's an algorithm there",
        "tokens": [
          50364,
          457,
          456,
          311,
          364,
          9284,
          456,
          50418
        ]
      },
      {
        "avg_logprob": -0.21690504286024306,
        "compression_ratio": 1.8736462093862816,
        "end": 1481.3400000000001,
        "id": 552,
        "no_speech_prob": 0.00007966504927026108,
        "seek": 147690,
        "start": 1477.98,
        "temperature": 0,
        "text": " in terms of how the data flows through the structure.",
        "tokens": [
          50418,
          294,
          2115,
          295,
          577,
          264,
          1412,
          12867,
          807,
          264,
          3877,
          13,
          50586
        ]
      },
      {
        "avg_logprob": -0.21690504286024306,
        "compression_ratio": 1.8736462093862816,
        "end": 1482.94,
        "id": 553,
        "no_speech_prob": 0.00007966504927026108,
        "seek": 147690,
        "start": 1481.3400000000001,
        "temperature": 0,
        "text": " So that's what I think.",
        "tokens": [
          50586,
          407,
          300,
          311,
          437,
          286,
          519,
          13,
          50666
        ]
      },
      {
        "avg_logprob": -0.21690504286024306,
        "compression_ratio": 1.8736462093862816,
        "end": 1486.98,
        "id": 554,
        "no_speech_prob": 0.00007966504927026108,
        "seek": 147690,
        "start": 1482.94,
        "temperature": 0,
        "text": " And then I think that, to me, artificial intelligence,",
        "tokens": [
          50666,
          400,
          550,
          286,
          519,
          300,
          11,
          281,
          385,
          11,
          11677,
          7599,
          11,
          50868
        ]
      },
      {
        "avg_logprob": -0.21690504286024306,
        "compression_ratio": 1.8736462093862816,
        "end": 1488.7,
        "id": 555,
        "no_speech_prob": 0.00007966504927026108,
        "seek": 147690,
        "start": 1486.98,
        "temperature": 0,
        "text": " I think of that as a very broad umbrella term",
        "tokens": [
          50868,
          286,
          519,
          295,
          300,
          382,
          257,
          588,
          4152,
          21925,
          1433,
          50954
        ]
      },
      {
        "avg_logprob": -0.21690504286024306,
        "compression_ratio": 1.8736462093862816,
        "end": 1491.46,
        "id": 556,
        "no_speech_prob": 0.00007966504927026108,
        "seek": 147690,
        "start": 1488.7,
        "temperature": 0,
        "text": " to just the big field of like simulated intelligence.",
        "tokens": [
          50954,
          281,
          445,
          264,
          955,
          2519,
          295,
          411,
          41713,
          7599,
          13,
          51092
        ]
      },
      {
        "avg_logprob": -0.21690504286024306,
        "compression_ratio": 1.8736462093862816,
        "end": 1493.42,
        "id": 557,
        "no_speech_prob": 0.00007966504927026108,
        "seek": 147690,
        "start": 1491.46,
        "temperature": 0,
        "text": " Whether is that, is it real intelligence?",
        "tokens": [
          51092,
          8503,
          307,
          300,
          11,
          307,
          309,
          957,
          7599,
          30,
          51190
        ]
      },
      {
        "avg_logprob": -0.21690504286024306,
        "compression_ratio": 1.8736462093862816,
        "end": 1494.6200000000001,
        "id": 558,
        "no_speech_prob": 0.00007966504927026108,
        "seek": 147690,
        "start": 1493.42,
        "temperature": 0,
        "text": " Is it the illusion of intelligence?",
        "tokens": [
          51190,
          1119,
          309,
          264,
          18854,
          295,
          7599,
          30,
          51250
        ]
      },
      {
        "avg_logprob": -0.21690504286024306,
        "compression_ratio": 1.8736462093862816,
        "end": 1495.46,
        "id": 559,
        "no_speech_prob": 0.00007966504927026108,
        "seek": 147690,
        "start": 1494.6200000000001,
        "temperature": 0,
        "text": " Is that the same thing?",
        "tokens": [
          51250,
          1119,
          300,
          264,
          912,
          551,
          30,
          51292
        ]
      },
      {
        "avg_logprob": -0.21690504286024306,
        "compression_ratio": 1.8736462093862816,
        "end": 1498.5,
        "id": 560,
        "no_speech_prob": 0.00007966504927026108,
        "seek": 147690,
        "start": 1495.46,
        "temperature": 0,
        "text": " It's like kind of a deep philosophical question.",
        "tokens": [
          51292,
          467,
          311,
          411,
          733,
          295,
          257,
          2452,
          25066,
          1168,
          13,
          51444
        ]
      },
      {
        "avg_logprob": -0.21690504286024306,
        "compression_ratio": 1.8736462093862816,
        "end": 1500.7,
        "id": 561,
        "no_speech_prob": 0.00007966504927026108,
        "seek": 147690,
        "start": 1498.5,
        "temperature": 0,
        "text": " And then machine learning to me",
        "tokens": [
          51444,
          400,
          550,
          3479,
          2539,
          281,
          385,
          51554
        ]
      },
      {
        "avg_logprob": -0.21690504286024306,
        "compression_ratio": 1.8736462093862816,
        "end": 1503.5,
        "id": 562,
        "no_speech_prob": 0.00007966504927026108,
        "seek": 147690,
        "start": 1500.7,
        "temperature": 0,
        "text": " is a subfield of artificial intelligence",
        "tokens": [
          51554,
          307,
          257,
          1422,
          7610,
          295,
          11677,
          7599,
          51694
        ]
      },
      {
        "avg_logprob": -0.21690504286024306,
        "compression_ratio": 1.8736462093862816,
        "end": 1505.8200000000002,
        "id": 563,
        "no_speech_prob": 0.00007966504927026108,
        "seek": 147690,
        "start": 1503.5,
        "temperature": 0,
        "text": " involving making sense of data.",
        "tokens": [
          51694,
          17030,
          1455,
          2020,
          295,
          1412,
          13,
          51810
        ]
      },
      {
        "avg_logprob": -0.2411637950587917,
        "compression_ratio": 1.7962962962962963,
        "end": 1508.86,
        "id": 564,
        "no_speech_prob": 0.000019525099560269155,
        "seek": 150582,
        "start": 1505.82,
        "temperature": 0,
        "text": " So you have data and that's input to a system",
        "tokens": [
          50364,
          407,
          291,
          362,
          1412,
          293,
          300,
          311,
          4846,
          281,
          257,
          1185,
          50516
        ]
      },
      {
        "avg_logprob": -0.2411637950587917,
        "compression_ratio": 1.7962962962962963,
        "end": 1511.06,
        "id": 565,
        "no_speech_prob": 0.000019525099560269155,
        "seek": 150582,
        "start": 1508.86,
        "temperature": 0,
        "text": " and you have some output which might be making sense",
        "tokens": [
          50516,
          293,
          291,
          362,
          512,
          5598,
          597,
          1062,
          312,
          1455,
          2020,
          50626
        ]
      },
      {
        "avg_logprob": -0.2411637950587917,
        "compression_ratio": 1.7962962962962963,
        "end": 1512.74,
        "id": 566,
        "no_speech_prob": 0.000019525099560269155,
        "seek": 150582,
        "start": 1511.06,
        "temperature": 0,
        "text": " of that data, whether it's a prediction",
        "tokens": [
          50626,
          295,
          300,
          1412,
          11,
          1968,
          309,
          311,
          257,
          17630,
          50710
        ]
      },
      {
        "avg_logprob": -0.2411637950587917,
        "compression_ratio": 1.7962962962962963,
        "end": 1514.1799999999998,
        "id": 567,
        "no_speech_prob": 0.000019525099560269155,
        "seek": 150582,
        "start": 1512.74,
        "temperature": 0,
        "text": " for something that's gonna happen in the future",
        "tokens": [
          50710,
          337,
          746,
          300,
          311,
          799,
          1051,
          294,
          264,
          2027,
          50782
        ]
      },
      {
        "avg_logprob": -0.2411637950587917,
        "compression_ratio": 1.7962962962962963,
        "end": 1515.8999999999999,
        "id": 568,
        "no_speech_prob": 0.000019525099560269155,
        "seek": 150582,
        "start": 1514.1799999999998,
        "temperature": 0,
        "text": " or a classification of something.",
        "tokens": [
          50782,
          420,
          257,
          21538,
          295,
          746,
          13,
          50868
        ]
      },
      {
        "avg_logprob": -0.2411637950587917,
        "compression_ratio": 1.7962962962962963,
        "end": 1519.3799999999999,
        "id": 569,
        "no_speech_prob": 0.000019525099560269155,
        "seek": 150582,
        "start": 1515.8999999999999,
        "temperature": 0,
        "text": " And then I think of deep learning",
        "tokens": [
          50868,
          400,
          550,
          286,
          519,
          295,
          2452,
          2539,
          51042
        ]
      },
      {
        "avg_logprob": -0.2411637950587917,
        "compression_ratio": 1.7962962962962963,
        "end": 1523.3799999999999,
        "id": 570,
        "no_speech_prob": 0.000019525099560269155,
        "seek": 150582,
        "start": 1519.3799999999999,
        "temperature": 0,
        "text": " as a kind of almost like a modern rebranding",
        "tokens": [
          51042,
          382,
          257,
          733,
          295,
          1920,
          411,
          257,
          4363,
          12970,
          3699,
          278,
          51242
        ]
      },
      {
        "avg_logprob": -0.2411637950587917,
        "compression_ratio": 1.7962962962962963,
        "end": 1525.34,
        "id": 571,
        "no_speech_prob": 0.000019525099560269155,
        "seek": 150582,
        "start": 1523.3799999999999,
        "temperature": 0,
        "text": " of machine learning with neural networks.",
        "tokens": [
          51242,
          295,
          3479,
          2539,
          365,
          18161,
          9590,
          13,
          51340
        ]
      },
      {
        "avg_logprob": -0.2411637950587917,
        "compression_ratio": 1.7962962962962963,
        "end": 1527.62,
        "id": 572,
        "no_speech_prob": 0.000019525099560269155,
        "seek": 150582,
        "start": 1525.34,
        "temperature": 0,
        "text": " It's like, hey, we have bigger data sets now",
        "tokens": [
          51340,
          467,
          311,
          411,
          11,
          4177,
          11,
          321,
          362,
          3801,
          1412,
          6352,
          586,
          51454
        ]
      },
      {
        "avg_logprob": -0.2411637950587917,
        "compression_ratio": 1.7962962962962963,
        "end": 1529.1399999999999,
        "id": 573,
        "no_speech_prob": 0.000019525099560269155,
        "seek": 150582,
        "start": 1527.62,
        "temperature": 0,
        "text": " and faster computers now.",
        "tokens": [
          51454,
          293,
          4663,
          10807,
          586,
          13,
          51530
        ]
      },
      {
        "avg_logprob": -0.2411637950587917,
        "compression_ratio": 1.7962962962962963,
        "end": 1531.26,
        "id": 574,
        "no_speech_prob": 0.000019525099560269155,
        "seek": 150582,
        "start": 1529.1399999999999,
        "temperature": 0,
        "text": " All of a sudden, the things that people researched",
        "tokens": [
          51530,
          1057,
          295,
          257,
          3990,
          11,
          264,
          721,
          300,
          561,
          37098,
          51636
        ]
      },
      {
        "avg_logprob": -0.2411637950587917,
        "compression_ratio": 1.7962962962962963,
        "end": 1532.9399999999998,
        "id": 575,
        "no_speech_prob": 0.000019525099560269155,
        "seek": 150582,
        "start": 1531.26,
        "temperature": 0,
        "text": " many years ago called neural networks",
        "tokens": [
          51636,
          867,
          924,
          2057,
          1219,
          18161,
          9590,
          51720
        ]
      },
      {
        "avg_logprob": -0.2411637950587917,
        "compression_ratio": 1.7962962962962963,
        "end": 1534.3,
        "id": 576,
        "no_speech_prob": 0.000019525099560269155,
        "seek": 150582,
        "start": 1532.9399999999998,
        "temperature": 0,
        "text": " that nobody thought could really do anything",
        "tokens": [
          51720,
          300,
          5079,
          1194,
          727,
          534,
          360,
          1340,
          51788
        ]
      },
      {
        "avg_logprob": -0.2411637950587917,
        "compression_ratio": 1.7962962962962963,
        "end": 1535.78,
        "id": 577,
        "no_speech_prob": 0.000019525099560269155,
        "seek": 150582,
        "start": 1534.3,
        "temperature": 0,
        "text": " or they thought could but couldn't,",
        "tokens": [
          51788,
          420,
          436,
          1194,
          727,
          457,
          2809,
          380,
          11,
          51862
        ]
      },
      {
        "avg_logprob": -0.27813674619533874,
        "compression_ratio": 1.7647058823529411,
        "end": 1538.62,
        "id": 578,
        "no_speech_prob": 0.00008219832670874894,
        "seek": 153578,
        "start": 1536.62,
        "temperature": 0,
        "text": " now all of a sudden, we can do more of them with.",
        "tokens": [
          50406,
          586,
          439,
          295,
          257,
          3990,
          11,
          321,
          393,
          360,
          544,
          295,
          552,
          365,
          13,
          50506
        ]
      },
      {
        "avg_logprob": -0.27813674619533874,
        "compression_ratio": 1.7647058823529411,
        "end": 1540.8999999999999,
        "id": 579,
        "no_speech_prob": 0.00008219832670874894,
        "seek": 153578,
        "start": 1538.62,
        "temperature": 0,
        "text": " And so it's really just like,",
        "tokens": [
          50506,
          400,
          370,
          309,
          311,
          534,
          445,
          411,
          11,
          50620
        ]
      },
      {
        "avg_logprob": -0.27813674619533874,
        "compression_ratio": 1.7647058823529411,
        "end": 1542.3799999999999,
        "id": 580,
        "no_speech_prob": 0.00008219832670874894,
        "seek": 153578,
        "start": 1540.8999999999999,
        "temperature": 0,
        "text": " I mean, it's not meant to be marketing,",
        "tokens": [
          50620,
          286,
          914,
          11,
          309,
          311,
          406,
          4140,
          281,
          312,
          6370,
          11,
          50694
        ]
      },
      {
        "avg_logprob": -0.27813674619533874,
        "compression_ratio": 1.7647058823529411,
        "end": 1544.18,
        "id": 581,
        "no_speech_prob": 0.00008219832670874894,
        "seek": 153578,
        "start": 1542.3799999999999,
        "temperature": 0,
        "text": " but it's kind of like marketing this idea",
        "tokens": [
          50694,
          457,
          309,
          311,
          733,
          295,
          411,
          6370,
          341,
          1558,
          50784
        ]
      },
      {
        "avg_logprob": -0.27813674619533874,
        "compression_ratio": 1.7647058823529411,
        "end": 1546.74,
        "id": 582,
        "no_speech_prob": 0.00008219832670874894,
        "seek": 153578,
        "start": 1544.18,
        "temperature": 0,
        "text": " of big data, neural networks.",
        "tokens": [
          50784,
          295,
          955,
          1412,
          11,
          18161,
          9590,
          13,
          50912
        ]
      },
      {
        "avg_logprob": -0.27813674619533874,
        "compression_ratio": 1.7647058823529411,
        "end": 1547.98,
        "id": 583,
        "no_speech_prob": 0.00008219832670874894,
        "seek": 153578,
        "start": 1546.74,
        "temperature": 0,
        "text": " Yeah, I agree.",
        "tokens": [
          50912,
          865,
          11,
          286,
          3986,
          13,
          50974
        ]
      },
      {
        "avg_logprob": -0.27813674619533874,
        "compression_ratio": 1.7647058823529411,
        "end": 1550.54,
        "id": 584,
        "no_speech_prob": 0.00008219832670874894,
        "seek": 153578,
        "start": 1547.98,
        "temperature": 0,
        "text": " I think it's a lot of the marketing side of things",
        "tokens": [
          50974,
          286,
          519,
          309,
          311,
          257,
          688,
          295,
          264,
          6370,
          1252,
          295,
          721,
          51102
        ]
      },
      {
        "avg_logprob": -0.27813674619533874,
        "compression_ratio": 1.7647058823529411,
        "end": 1552.26,
        "id": 585,
        "no_speech_prob": 0.00008219832670874894,
        "seek": 153578,
        "start": 1550.54,
        "temperature": 0,
        "text": " because you'll read so many different posts",
        "tokens": [
          51102,
          570,
          291,
          603,
          1401,
          370,
          867,
          819,
          12300,
          51188
        ]
      },
      {
        "avg_logprob": -0.27813674619533874,
        "compression_ratio": 1.7647058823529411,
        "end": 1553.7,
        "id": 586,
        "no_speech_prob": 0.00008219832670874894,
        "seek": 153578,
        "start": 1552.26,
        "temperature": 0,
        "text": " like what's the difference between deep learning",
        "tokens": [
          51188,
          411,
          437,
          311,
          264,
          2649,
          1296,
          2452,
          2539,
          51260
        ]
      },
      {
        "avg_logprob": -0.27813674619533874,
        "compression_ratio": 1.7647058823529411,
        "end": 1554.94,
        "id": 587,
        "no_speech_prob": 0.00008219832670874894,
        "seek": 153578,
        "start": 1553.7,
        "temperature": 0,
        "text": " and machine learning?",
        "tokens": [
          51260,
          293,
          3479,
          2539,
          30,
          51322
        ]
      },
      {
        "avg_logprob": -0.27813674619533874,
        "compression_ratio": 1.7647058823529411,
        "end": 1556.62,
        "id": 588,
        "no_speech_prob": 0.00008219832670874894,
        "seek": 153578,
        "start": 1554.94,
        "temperature": 0,
        "text": " It has two hidden layers.",
        "tokens": [
          51322,
          467,
          575,
          732,
          7633,
          7914,
          13,
          51406
        ]
      },
      {
        "avg_logprob": -0.27813674619533874,
        "compression_ratio": 1.7647058823529411,
        "end": 1558.22,
        "id": 589,
        "no_speech_prob": 0.00008219832670874894,
        "seek": 153578,
        "start": 1556.62,
        "temperature": 0,
        "text": " That's it, yeah.",
        "tokens": [
          51406,
          663,
          311,
          309,
          11,
          1338,
          13,
          51486
        ]
      },
      {
        "avg_logprob": -0.27813674619533874,
        "compression_ratio": 1.7647058823529411,
        "end": 1560.22,
        "id": 590,
        "no_speech_prob": 0.00008219832670874894,
        "seek": 153578,
        "start": 1558.22,
        "temperature": 0,
        "text": " Like literally, I think it's more the marketing side,",
        "tokens": [
          51486,
          1743,
          3736,
          11,
          286,
          519,
          309,
          311,
          544,
          264,
          6370,
          1252,
          11,
          51586
        ]
      },
      {
        "avg_logprob": -0.27813674619533874,
        "compression_ratio": 1.7647058823529411,
        "end": 1562.8999999999999,
        "id": 591,
        "no_speech_prob": 0.00008219832670874894,
        "seek": 153578,
        "start": 1560.22,
        "temperature": 0,
        "text": " my personal opinion, but that's awesome.",
        "tokens": [
          51586,
          452,
          2973,
          4800,
          11,
          457,
          300,
          311,
          3476,
          13,
          51720
        ]
      },
      {
        "avg_logprob": -0.24861259758472443,
        "compression_ratio": 1.7611336032388665,
        "end": 1563.94,
        "id": 592,
        "no_speech_prob": 0.00016865185170900077,
        "seek": 156290,
        "start": 1562.94,
        "temperature": 0,
        "text": " Cool.",
        "tokens": [
          50366,
          8561,
          13,
          50416
        ]
      },
      {
        "avg_logprob": -0.24861259758472443,
        "compression_ratio": 1.7611336032388665,
        "end": 1565.9,
        "id": 593,
        "no_speech_prob": 0.00016865185170900077,
        "seek": 156290,
        "start": 1563.94,
        "temperature": 0,
        "text": " Let me see if we have any other,",
        "tokens": [
          50416,
          961,
          385,
          536,
          498,
          321,
          362,
          604,
          661,
          11,
          50514
        ]
      },
      {
        "avg_logprob": -0.24861259758472443,
        "compression_ratio": 1.7611336032388665,
        "end": 1568.3000000000002,
        "id": 594,
        "no_speech_prob": 0.00016865185170900077,
        "seek": 156290,
        "start": 1565.9,
        "temperature": 0,
        "text": " oh yeah, neural network, I like this definition.",
        "tokens": [
          50514,
          1954,
          1338,
          11,
          18161,
          3209,
          11,
          286,
          411,
          341,
          7123,
          13,
          50634
        ]
      },
      {
        "avg_logprob": -0.24861259758472443,
        "compression_ratio": 1.7611336032388665,
        "end": 1571.94,
        "id": 595,
        "no_speech_prob": 0.00016865185170900077,
        "seek": 156290,
        "start": 1568.3000000000002,
        "temperature": 0,
        "text": " Neural network is a universal function approximator.",
        "tokens": [
          50634,
          1734,
          1807,
          3209,
          307,
          257,
          11455,
          2445,
          8542,
          1639,
          13,
          50816
        ]
      },
      {
        "avg_logprob": -0.24861259758472443,
        "compression_ratio": 1.7611336032388665,
        "end": 1575.5,
        "id": 596,
        "no_speech_prob": 0.00016865185170900077,
        "seek": 156290,
        "start": 1571.94,
        "temperature": 0,
        "text": " I actually, this is actually like a really,",
        "tokens": [
          50816,
          286,
          767,
          11,
          341,
          307,
          767,
          411,
          257,
          534,
          11,
          50994
        ]
      },
      {
        "avg_logprob": -0.24861259758472443,
        "compression_ratio": 1.7611336032388665,
        "end": 1577.94,
        "id": 597,
        "no_speech_prob": 0.00016865185170900077,
        "seek": 156290,
        "start": 1575.5,
        "temperature": 0,
        "text": " this is Robin, I'm gonna, let's try this.",
        "tokens": [
          50994,
          341,
          307,
          16533,
          11,
          286,
          478,
          799,
          11,
          718,
          311,
          853,
          341,
          13,
          51116
        ]
      },
      {
        "avg_logprob": -0.24861259758472443,
        "compression_ratio": 1.7611336032388665,
        "end": 1578.94,
        "id": 598,
        "no_speech_prob": 0.00016865185170900077,
        "seek": 156290,
        "start": 1577.94,
        "temperature": 0,
        "text": " Oh, this is crazy talk.",
        "tokens": [
          51116,
          876,
          11,
          341,
          307,
          3219,
          751,
          13,
          51166
        ]
      },
      {
        "avg_logprob": -0.24861259758472443,
        "compression_ratio": 1.7611336032388665,
        "end": 1580.3400000000001,
        "id": 599,
        "no_speech_prob": 0.00016865185170900077,
        "seek": 156290,
        "start": 1578.94,
        "temperature": 0,
        "text": " Now I'm coming over here.",
        "tokens": [
          51166,
          823,
          286,
          478,
          1348,
          670,
          510,
          13,
          51236
        ]
      },
      {
        "avg_logprob": -0.24861259758472443,
        "compression_ratio": 1.7611336032388665,
        "end": 1582.18,
        "id": 600,
        "no_speech_prob": 0.00016865185170900077,
        "seek": 156290,
        "start": 1580.3400000000001,
        "temperature": 0,
        "text": " I actually think this is really kind of a good way",
        "tokens": [
          51236,
          286,
          767,
          519,
          341,
          307,
          534,
          733,
          295,
          257,
          665,
          636,
          51328
        ]
      },
      {
        "avg_logprob": -0.24861259758472443,
        "compression_ratio": 1.7611336032388665,
        "end": 1583.02,
        "id": 601,
        "no_speech_prob": 0.00016865185170900077,
        "seek": 156290,
        "start": 1582.18,
        "temperature": 0,
        "text": " to think about it.",
        "tokens": [
          51328,
          281,
          519,
          466,
          309,
          13,
          51370
        ]
      },
      {
        "avg_logprob": -0.24861259758472443,
        "compression_ratio": 1.7611336032388665,
        "end": 1584.22,
        "id": 602,
        "no_speech_prob": 0.00016865185170900077,
        "seek": 156290,
        "start": 1583.02,
        "temperature": 0,
        "text": " I was thinking about this the other day",
        "tokens": [
          51370,
          286,
          390,
          1953,
          466,
          341,
          264,
          661,
          786,
          51430
        ]
      },
      {
        "avg_logprob": -0.24861259758472443,
        "compression_ratio": 1.7611336032388665,
        "end": 1589.22,
        "id": 603,
        "no_speech_prob": 0.00016865185170900077,
        "seek": 156290,
        "start": 1584.22,
        "temperature": 0,
        "text": " because what if we made function color predictor",
        "tokens": [
          51430,
          570,
          437,
          498,
          321,
          1027,
          2445,
          2017,
          6069,
          284,
          51680
        ]
      },
      {
        "avg_logprob": -0.2640015132843502,
        "compression_ratio": 1.796812749003984,
        "end": 1593.78,
        "id": 604,
        "no_speech_prob": 0.00009027573833009228,
        "seek": 158922,
        "start": 1589.78,
        "temperature": 0,
        "text": " and we just had like an if statement in there",
        "tokens": [
          50392,
          293,
          321,
          445,
          632,
          411,
          364,
          498,
          5629,
          294,
          456,
          50592
        ]
      },
      {
        "avg_logprob": -0.2640015132843502,
        "compression_ratio": 1.796812749003984,
        "end": 1595.9,
        "id": 605,
        "no_speech_prob": 0.00009027573833009228,
        "seek": 158922,
        "start": 1593.78,
        "temperature": 0,
        "text": " and you give it a color.",
        "tokens": [
          50592,
          293,
          291,
          976,
          309,
          257,
          2017,
          13,
          50698
        ]
      },
      {
        "avg_logprob": -0.2640015132843502,
        "compression_ratio": 1.796812749003984,
        "end": 1598.8600000000001,
        "id": 606,
        "no_speech_prob": 0.00009027573833009228,
        "seek": 158922,
        "start": 1595.9,
        "temperature": 0,
        "text": " So if the brightness, blah, blah, blah,",
        "tokens": [
          50698,
          407,
          498,
          264,
          21367,
          11,
          12288,
          11,
          12288,
          11,
          12288,
          11,
          50846
        ]
      },
      {
        "avg_logprob": -0.2640015132843502,
        "compression_ratio": 1.796812749003984,
        "end": 1601.82,
        "id": 607,
        "no_speech_prob": 0.00009027573833009228,
        "seek": 158922,
        "start": 1598.8600000000001,
        "temperature": 0,
        "text": " of that color is greater than some value,",
        "tokens": [
          50846,
          295,
          300,
          2017,
          307,
          5044,
          813,
          512,
          2158,
          11,
          50994
        ]
      },
      {
        "avg_logprob": -0.2640015132843502,
        "compression_ratio": 1.796812749003984,
        "end": 1603.74,
        "id": 608,
        "no_speech_prob": 0.00009027573833009228,
        "seek": 158922,
        "start": 1601.82,
        "temperature": 0,
        "text": " then you should put black on that color",
        "tokens": [
          50994,
          550,
          291,
          820,
          829,
          2211,
          322,
          300,
          2017,
          51090
        ]
      },
      {
        "avg_logprob": -0.2640015132843502,
        "compression_ratio": 1.796812749003984,
        "end": 1605.58,
        "id": 609,
        "no_speech_prob": 0.00009027573833009228,
        "seek": 158922,
        "start": 1603.74,
        "temperature": 0,
        "text": " or white on that color otherwise.",
        "tokens": [
          51090,
          420,
          2418,
          322,
          300,
          2017,
          5911,
          13,
          51182
        ]
      },
      {
        "avg_logprob": -0.2640015132843502,
        "compression_ratio": 1.796812749003984,
        "end": 1608.66,
        "id": 610,
        "no_speech_prob": 0.00009027573833009228,
        "seek": 158922,
        "start": 1605.58,
        "temperature": 0,
        "text": " So this is like a hard coded function",
        "tokens": [
          51182,
          407,
          341,
          307,
          411,
          257,
          1152,
          34874,
          2445,
          51336
        ]
      },
      {
        "avg_logprob": -0.2640015132843502,
        "compression_ratio": 1.796812749003984,
        "end": 1610.78,
        "id": 611,
        "no_speech_prob": 0.00009027573833009228,
        "seek": 158922,
        "start": 1608.66,
        "temperature": 0,
        "text": " that takes inputs and returns an output.",
        "tokens": [
          51336,
          300,
          2516,
          15743,
          293,
          11247,
          364,
          5598,
          13,
          51442
        ]
      },
      {
        "avg_logprob": -0.2640015132843502,
        "compression_ratio": 1.796812749003984,
        "end": 1611.66,
        "id": 612,
        "no_speech_prob": 0.00009027573833009228,
        "seek": 158922,
        "start": 1610.78,
        "temperature": 0,
        "text": " Right.",
        "tokens": [
          51442,
          1779,
          13,
          51486
        ]
      },
      {
        "avg_logprob": -0.2640015132843502,
        "compression_ratio": 1.796812749003984,
        "end": 1614.02,
        "id": 613,
        "no_speech_prob": 0.00009027573833009228,
        "seek": 158922,
        "start": 1611.66,
        "temperature": 0,
        "text": " And so we could write a lot of if statements.",
        "tokens": [
          51486,
          400,
          370,
          321,
          727,
          2464,
          257,
          688,
          295,
          498,
          12363,
          13,
          51604
        ]
      },
      {
        "avg_logprob": -0.2640015132843502,
        "compression_ratio": 1.796812749003984,
        "end": 1615.74,
        "id": 614,
        "no_speech_prob": 0.00009027573833009228,
        "seek": 158922,
        "start": 1614.02,
        "temperature": 0,
        "text": " We could get really crazy complicated about this.",
        "tokens": [
          51604,
          492,
          727,
          483,
          534,
          3219,
          6179,
          466,
          341,
          13,
          51690
        ]
      },
      {
        "avg_logprob": -0.2640015132843502,
        "compression_ratio": 1.796812749003984,
        "end": 1617.6200000000001,
        "id": 615,
        "no_speech_prob": 0.00009027573833009228,
        "seek": 158922,
        "start": 1615.74,
        "temperature": 0,
        "text": " We could come up with a whole set of rules",
        "tokens": [
          51690,
          492,
          727,
          808,
          493,
          365,
          257,
          1379,
          992,
          295,
          4474,
          51784
        ]
      },
      {
        "avg_logprob": -0.24611291540674415,
        "compression_ratio": 1.9297658862876255,
        "end": 1619.3799999999999,
        "id": 616,
        "no_speech_prob": 0.000003905453013430815,
        "seek": 161762,
        "start": 1617.62,
        "temperature": 0,
        "text": " and a neural network in a way is a thing",
        "tokens": [
          50364,
          293,
          257,
          18161,
          3209,
          294,
          257,
          636,
          307,
          257,
          551,
          50452
        ]
      },
      {
        "avg_logprob": -0.24611291540674415,
        "compression_ratio": 1.9297658862876255,
        "end": 1621.86,
        "id": 617,
        "no_speech_prob": 0.000003905453013430815,
        "seek": 161762,
        "start": 1619.3799999999999,
        "temperature": 0,
        "text": " that you could put in here to kind of learn",
        "tokens": [
          50452,
          300,
          291,
          727,
          829,
          294,
          510,
          281,
          733,
          295,
          1466,
          50576
        ]
      },
      {
        "avg_logprob": -0.24611291540674415,
        "compression_ratio": 1.9297658862876255,
        "end": 1624.26,
        "id": 618,
        "no_speech_prob": 0.000003905453013430815,
        "seek": 161762,
        "start": 1621.86,
        "temperature": 0,
        "text": " to return the value according to,",
        "tokens": [
          50576,
          281,
          2736,
          264,
          2158,
          4650,
          281,
          11,
          50696
        ]
      },
      {
        "avg_logprob": -0.24611291540674415,
        "compression_ratio": 1.9297658862876255,
        "end": 1626.06,
        "id": 619,
        "no_speech_prob": 0.000003905453013430815,
        "seek": 161762,
        "start": 1624.26,
        "temperature": 0,
        "text": " in a more mysterious way in a way,",
        "tokens": [
          50696,
          294,
          257,
          544,
          13831,
          636,
          294,
          257,
          636,
          11,
          50786
        ]
      },
      {
        "avg_logprob": -0.24611291540674415,
        "compression_ratio": 1.9297658862876255,
        "end": 1627.2199999999998,
        "id": 620,
        "no_speech_prob": 0.000003905453013430815,
        "seek": 161762,
        "start": 1626.06,
        "temperature": 0,
        "text": " like in a sense.",
        "tokens": [
          50786,
          411,
          294,
          257,
          2020,
          13,
          50844
        ]
      },
      {
        "avg_logprob": -0.24611291540674415,
        "compression_ratio": 1.9297658862876255,
        "end": 1628.7399999999998,
        "id": 621,
        "no_speech_prob": 0.000003905453013430815,
        "seek": 161762,
        "start": 1627.2199999999998,
        "temperature": 0,
        "text": " It can learn, it acts.",
        "tokens": [
          50844,
          467,
          393,
          1466,
          11,
          309,
          10672,
          13,
          50920
        ]
      },
      {
        "avg_logprob": -0.24611291540674415,
        "compression_ratio": 1.9297658862876255,
        "end": 1630.26,
        "id": 622,
        "no_speech_prob": 0.000003905453013430815,
        "seek": 161762,
        "start": 1628.7399999999998,
        "temperature": 0,
        "text": " So in a way like does,",
        "tokens": [
          50920,
          407,
          294,
          257,
          636,
          411,
          775,
          11,
          50996
        ]
      },
      {
        "avg_logprob": -0.24611291540674415,
        "compression_ratio": 1.9297658862876255,
        "end": 1633.7399999999998,
        "id": 623,
        "no_speech_prob": 0.000003905453013430815,
        "seek": 161762,
        "start": 1630.26,
        "temperature": 0,
        "text": " do neural networks and machine learning replace coding?",
        "tokens": [
          50996,
          360,
          18161,
          9590,
          293,
          3479,
          2539,
          7406,
          17720,
          30,
          51170
        ]
      },
      {
        "avg_logprob": -0.24611291540674415,
        "compression_ratio": 1.9297658862876255,
        "end": 1634.58,
        "id": 624,
        "no_speech_prob": 0.000003905453013430815,
        "seek": 161762,
        "start": 1633.7399999999998,
        "temperature": 0,
        "text": " Right.",
        "tokens": [
          51170,
          1779,
          13,
          51212
        ]
      },
      {
        "avg_logprob": -0.24611291540674415,
        "compression_ratio": 1.9297658862876255,
        "end": 1635.4199999999998,
        "id": 625,
        "no_speech_prob": 0.000003905453013430815,
        "seek": 161762,
        "start": 1634.58,
        "temperature": 0,
        "text": " I don't think of them as,",
        "tokens": [
          51212,
          286,
          500,
          380,
          519,
          295,
          552,
          382,
          11,
          51254
        ]
      },
      {
        "avg_logprob": -0.24611291540674415,
        "compression_ratio": 1.9297658862876255,
        "end": 1637.26,
        "id": 626,
        "no_speech_prob": 0.000003905453013430815,
        "seek": 161762,
        "start": 1635.4199999999998,
        "temperature": 0,
        "text": " maybe someday they will in some weird way",
        "tokens": [
          51254,
          1310,
          19412,
          436,
          486,
          294,
          512,
          3657,
          636,
          51346
        ]
      },
      {
        "avg_logprob": -0.24611291540674415,
        "compression_ratio": 1.9297658862876255,
        "end": 1639.34,
        "id": 627,
        "no_speech_prob": 0.000003905453013430815,
        "seek": 161762,
        "start": 1637.26,
        "temperature": 0,
        "text": " but I think of it as like they don't replace coding",
        "tokens": [
          51346,
          457,
          286,
          519,
          295,
          309,
          382,
          411,
          436,
          500,
          380,
          7406,
          17720,
          51450
        ]
      },
      {
        "avg_logprob": -0.24611291540674415,
        "compression_ratio": 1.9297658862876255,
        "end": 1642.78,
        "id": 628,
        "no_speech_prob": 0.000003905453013430815,
        "seek": 161762,
        "start": 1639.34,
        "temperature": 0,
        "text": " but they can replace or act as a function in your code.",
        "tokens": [
          51450,
          457,
          436,
          393,
          7406,
          420,
          605,
          382,
          257,
          2445,
          294,
          428,
          3089,
          13,
          51622
        ]
      },
      {
        "avg_logprob": -0.24611291540674415,
        "compression_ratio": 1.9297658862876255,
        "end": 1644.34,
        "id": 629,
        "no_speech_prob": 0.000003905453013430815,
        "seek": 161762,
        "start": 1642.78,
        "temperature": 0,
        "text": " So that function that you might have hard coded",
        "tokens": [
          51622,
          407,
          300,
          2445,
          300,
          291,
          1062,
          362,
          1152,
          34874,
          51700
        ]
      },
      {
        "avg_logprob": -0.24611291540674415,
        "compression_ratio": 1.9297658862876255,
        "end": 1645.82,
        "id": 630,
        "no_speech_prob": 0.000003905453013430815,
        "seek": 161762,
        "start": 1644.34,
        "temperature": 0,
        "text": " with a lot of if statements can now have",
        "tokens": [
          51700,
          365,
          257,
          688,
          295,
          498,
          12363,
          393,
          586,
          362,
          51774
        ]
      },
      {
        "avg_logprob": -0.24611291540674415,
        "compression_ratio": 1.9297658862876255,
        "end": 1647.26,
        "id": 631,
        "no_speech_prob": 0.000003905453013430815,
        "seek": 161762,
        "start": 1645.82,
        "temperature": 0,
        "text": " a machine learning system in it,",
        "tokens": [
          51774,
          257,
          3479,
          2539,
          1185,
          294,
          309,
          11,
          51846
        ]
      },
      {
        "avg_logprob": -0.28238043815467007,
        "compression_ratio": 1.7089783281733746,
        "end": 1649.46,
        "id": 632,
        "no_speech_prob": 0.00009610062988940626,
        "seek": 164726,
        "start": 1647.9,
        "temperature": 0,
        "text": " take some inputs and generate an output.",
        "tokens": [
          50396,
          747,
          512,
          15743,
          293,
          8460,
          364,
          5598,
          13,
          50474
        ]
      },
      {
        "avg_logprob": -0.28238043815467007,
        "compression_ratio": 1.7089783281733746,
        "end": 1650.66,
        "id": 633,
        "no_speech_prob": 0.00009610062988940626,
        "seek": 164726,
        "start": 1649.46,
        "temperature": 0,
        "text": " Yeah, I agree.",
        "tokens": [
          50474,
          865,
          11,
          286,
          3986,
          13,
          50534
        ]
      },
      {
        "avg_logprob": -0.28238043815467007,
        "compression_ratio": 1.7089783281733746,
        "end": 1652.3799999999999,
        "id": 634,
        "no_speech_prob": 0.00009610062988940626,
        "seek": 164726,
        "start": 1650.66,
        "temperature": 0,
        "text": " I mean, when it's all said and done,",
        "tokens": [
          50534,
          286,
          914,
          11,
          562,
          309,
          311,
          439,
          848,
          293,
          1096,
          11,
          50620
        ]
      },
      {
        "avg_logprob": -0.28238043815467007,
        "compression_ratio": 1.7089783281733746,
        "end": 1655.98,
        "id": 635,
        "no_speech_prob": 0.00009610062988940626,
        "seek": 164726,
        "start": 1652.3799999999999,
        "temperature": 0,
        "text": " algorithms are input, instructions, output.",
        "tokens": [
          50620,
          14642,
          366,
          4846,
          11,
          9415,
          11,
          5598,
          13,
          50800
        ]
      },
      {
        "avg_logprob": -0.28238043815467007,
        "compression_ratio": 1.7089783281733746,
        "end": 1656.82,
        "id": 636,
        "no_speech_prob": 0.00009610062988940626,
        "seek": 164726,
        "start": 1655.98,
        "temperature": 0,
        "text": " Yeah.",
        "tokens": [
          50800,
          865,
          13,
          50842
        ]
      },
      {
        "avg_logprob": -0.28238043815467007,
        "compression_ratio": 1.7089783281733746,
        "end": 1658.22,
        "id": 637,
        "no_speech_prob": 0.00009610062988940626,
        "seek": 164726,
        "start": 1656.82,
        "temperature": 0,
        "text": " And you're just replacing the instruction part.",
        "tokens": [
          50842,
          400,
          291,
          434,
          445,
          19139,
          264,
          10951,
          644,
          13,
          50912
        ]
      },
      {
        "avg_logprob": -0.28238043815467007,
        "compression_ratio": 1.7089783281733746,
        "end": 1659.06,
        "id": 638,
        "no_speech_prob": 0.00009610062988940626,
        "seek": 164726,
        "start": 1658.22,
        "temperature": 0,
        "text": " Yeah, totally.",
        "tokens": [
          50912,
          865,
          11,
          3879,
          13,
          50954
        ]
      },
      {
        "avg_logprob": -0.28238043815467007,
        "compression_ratio": 1.7089783281733746,
        "end": 1662.62,
        "id": 639,
        "no_speech_prob": 0.00009610062988940626,
        "seek": 164726,
        "start": 1660.26,
        "temperature": 0,
        "text": " Thank you so much, Jabril, for being here,",
        "tokens": [
          51014,
          1044,
          291,
          370,
          709,
          11,
          40319,
          24216,
          11,
          337,
          885,
          510,
          11,
          51132
        ]
      },
      {
        "avg_logprob": -0.28238043815467007,
        "compression_ratio": 1.7089783281733746,
        "end": 1665.5,
        "id": 640,
        "no_speech_prob": 0.00009610062988940626,
        "seek": 164726,
        "start": 1662.62,
        "temperature": 0,
        "text": " for participating in the live stream,",
        "tokens": [
          51132,
          337,
          13950,
          294,
          264,
          1621,
          4309,
          11,
          51276
        ]
      },
      {
        "avg_logprob": -0.28238043815467007,
        "compression_ratio": 1.7089783281733746,
        "end": 1666.78,
        "id": 641,
        "no_speech_prob": 0.00009610062988940626,
        "seek": 164726,
        "start": 1665.5,
        "temperature": 0,
        "text": " for showing me your color predictor.",
        "tokens": [
          51276,
          337,
          4099,
          385,
          428,
          2017,
          6069,
          284,
          13,
          51340
        ]
      },
      {
        "avg_logprob": -0.28238043815467007,
        "compression_ratio": 1.7089783281733746,
        "end": 1668.3799999999999,
        "id": 642,
        "no_speech_prob": 0.00009610062988940626,
        "seek": 164726,
        "start": 1666.78,
        "temperature": 0,
        "text": " So much inspiration all week.",
        "tokens": [
          51340,
          407,
          709,
          10249,
          439,
          1243,
          13,
          51420
        ]
      },
      {
        "avg_logprob": -0.28238043815467007,
        "compression_ratio": 1.7089783281733746,
        "end": 1669.9,
        "id": 643,
        "no_speech_prob": 0.00009610062988940626,
        "seek": 164726,
        "start": 1668.3799999999999,
        "temperature": 0,
        "text": " I don't know if you know this,",
        "tokens": [
          51420,
          286,
          500,
          380,
          458,
          498,
          291,
          458,
          341,
          11,
          51496
        ]
      },
      {
        "avg_logprob": -0.28238043815467007,
        "compression_ratio": 1.7089783281733746,
        "end": 1670.74,
        "id": 644,
        "no_speech_prob": 0.00009610062988940626,
        "seek": 164726,
        "start": 1669.9,
        "temperature": 0,
        "text": " why you should know this,",
        "tokens": [
          51496,
          983,
          291,
          820,
          458,
          341,
          11,
          51538
        ]
      },
      {
        "avg_logprob": -0.28238043815467007,
        "compression_ratio": 1.7089783281733746,
        "end": 1672.98,
        "id": 645,
        "no_speech_prob": 0.00009610062988940626,
        "seek": 164726,
        "start": 1670.74,
        "temperature": 0,
        "text": " but Jabril only started working with machine learning",
        "tokens": [
          51538,
          457,
          40319,
          24216,
          787,
          1409,
          1364,
          365,
          3479,
          2539,
          51650
        ]
      },
      {
        "avg_logprob": -0.28238043815467007,
        "compression_ratio": 1.7089783281733746,
        "end": 1675.26,
        "id": 646,
        "no_speech_prob": 0.00009610062988940626,
        "seek": 164726,
        "start": 1672.98,
        "temperature": 0,
        "text": " less than a year ago, completely self-taught,",
        "tokens": [
          51650,
          1570,
          813,
          257,
          1064,
          2057,
          11,
          2584,
          2698,
          12,
          1328,
          1599,
          11,
          51764
        ]
      },
      {
        "avg_logprob": -0.28238043815467007,
        "compression_ratio": 1.7089783281733746,
        "end": 1677.22,
        "id": 647,
        "no_speech_prob": 0.00009610062988940626,
        "seek": 164726,
        "start": 1675.26,
        "temperature": 0,
        "text": " wrote his own neural network from scratch",
        "tokens": [
          51764,
          4114,
          702,
          1065,
          18161,
          3209,
          490,
          8459,
          51862
        ]
      },
      {
        "avg_logprob": -0.22481779856224582,
        "compression_ratio": 1.7170418006430868,
        "end": 1678.9,
        "id": 648,
        "no_speech_prob": 0.002934568328782916,
        "seek": 167722,
        "start": 1678.06,
        "temperature": 0,
        "text": " and JavaScript, he did it in Unity,",
        "tokens": [
          50406,
          293,
          15778,
          11,
          415,
          630,
          309,
          294,
          27913,
          11,
          50448
        ]
      },
      {
        "avg_logprob": -0.22481779856224582,
        "compression_ratio": 1.7170418006430868,
        "end": 1680.9,
        "id": 649,
        "no_speech_prob": 0.002934568328782916,
        "seek": 167722,
        "start": 1678.9,
        "temperature": 0,
        "text": " he's got a whole video project he made",
        "tokens": [
          50448,
          415,
          311,
          658,
          257,
          1379,
          960,
          1716,
          415,
          1027,
          50548
        ]
      },
      {
        "avg_logprob": -0.22481779856224582,
        "compression_ratio": 1.7170418006430868,
        "end": 1682.02,
        "id": 650,
        "no_speech_prob": 0.002934568328782916,
        "seek": 167722,
        "start": 1680.9,
        "temperature": 0,
        "text": " on his channel in Unity.",
        "tokens": [
          50548,
          322,
          702,
          2269,
          294,
          27913,
          13,
          50604
        ]
      },
      {
        "avg_logprob": -0.22481779856224582,
        "compression_ratio": 1.7170418006430868,
        "end": 1683.8600000000001,
        "id": 651,
        "no_speech_prob": 0.002934568328782916,
        "seek": 167722,
        "start": 1682.02,
        "temperature": 0,
        "text": " Just amazing, really inspiring stuff.",
        "tokens": [
          50604,
          1449,
          2243,
          11,
          534,
          15883,
          1507,
          13,
          50696
        ]
      },
      {
        "avg_logprob": -0.22481779856224582,
        "compression_ratio": 1.7170418006430868,
        "end": 1686.5,
        "id": 652,
        "no_speech_prob": 0.002934568328782916,
        "seek": 167722,
        "start": 1683.8600000000001,
        "temperature": 0,
        "text": " So make sure you subscribe to his channel,",
        "tokens": [
          50696,
          407,
          652,
          988,
          291,
          3022,
          281,
          702,
          2269,
          11,
          50828
        ]
      },
      {
        "avg_logprob": -0.22481779856224582,
        "compression_ratio": 1.7170418006430868,
        "end": 1688.6200000000001,
        "id": 653,
        "no_speech_prob": 0.002934568328782916,
        "seek": 167722,
        "start": 1686.5,
        "temperature": 0,
        "text": " link in the description below,",
        "tokens": [
          50828,
          2113,
          294,
          264,
          3855,
          2507,
          11,
          50934
        ]
      },
      {
        "avg_logprob": -0.22481779856224582,
        "compression_ratio": 1.7170418006430868,
        "end": 1689.8600000000001,
        "id": 654,
        "no_speech_prob": 0.002934568328782916,
        "seek": 167722,
        "start": 1688.6200000000001,
        "temperature": 0,
        "text": " click that alarm bell icon,",
        "tokens": [
          50934,
          2052,
          300,
          14183,
          4549,
          6528,
          11,
          50996
        ]
      },
      {
        "avg_logprob": -0.22481779856224582,
        "compression_ratio": 1.7170418006430868,
        "end": 1690.98,
        "id": 655,
        "no_speech_prob": 0.002934568328782916,
        "seek": 167722,
        "start": 1689.8600000000001,
        "temperature": 0,
        "text": " subscribe to him on Twitter,",
        "tokens": [
          50996,
          3022,
          281,
          796,
          322,
          5794,
          11,
          51052
        ]
      },
      {
        "avg_logprob": -0.22481779856224582,
        "compression_ratio": 1.7170418006430868,
        "end": 1691.82,
        "id": 656,
        "no_speech_prob": 0.002934568328782916,
        "seek": 167722,
        "start": 1690.98,
        "temperature": 0,
        "text": " check out his website,",
        "tokens": [
          51052,
          1520,
          484,
          702,
          3144,
          11,
          51094
        ]
      },
      {
        "avg_logprob": -0.22481779856224582,
        "compression_ratio": 1.7170418006430868,
        "end": 1693.46,
        "id": 657,
        "no_speech_prob": 0.002934568328782916,
        "seek": 167722,
        "start": 1691.82,
        "temperature": 0,
        "text": " all that stuff will be in this video's description.",
        "tokens": [
          51094,
          439,
          300,
          1507,
          486,
          312,
          294,
          341,
          960,
          311,
          3855,
          13,
          51176
        ]
      },
      {
        "avg_logprob": -0.22481779856224582,
        "compression_ratio": 1.7170418006430868,
        "end": 1695.8600000000001,
        "id": 658,
        "no_speech_prob": 0.002934568328782916,
        "seek": 167722,
        "start": 1693.46,
        "temperature": 0,
        "text": " Stay tuned for later this week,",
        "tokens": [
          51176,
          8691,
          10870,
          337,
          1780,
          341,
          1243,
          11,
          51296
        ]
      },
      {
        "avg_logprob": -0.22481779856224582,
        "compression_ratio": 1.7170418006430868,
        "end": 1699.38,
        "id": 659,
        "no_speech_prob": 0.002934568328782916,
        "seek": 167722,
        "start": 1695.8600000000001,
        "temperature": 0,
        "text": " I am going to attempt to make my own version",
        "tokens": [
          51296,
          286,
          669,
          516,
          281,
          5217,
          281,
          652,
          452,
          1065,
          3037,
          51472
        ]
      },
      {
        "avg_logprob": -0.22481779856224582,
        "compression_ratio": 1.7170418006430868,
        "end": 1700.54,
        "id": 660,
        "no_speech_prob": 0.002934568328782916,
        "seek": 167722,
        "start": 1699.38,
        "temperature": 0,
        "text": " of the color predictor",
        "tokens": [
          51472,
          295,
          264,
          2017,
          6069,
          284,
          51530
        ]
      },
      {
        "avg_logprob": -0.22481779856224582,
        "compression_ratio": 1.7170418006430868,
        "end": 1703.7,
        "id": 661,
        "no_speech_prob": 0.002934568328782916,
        "seek": 167722,
        "start": 1701.58,
        "temperature": 0,
        "text": " with my toy neural network JavaScript library.",
        "tokens": [
          51582,
          365,
          452,
          12058,
          18161,
          3209,
          15778,
          6405,
          13,
          51688
        ]
      },
      {
        "avg_logprob": -0.22481779856224582,
        "compression_ratio": 1.7170418006430868,
        "end": 1705.66,
        "id": 662,
        "no_speech_prob": 0.002934568328782916,
        "seek": 167722,
        "start": 1703.7,
        "temperature": 0,
        "text": " So that will come in as a coding challenge.",
        "tokens": [
          51688,
          407,
          300,
          486,
          808,
          294,
          382,
          257,
          17720,
          3430,
          13,
          51786
        ]
      },
      {
        "avg_logprob": -0.5321405792236328,
        "compression_ratio": 0.9295774647887324,
        "end": 1708.14,
        "id": 663,
        "no_speech_prob": 0.0002340612409170717,
        "seek": 170566,
        "start": 1705.7,
        "temperature": 0.4,
        "text": " So stay tuned for that and I'll see you in the future.",
        "tokens": [
          50366,
          407,
          1754,
          10870,
          337,
          300,
          293,
          286,
          603,
          536,
          291,
          294,
          264,
          2027,
          13,
          50488
        ]
      },
      {
        "avg_logprob": -0.5321405792236328,
        "compression_ratio": 0.9295774647887324,
        "end": 1709.1000000000001,
        "id": 664,
        "no_speech_prob": 0.0002340612409170717,
        "seek": 170566,
        "start": 1708.14,
        "temperature": 0.4,
        "text": " Goodbye.",
        "tokens": [
          50488,
          15528,
          13,
          50536
        ]
      },
      {
        "avg_logprob": -0.5321405792236328,
        "compression_ratio": 0.9295774647887324,
        "end": 1710.1000000000001,
        "id": 665,
        "no_speech_prob": 0.0002340612409170717,
        "seek": 170566,
        "start": 1709.1000000000001,
        "temperature": 0.4,
        "text": " ()",
        "tokens": [
          50536,
          220,
          45191,
          50586
        ]
      }
    ],
    "transcription": " Hello, welcome to another guest video on the Coding Train. Today I have a very exciting guest for you, Jabril from Ceph Science. I'm probably saying that wrong. Jabril told me like 10 different times how to say it. I still couldn't get it right. Anyway, Jabril is awesome. I'm a big fan of his YouTube channel. He actually came to visit NYU for a whole week and did a workshop and a talk and made a project and it's been sort of been an inspiring presence to have here for all this time. And anyway, what you're about to watch is an edited version of a live stream that the two of us did. He is going to create or talk about a project that he recently made in JavaScript with his own from scratch neural network code where he makes a color predictor. And in fact, if you're interested in more about the color predictor, you can click on, well, you can't click on that, but I don't know, something will come up over here that will maybe suggest that video over there that the two of us made together for his channel. So enjoy Jabril. Stay tuned, later this week, I will do my own coding challenge to try to make my own color predictor. So we'll see how that goes. Thanks Jabril for being here and hope you enjoy this edited version of the live stream we did together. Thanks very much. All right, howdy everyone. How is it going? So yeah, I'm going to give a little brief overview about who I am for those of you that do not know, which I'm sure is all of you. So my name is Jabril. I run a little YouTube channel called Jabril's here on YouTube. And recently I converted my channel to focus on computer science. That happened in September. And that was probably one of the best things I've ever done because I learned that, you know, I had a great passion for writing code and making products and projects that were based on computer science. And so, yeah, I mean, obviously if I had a passion for that, it was easy to show that in video projects as well. And fast forward, so one of the biggest projects that I've produced to date is the Run Forest project that got a lot of eyes. I'm really grateful for. And that really harnessed the power of machine learning, which is a really big buzzword these days. But yeah, that's pretty much the overview. I spent about nine months learning how to write machine learning algorithms from scratch because it was something, AI is really cool to me, I think. And so, yeah, the Run Forest was released. And today, what we're gonna do is we're going to examine this really simple JavaScript machine learning application, kind of how it was done. It's another machine learning application written from scratch. So we're gonna take a look at the code and all that good stuff. So let's get into this. So here we have this example. It's what I call a color predictor neural network demo. And it asks you a simple question. Does white or black look better over this color? And so the color is within the circle and it's randomly generated. Okay, so what's important for us to start before we can get into the application, we have to understand the main computational part of this application. So we have a color. And as you know, colors are, they're represented as a vector of three, or sometimes four if you include the alpha, but we're not including the alpha. We're only gonna use the RGB values. So we have our inputs, which is three. Is that on frame? Yes, all right. So we have red, green, and blue. And these are values between zero and 255 for each input. So we need to build a neural network that will be able to take these inputs and then do a computation on them and then pass into an output to predict if it looks better over black or white. So let's first draw our outputs. Just make sure it's all in frame. Yes, that's good. And this is gonna be, it predicts black and this predicts white. So now we need a hidden layer, is what we call a hidden layer with artificial neural networks in the middle that does the computation part. And this is our guess. And so we are just gonna arbitrarily just duplicate the same size of our inputs for our hidden layer nodes. We're just gonna say three. It's a good place to start. If we're really serious about this, we could expand it, try five, try seven, and just log the results for all of them and see which one works the best. But we're just gonna say three for this example, make it nice and simple. And so we have our RGB. And if we go back to our example, what's happening here is there's a computation that happens within our network. Two, three, one, two, three, one, two, three. And Dan, feel free to interrupt if you think that I'm a little off base with anything. Uh-huh. Uh-huh. Uh-huh. Okay, so this, what did I just do? That looks really confusing. But it's actually really simple. So we need to somehow get our inputs, computation, and then to our outputs. And the way that we do that is we use what I'm using, bubbles, to represent what are called weights within our neural network. And so every single node within our hidden layer has the same amount of weights as there are inputs. So what that means is there's one weight for this input, there's one weight for this input, and there's one weight for this input. And the same for the rest of them. One weight for this input, one weight for this input, one weight for that input, and repeat. I didn't do that right. Boom, boom. And so what then happens is that we pass this through, we do our input times the weight, and then plus our bias, and we can repeat the process. This will give us a value. Let's say that after we compute all these, sum them up, add a bias, it will give us, let's just say.5, and then we'll pass that to our outputs, which is three. Boom. Pass this to our output, and then that will give us a value for each of these. So let's say this is.3, and then this is.7. And then it's just as simple as we'll just say that this is higher,.7, so it's guessing white. So that's a quick overview on what's going on here. Daniel's gonna post a more in-depth tutorial on this, or you already have. Well, so I have tutorials on neural network stuff like this that people could go back. So this is the same kind of structure that I've used in my neural network library videos. I was thinking at some point, maybe next week hopefully, I might try to recreate your project as a coding challenge, in which case I'll revisit this. So we can put a link to your, okay. So we'll put a link to Daniel's Shipman series in which he goes in-depth with this, so if you wanna learn more about what's going on here. But that's a quick overview on the math, on the computation. So our inputs, it gets times by weight and biases, then we get a value, and then we pass that to our output, same computation, and then it gives us a prediction. Oh, and actually, there's a bunch of questions. I might as well ask, can I interrupt you and ask a question? Yeah, let's do it. All right, does the input have to be from zero to 255? Should inputs have to be normalized? What's from zero to one? Yes, great question, great question. So again, I just glossed over this, but so normalizing inputs for colors is actually really simple, and it is always best to normalize your input data. So because we know that the domain for a color value is always gonna be one to 256, or in computer language, we shift that by one, zero to 255, we can simply just divide whatever this value is over 255, and that will remap this between zero and one. And so essentially, when you're writing your program, you would just pass the input through a function that would just divide it by 255. So yes, great question. All right, one more question. So I'm kind of curious about this too. I sort of think it's probably, I'm answering the question before I ask it. But is there a benefit to having two output nodes rather than just have one, since there's only, that's like a range between negative one and one or something like that? Yeah, so there's a lot of debate on this, and I agree with the side that it's easier when you have classifiers versus like, if you have just one output node that is mapped between negative one and one, and then if it's above zero, then it's gonna be white. If it's below zero, then it's gonna be black. Yeah, based on the research that I've read, it's always best to go on a classifier. Yeah, and it seems to me like maybe this would be fine in the case of there's only two labels or two classes, but once you have more than two, it's gonna be problematic. And so as a demonstration and learning, even though this might be a very basic scenario, it's useful to demonstrate the multiple outputs because you're gonna need to do that if you were to expand this further. Correct, and the whole reason for that is because what happens when you separate them is you get probabilities versus you get a map of between zero and one, which again, if it's one output, you can get away with that, but if you try and encode your outputs using this for like 30 different, the neural network might not make good sense of that. Cool, all right. Cool, so let's continue on. Let's look at some of the code as to how we went about writing that part of our neural network. Sweet, so we set up our variables. RGB is our input data. And then so one thing that's really important that I should go over just to make sense of what's going on here is, so it's really important, in order for you to write your algorithm, you need to know how to compute this, compute both of these. So this is really just an array of values. So we can call this array G of I, right? So this is G of zero, and this is G of one. And G just stands for a guess. I put zero, G of one, right? And so we wanna know what does G of I, or what does G of zero, or what does G of one equal? How can we get that equation? Well, if we look at our diagram for our neural network, it's actually quite simple. So G of I, which again is this array, this output layer, G of I equals the summation of a hidden layer. This is gonna be a hidden layer of I, and this is gonna be inputs of I. That's how we define each of these vectors. So G of I equals a summation of HL, hidden layer, and then we have to go into another loop, because we can't use the same in this C of I and J, because it won't return the right value. So a hidden layer of J, which is just gonna be zero, one, two times the weight of G of I. Right, and then we simply just add our bias of G of I. And so this is the equation that we can use to compute each of our output nodes. And so just to clarify what's going on here, this is summation symbol, which simply means to add up all within the array. So hidden layer of J times the weight. This is a function, which simply just grabs the weight of whatever output node you're on. So if you pass G of zero, for example, to do this weight function, it will just grab whatever bias, or I'm sorry, whatever weight of G of I is there. So that's actually G of I of J, actually. Oops, G of I of J. So now we have this equation that tells us exactly what these values equal. So now we don't know what HL of J equals. So we also have to define HL of J. And we go about doing that by doing the same exact process. Actually, this would be HL of I for indices. Same exact process, summation of our inputs, right? Inputs, what did I use, I and P. Inputs, J times weight of HL of J. HL of I, so the same exact input. We need HL of I. And then we simply just pass our bias. And again, this right here is a function. All it does is it grabs the bias for whatever node that we pass through it. So bias of HL of I. And there we have it. We have our entire equation because we know exactly what input of J equals. It's gonna be simply the random value of our color. And so this is what we need to write in our software. Okay, so same exact thing that you see on the board is what we write here in our code. So first, before we can get what the guess nodes equal, we have to first get what the hidden layer nodes equal. So simply put, as we did on the whiteboard, hidden layer zero equals, we'll get to what ReLU is in a second, but hidden layer zero equals, we did our input encoder, which was a question that was asked earlier about normalizing our input data. So this function simply just divides our input, divided by 255, and then we'll times that by the weight of our hidden layer. So this is an array function that I will go over really quickly that we instantiate to hold all of our weights. So color predictor zero, zero, zero. I'll go over this. I think it's important. So the function color predictor zero, zero, zero, the function color predictor variable. So there's all these different dimensions to it, and I think it's interesting, or it's important to go over what the dimensions mean. So let's just get two, and then let's just do, I don't know, one. So what does this mean? If you have color predictor I zero to one, what does that mean? Well, well, so we want to store these arrays into our color predictor variable, and we go about doing that by defining the location of all of these. So the hidden layer is going to be zero, and then the guess is going to be one, right? And so all the nodes are then gonna have their own assignment, so zero, one, and then two. Same here, this is gonna be zero and one, and then the weights are also gonna have their own assignment as well. So zero, one, two, three, and the same deal, zero, one, two, three, and we repeat that for every single weight inside of the nodes, right? And so how this works is our color predictor is if we want to grab reference to zero, that is going to be hidden layer, and then two is going to be the last node, and then one is going to be the second weight, because start zero, one, second weight. So this variable is grabbing a reference to this weight. That's exactly what color predictor zero, two, one is doing. And so you see this I, that's an extra dimension that you might be confused about. So let's talk about that real briefly. So you see this extra I, and what does that mean? So traditionally, with such an example, you would use back propagation to train this neural network. However, time was a factor, as well as, we wanted to go over a lesson of both neural networks and genetic algorithms, so why not combine them together, is what we did for this example. So the I is actually just grabbing a reference to what predictor we are currently using. So genetic algorithms, real quick, you have to have a population, you have to assign fitness scores to every single, what's the word I'm looking for, creature within the population, and then you have to mutate them and breed them in X, Y, and Z. So we have a population of 100 predictors at start, and then they all have randomly initialized weights and biases, which again, just to make sure we're clear, are all of these values, weight, weight, weight, bias, weight, weight, weight, bias. These are all randomly initialized. So the function that we use for this program is randomized between zero and one, and then they all, based on their randomly initialized weights, will make a guess on which one they think is correct, and so most of them said that black is the correct color that looks best over this randomized color, and then we simply just use a generalization function, a genetic algorithm to train this predictor to converge on the best possible predictor. And yeah, that's a general overview on this. The code is available on GitHub, and I left a lot of comments. However, it will require a bit more in-depth if you really want to get a full grasp on this from knowing absolutely nothing. If you already know some stuff about neural networks and machine learning, I'm pretty sure this example is pretty straightforward for you, but the benefit of writing neural networks from scratch is that you really have a good grasp on what's going on behind the scenes versus using libraries, and you're able to debug different problems that might arise when you are writing your neural networks, be it from scratch or using libraries. That is pretty much it. We do have plans to update it with the actual backpropagation algorithm in there so that you can learn from that as well. So let me come over here. There are a few more questions. Okay. Let's see what I can find here. Oh. So first, people had asked, is the code already at GitHub, on GitHub, or are you going to update it on GitHub later? Yeah, I still have to upload it to GitHub. So stay tuned. Whenever it's on GitHub, I will come back and edit the description for this livestream and put a link to the code there. But they can currently go to cephstuff.com slash color. Oh, that link that I use. Right. Yeah, so you can go to, let me zoom in here and show you. So cephstuff.com, this one, right? So if you want to grab the code right now, and I don't know why, whenever I paste links into the YouTube chat, they don't seem to work for people. So but anyway, so I would paste this into the chat, but that wouldn't even work. So you can see it up there, that you can grab the code now. But I will also include a link to GitHub repository, whatever that's. And it is my intention, I think one of the reasons why I love this demo, and people were kind of asking this a bit in the chat, was like, oh, do you really need a neural network for this? Right. And I think to me, I mean, that's a perfectly valid and interesting question. And probably the answer is no, you don't need a neural network for this. But when learning about neural networks, when trying to build your own machine learning project, if you can start with a well-defined, small in scope problem, then you can really figure out, and because in some ways I do this similarly with the semi-genetic algorithm projects. I take a example where I know the answer. So I can see if the genetic algorithm worked. Because ultimately what I want to do is use a neural network or a genetic algorithm in some domain where maybe I don't know the answer, or couldn't solve the answer easily. But to figure out how those things work, I've got to come up with, and so this is a really nice problem for that, because it's simple, small in scope, and for people who want to do creative coding, and graphics, and design stuff, it's got color, and graphics. And not to mention, once you learn how to do this stuff from scratch, this is easily scalable for the most part. You still have to worry about some other stuff like vanishing gradients and stuff like that, but for the most part, you can take this and scale this up and it'll work just about the same. So there's also that benefit as well. So here's a question from IamRoshan on Twitter. This is a big question. Okay, let's do it. I don't know if, and I think this is a question I've certainly touched on, but how is AI, let me read the question how it actually is written. How is AI different from a neural network, or deep learning, or machine learning? They often seem to be used interchangeably and cause confusion. So first of all, I want to say, like this is a really good question, and I struggle with this question all the time, because there's all these different terms. So let me list those terms. Artificial intelligence, deep learning, machine learning. And then I might put neural network in a different category, but that's another term as well. I don't know if you have a kind of like way that you describe these, the terminology to people when they ask this kind of question. Yeah, I like to start with English is difficult. I like to start there. But for the most part, AI is, it's like a grab all for everything AI. Like you can hard code AI. You don't have to use machine learning. So that's kind of like the grab all for it all. And then what was the other, the code words for it? So, oh boy, all these things are coming in. AI, machine learning, deep learning, neural network. Yeah, and so machine learning, I think it's like the next level down. So you don't need to use a neural network to do machine learning. There are different ways that you can go about teaching a machine how to learn. One really good example is decision trees. People have developed really complex decision trees and the machine just kind of explores the space and learns the best way to use this decision tree. So that's another way of applying machine learning. And then what'd you say, neural network and deep learning? Yes. So neural networks is essentially what we showed. And even that's kind of up to debate because like recurrent neural networks are neural networks, but they're not really, you know, neural networked, you know, if that makes sense. So I don't know. If you say neural network, people know what you're talking about for the most part. All right, I'm gonna try to give my take on this. Let's see if I can do it. Let's do it. So neural network to me is a particular algorithm that involves connected nodes and data flows from one node to the other. There could be different architectures and styles. And so that neural network data structure algorithm can be applied in the fields of artificial intelligence, machine learning and deep learning. But neural network is an example of a particular algorithm, I would say. You could sort of think of it also as a data structure, but there's an algorithm there in terms of how the data flows through the structure. So that's what I think. And then I think that, to me, artificial intelligence, I think of that as a very broad umbrella term to just the big field of like simulated intelligence. Whether is that, is it real intelligence? Is it the illusion of intelligence? Is that the same thing? It's like kind of a deep philosophical question. And then machine learning to me is a subfield of artificial intelligence involving making sense of data. So you have data and that's input to a system and you have some output which might be making sense of that data, whether it's a prediction for something that's gonna happen in the future or a classification of something. And then I think of deep learning as a kind of almost like a modern rebranding of machine learning with neural networks. It's like, hey, we have bigger data sets now and faster computers now. All of a sudden, the things that people researched many years ago called neural networks that nobody thought could really do anything or they thought could but couldn't, now all of a sudden, we can do more of them with. And so it's really just like, I mean, it's not meant to be marketing, but it's kind of like marketing this idea of big data, neural networks. Yeah, I agree. I think it's a lot of the marketing side of things because you'll read so many different posts like what's the difference between deep learning and machine learning? It has two hidden layers. That's it, yeah. Like literally, I think it's more the marketing side, my personal opinion, but that's awesome. Cool. Let me see if we have any other, oh yeah, neural network, I like this definition. Neural network is a universal function approximator. I actually, this is actually like a really, this is Robin, I'm gonna, let's try this. Oh, this is crazy talk. Now I'm coming over here. I actually think this is really kind of a good way to think about it. I was thinking about this the other day because what if we made function color predictor and we just had like an if statement in there and you give it a color. So if the brightness, blah, blah, blah, of that color is greater than some value, then you should put black on that color or white on that color otherwise. So this is like a hard coded function that takes inputs and returns an output. Right. And so we could write a lot of if statements. We could get really crazy complicated about this. We could come up with a whole set of rules and a neural network in a way is a thing that you could put in here to kind of learn to return the value according to, in a more mysterious way in a way, like in a sense. It can learn, it acts. So in a way like does, do neural networks and machine learning replace coding? Right. I don't think of them as, maybe someday they will in some weird way but I think of it as like they don't replace coding but they can replace or act as a function in your code. So that function that you might have hard coded with a lot of if statements can now have a machine learning system in it, take some inputs and generate an output. Yeah, I agree. I mean, when it's all said and done, algorithms are input, instructions, output. Yeah. And you're just replacing the instruction part. Yeah, totally. Thank you so much, Jabril, for being here, for participating in the live stream, for showing me your color predictor. So much inspiration all week. I don't know if you know this, why you should know this, but Jabril only started working with machine learning less than a year ago, completely self-taught, wrote his own neural network from scratch and JavaScript, he did it in Unity, he's got a whole video project he made on his channel in Unity. Just amazing, really inspiring stuff. So make sure you subscribe to his channel, link in the description below, click that alarm bell icon, subscribe to him on Twitter, check out his website, all that stuff will be in this video's description. Stay tuned for later this week, I am going to attempt to make my own version of the color predictor with my toy neural network JavaScript library. So that will come in as a coding challenge. So stay tuned for that and I'll see you in the future. Goodbye. ()",
    "translation": null
  },
  "error": null,
  "status": "succeeded",
  "created_at": "2023-09-26T21:03:57.014569Z",
  "started_at": "2023-09-26T21:23:54.462744Z",
  "completed_at": "2023-09-26T21:32:02.294781Z",
  "webhook": "https://83ceaa0b612c.ngrok.app/?video_id=iN3WAko2rL8",
  "webhook_events_filter": [
    "completed"
  ],
  "metrics": {
    "predict_time": 487.832037
  },
  "urls": {
    "cancel": "https://api.replicate.com/v1/predictions/kqfku2bbuxdzjhbwm7l4jutu5e/cancel",
    "get": "https://api.replicate.com/v1/predictions/kqfku2bbuxdzjhbwm7l4jutu5e"
  }
}