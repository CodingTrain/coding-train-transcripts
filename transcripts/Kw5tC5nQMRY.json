{
  "id": "4fzrvzbbiel552lwjcbzl6so5m",
  "version": "91ee9c0c3df30478510ff8c8a3a545add1ad0259ad3a9f78fba57fbc05ee64f7",
  "input": {
    "audio": "https://upcdn.io/FW25b4F/raw/coding-train/Kw5tC5nQMRY.m4a"
  },
  "logs": "Transcribe with large-v2 model\nDetected language: English\n  0%|          | 0/96139 [00:00<?, ?frames/s]\n  3%|▎         | 2880/96139 [00:08<04:34, 340.05frames/s]\n  6%|▌         | 5816/96139 [00:17<04:30, 334.24frames/s]\n  9%|▉         | 8742/96139 [00:25<04:10, 349.21frames/s]\n 12%|█▏        | 11670/96139 [00:34<04:06, 343.07frames/s]\n 15%|█▌        | 14446/96139 [00:42<03:57, 344.27frames/s]\n 18%|█▊        | 17370/96139 [00:50<03:52, 338.45frames/s]\n 21%|██        | 20254/96139 [00:57<03:31, 359.28frames/s]\n 24%|██▍       | 23254/96139 [01:07<03:30, 346.59frames/s]\n 27%|██▋       | 26202/96139 [01:17<03:31, 331.21frames/s]\n 30%|███       | 29154/96139 [01:26<03:27, 322.75frames/s]\n 33%|███▎      | 31814/96139 [01:32<03:01, 353.70frames/s]\n 36%|███▌      | 34682/96139 [01:40<02:54, 353.10frames/s]\n 39%|███▉      | 37578/96139 [01:49<02:52, 340.19frames/s]\n 42%|████▏     | 40530/96139 [01:58<02:45, 335.91frames/s]\n 45%|████▌     | 43494/96139 [02:06<02:30, 349.52frames/s]\n 48%|████▊     | 46058/96139 [02:13<02:22, 350.77frames/s]\n 51%|█████     | 48998/96139 [02:21<02:13, 352.96frames/s]\n 54%|█████▍    | 51962/96139 [02:31<02:11, 335.73frames/s]\n 57%|█████▋    | 54766/96139 [02:40<02:02, 336.84frames/s]\n 60%|██████    | 57738/96139 [02:48<01:53, 339.67frames/s]\n 63%|██████▎   | 60710/96139 [02:58<01:48, 327.86frames/s]\n 66%|██████▌   | 63454/96139 [03:07<01:40, 324.05frames/s]\n 69%|██████▉   | 66398/96139 [03:15<01:30, 329.50frames/s]\n 72%|███████▏  | 69150/96139 [03:22<01:17, 346.82frames/s]\n 75%|███████▍  | 72068/96139 [03:29<01:06, 362.70frames/s]\n 78%|███████▊  | 75064/96139 [03:38<00:58, 358.12frames/s]\n 81%|████████  | 77768/96139 [03:46<00:53, 346.15frames/s]\n 84%|████████▎ | 80478/96139 [03:53<00:43, 358.53frames/s]\n 87%|████████▋ | 83478/96139 [04:02<00:35, 352.72frames/s]\n 90%|████████▉ | 86470/96139 [04:09<00:26, 370.67frames/s]\n 93%|█████████▎| 89358/96139 [04:19<00:19, 340.26frames/s]\n 93%|█████████▎| 89358/96139 [04:30<00:19, 340.26frames/s]\n 96%|█████████▌| 92342/96139 [04:31<00:12, 312.71frames/s]\n 99%|█████████▉| 95122/96139 [04:40<00:03, 310.70frames/s]\n100%|██████████| 96139/96139 [04:41<00:00, 338.11frames/s]\n100%|██████████| 96139/96139 [04:41<00:00, 341.51frames/s]\n",
  "output": {
    "detected_language": "english",
    "segments": [
      {
        "avg_logprob": -0.20763921737670898,
        "compression_ratio": 1.7153558052434457,
        "end": 2,
        "id": 0,
        "no_speech_prob": 0.013016810640692711,
        "seek": 0,
        "start": 0,
        "temperature": 0,
        "text": " Welcome back.",
        "tokens": [
          50364,
          4027,
          646,
          13,
          50464
        ]
      },
      {
        "avg_logprob": -0.20763921737670898,
        "compression_ratio": 1.7153558052434457,
        "end": 3.84,
        "id": 1,
        "no_speech_prob": 0.013016810640692711,
        "seek": 0,
        "start": 2,
        "temperature": 0,
        "text": " I don't know what you did since the last video,",
        "tokens": [
          50464,
          286,
          500,
          380,
          458,
          437,
          291,
          630,
          1670,
          264,
          1036,
          960,
          11,
          50556
        ]
      },
      {
        "avg_logprob": -0.20763921737670898,
        "compression_ratio": 1.7153558052434457,
        "end": 5.08,
        "id": 2,
        "no_speech_prob": 0.013016810640692711,
        "seek": 0,
        "start": 3.84,
        "temperature": 0,
        "text": " but I went and got a haircut.",
        "tokens": [
          50556,
          457,
          286,
          1437,
          293,
          658,
          257,
          30328,
          13,
          50618
        ]
      },
      {
        "avg_logprob": -0.20763921737670898,
        "compression_ratio": 1.7153558052434457,
        "end": 6.48,
        "id": 3,
        "no_speech_prob": 0.013016810640692711,
        "seek": 0,
        "start": 5.08,
        "temperature": 0,
        "text": " Thank you.",
        "tokens": [
          50618,
          1044,
          291,
          13,
          50688
        ]
      },
      {
        "avg_logprob": -0.20763921737670898,
        "compression_ratio": 1.7153558052434457,
        "end": 9.52,
        "id": 4,
        "no_speech_prob": 0.013016810640692711,
        "seek": 0,
        "start": 6.48,
        "temperature": 0,
        "text": " In this video, I am going to do something important,",
        "tokens": [
          50688,
          682,
          341,
          960,
          11,
          286,
          669,
          516,
          281,
          360,
          746,
          1021,
          11,
          50840
        ]
      },
      {
        "avg_logprob": -0.20763921737670898,
        "compression_ratio": 1.7153558052434457,
        "end": 10.56,
        "id": 5,
        "no_speech_prob": 0.013016810640692711,
        "seek": 0,
        "start": 9.52,
        "temperature": 0,
        "text": " a key feature here.",
        "tokens": [
          50840,
          257,
          2141,
          4111,
          510,
          13,
          50892
        ]
      },
      {
        "avg_logprob": -0.20763921737670898,
        "compression_ratio": 1.7153558052434457,
        "end": 15.32,
        "id": 6,
        "no_speech_prob": 0.013016810640692711,
        "seek": 0,
        "start": 10.56,
        "temperature": 0,
        "text": " What I want to do is take data from the client, the latitude",
        "tokens": [
          50892,
          708,
          286,
          528,
          281,
          360,
          307,
          747,
          1412,
          490,
          264,
          6423,
          11,
          264,
          45436,
          51130
        ]
      },
      {
        "avg_logprob": -0.20763921737670898,
        "compression_ratio": 1.7153558052434457,
        "end": 18.32,
        "id": 7,
        "no_speech_prob": 0.013016810640692711,
        "seek": 0,
        "start": 15.32,
        "temperature": 0,
        "text": " and longitude, and send it to the server",
        "tokens": [
          51130,
          293,
          938,
          4377,
          11,
          293,
          2845,
          309,
          281,
          264,
          7154,
          51280
        ]
      },
      {
        "avg_logprob": -0.20763921737670898,
        "compression_ratio": 1.7153558052434457,
        "end": 19.76,
        "id": 8,
        "no_speech_prob": 0.013016810640692711,
        "seek": 0,
        "start": 18.32,
        "temperature": 0,
        "text": " and have the server receive it.",
        "tokens": [
          51280,
          293,
          362,
          264,
          7154,
          4774,
          309,
          13,
          51352
        ]
      },
      {
        "avg_logprob": -0.20763921737670898,
        "compression_ratio": 1.7153558052434457,
        "end": 21.84,
        "id": 9,
        "no_speech_prob": 0.013016810640692711,
        "seek": 0,
        "start": 19.76,
        "temperature": 0,
        "text": " Ultimately, the point of this is for the server",
        "tokens": [
          51352,
          23921,
          11,
          264,
          935,
          295,
          341,
          307,
          337,
          264,
          7154,
          51456
        ]
      },
      {
        "avg_logprob": -0.20763921737670898,
        "compression_ratio": 1.7153558052434457,
        "end": 24.080000000000002,
        "id": 10,
        "no_speech_prob": 0.013016810640692711,
        "seek": 0,
        "start": 21.84,
        "temperature": 0,
        "text": " to eventually save that to a database.",
        "tokens": [
          51456,
          281,
          4728,
          3155,
          300,
          281,
          257,
          8149,
          13,
          51568
        ]
      },
      {
        "avg_logprob": -0.20763921737670898,
        "compression_ratio": 1.7153558052434457,
        "end": 25,
        "id": 11,
        "no_speech_prob": 0.013016810640692711,
        "seek": 0,
        "start": 24.080000000000002,
        "temperature": 0,
        "text": " But I'm not there yet.",
        "tokens": [
          51568,
          583,
          286,
          478,
          406,
          456,
          1939,
          13,
          51614
        ]
      },
      {
        "avg_logprob": -0.20763921737670898,
        "compression_ratio": 1.7153558052434457,
        "end": 28.8,
        "id": 12,
        "no_speech_prob": 0.013016810640692711,
        "seek": 0,
        "start": 25,
        "temperature": 0,
        "text": " I just want to say the client executes",
        "tokens": [
          51614,
          286,
          445,
          528,
          281,
          584,
          264,
          6423,
          4454,
          1819,
          51804
        ]
      },
      {
        "avg_logprob": -0.19055909904644644,
        "compression_ratio": 1.7517985611510791,
        "end": 31.400000000000002,
        "id": 13,
        "no_speech_prob": 0.004133724607527256,
        "seek": 2880,
        "start": 28.8,
        "temperature": 0,
        "text": " its own client-side JavaScript, gets the latitude",
        "tokens": [
          50364,
          1080,
          1065,
          6423,
          12,
          1812,
          15778,
          11,
          2170,
          264,
          45436,
          50494
        ]
      },
      {
        "avg_logprob": -0.19055909904644644,
        "compression_ratio": 1.7517985611510791,
        "end": 34.52,
        "id": 14,
        "no_speech_prob": 0.004133724607527256,
        "seek": 2880,
        "start": 31.400000000000002,
        "temperature": 0,
        "text": " and longitude, and sends that data to the server.",
        "tokens": [
          50494,
          293,
          938,
          4377,
          11,
          293,
          14790,
          300,
          1412,
          281,
          264,
          7154,
          13,
          50650
        ]
      },
      {
        "avg_logprob": -0.19055909904644644,
        "compression_ratio": 1.7517985611510791,
        "end": 36.52,
        "id": 15,
        "no_speech_prob": 0.004133724607527256,
        "seek": 2880,
        "start": 34.52,
        "temperature": 0,
        "text": " And the server can just console log it.",
        "tokens": [
          50650,
          400,
          264,
          7154,
          393,
          445,
          11076,
          3565,
          309,
          13,
          50750
        ]
      },
      {
        "avg_logprob": -0.19055909904644644,
        "compression_ratio": 1.7517985611510791,
        "end": 38.4,
        "id": 16,
        "no_speech_prob": 0.004133724607527256,
        "seek": 2880,
        "start": 36.52,
        "temperature": 0,
        "text": " I'm going to need to look at three things",
        "tokens": [
          50750,
          286,
          478,
          516,
          281,
          643,
          281,
          574,
          412,
          1045,
          721,
          50844
        ]
      },
      {
        "avg_logprob": -0.19055909904644644,
        "compression_ratio": 1.7517985611510791,
        "end": 39.8,
        "id": 17,
        "no_speech_prob": 0.004133724607527256,
        "seek": 2880,
        "start": 38.4,
        "temperature": 0,
        "text": " in order to do this.",
        "tokens": [
          50844,
          294,
          1668,
          281,
          360,
          341,
          13,
          50914
        ]
      },
      {
        "avg_logprob": -0.19055909904644644,
        "compression_ratio": 1.7517985611510791,
        "end": 42.64,
        "id": 18,
        "no_speech_prob": 0.004133724607527256,
        "seek": 2880,
        "start": 39.8,
        "temperature": 0,
        "text": " I'm going to need to look at a concept known as routing.",
        "tokens": [
          50914,
          286,
          478,
          516,
          281,
          643,
          281,
          574,
          412,
          257,
          3410,
          2570,
          382,
          32722,
          13,
          51056
        ]
      },
      {
        "avg_logprob": -0.19055909904644644,
        "compression_ratio": 1.7517985611510791,
        "end": 45.519999999999996,
        "id": 19,
        "no_speech_prob": 0.004133724607527256,
        "seek": 2880,
        "start": 42.64,
        "temperature": 0,
        "text": " How do I set up a route in Express?",
        "tokens": [
          51056,
          1012,
          360,
          286,
          992,
          493,
          257,
          7955,
          294,
          20212,
          30,
          51200
        ]
      },
      {
        "avg_logprob": -0.19055909904644644,
        "compression_ratio": 1.7517985611510791,
        "end": 48.8,
        "id": 20,
        "no_speech_prob": 0.004133724607527256,
        "seek": 2880,
        "start": 45.519999999999996,
        "temperature": 0,
        "text": " This is the place, the endpoint for the API,",
        "tokens": [
          51200,
          639,
          307,
          264,
          1081,
          11,
          264,
          35795,
          337,
          264,
          9362,
          11,
          51364
        ]
      },
      {
        "avg_logprob": -0.19055909904644644,
        "compression_ratio": 1.7517985611510791,
        "end": 52.08,
        "id": 21,
        "no_speech_prob": 0.004133724607527256,
        "seek": 2880,
        "start": 48.8,
        "temperature": 0,
        "text": " the address at which I will send the data to",
        "tokens": [
          51364,
          264,
          2985,
          412,
          597,
          286,
          486,
          2845,
          264,
          1412,
          281,
          51528
        ]
      },
      {
        "avg_logprob": -0.19055909904644644,
        "compression_ratio": 1.7517985611510791,
        "end": 54.72,
        "id": 22,
        "no_speech_prob": 0.004133724607527256,
        "seek": 2880,
        "start": 52.08,
        "temperature": 0,
        "text": " and later also make a request to receive some data.",
        "tokens": [
          51528,
          293,
          1780,
          611,
          652,
          257,
          5308,
          281,
          4774,
          512,
          1412,
          13,
          51660
        ]
      },
      {
        "avg_logprob": -0.19055909904644644,
        "compression_ratio": 1.7517985611510791,
        "end": 58.16,
        "id": 23,
        "no_speech_prob": 0.004133724607527256,
        "seek": 2880,
        "start": 54.72,
        "temperature": 0,
        "text": " I need to look at adding JSON parsing to Express.",
        "tokens": [
          51660,
          286,
          643,
          281,
          574,
          412,
          5127,
          31828,
          21156,
          278,
          281,
          20212,
          13,
          51832
        ]
      },
      {
        "avg_logprob": -0.19853950949276195,
        "compression_ratio": 1.6963562753036436,
        "end": 60.31999999999999,
        "id": 24,
        "no_speech_prob": 0.00009027975465869531,
        "seek": 5816,
        "start": 58.16,
        "temperature": 0,
        "text": " I need the route when it receives",
        "tokens": [
          50364,
          286,
          643,
          264,
          7955,
          562,
          309,
          20717,
          50472
        ]
      },
      {
        "avg_logprob": -0.19853950949276195,
        "compression_ratio": 1.6963562753036436,
        "end": 62.559999999999995,
        "id": 25,
        "no_speech_prob": 0.00009027975465869531,
        "seek": 5816,
        "start": 60.31999999999999,
        "temperature": 0,
        "text": " data to understand that data as JSON",
        "tokens": [
          50472,
          1412,
          281,
          1223,
          300,
          1412,
          382,
          31828,
          50584
        ]
      },
      {
        "avg_logprob": -0.19853950949276195,
        "compression_ratio": 1.6963562753036436,
        "end": 65.03999999999999,
        "id": 26,
        "no_speech_prob": 0.00009027975465869531,
        "seek": 5816,
        "start": 62.559999999999995,
        "temperature": 0,
        "text": " and make it readable in my code.",
        "tokens": [
          50584,
          293,
          652,
          309,
          49857,
          294,
          452,
          3089,
          13,
          50708
        ]
      },
      {
        "avg_logprob": -0.19853950949276195,
        "compression_ratio": 1.6963562753036436,
        "end": 68.12,
        "id": 27,
        "no_speech_prob": 0.00009027975465869531,
        "seek": 5816,
        "start": 65.03999999999999,
        "temperature": 0,
        "text": " And then I also need to look at adapting the fetch function",
        "tokens": [
          50708,
          400,
          550,
          286,
          611,
          643,
          281,
          574,
          412,
          34942,
          264,
          23673,
          2445,
          50862
        ]
      },
      {
        "avg_logprob": -0.19853950949276195,
        "compression_ratio": 1.6963562753036436,
        "end": 73.67999999999999,
        "id": 28,
        "no_speech_prob": 0.00009027975465869531,
        "seek": 5816,
        "start": 68.12,
        "temperature": 0,
        "text": " to specify a POST request, a request that is posting data",
        "tokens": [
          50862,
          281,
          16500,
          257,
          430,
          28067,
          5308,
          11,
          257,
          5308,
          300,
          307,
          15978,
          1412,
          51140
        ]
      },
      {
        "avg_logprob": -0.19853950949276195,
        "compression_ratio": 1.6963562753036436,
        "end": 76.16,
        "id": 29,
        "no_speech_prob": 0.00009027975465869531,
        "seek": 5816,
        "start": 73.67999999999999,
        "temperature": 0,
        "text": " or sending data to the server.",
        "tokens": [
          51140,
          420,
          7750,
          1412,
          281,
          264,
          7154,
          13,
          51264
        ]
      },
      {
        "avg_logprob": -0.19853950949276195,
        "compression_ratio": 1.6963562753036436,
        "end": 77.75999999999999,
        "id": 30,
        "no_speech_prob": 0.00009027975465869531,
        "seek": 5816,
        "start": 76.16,
        "temperature": 0,
        "text": " Let's start with the route.",
        "tokens": [
          51264,
          961,
          311,
          722,
          365,
          264,
          7955,
          13,
          51344
        ]
      },
      {
        "avg_logprob": -0.19853950949276195,
        "compression_ratio": 1.6963562753036436,
        "end": 81.6,
        "id": 31,
        "no_speech_prob": 0.00009027975465869531,
        "seek": 5816,
        "start": 77.75999999999999,
        "temperature": 0,
        "text": " So I want to set up a route on my server.",
        "tokens": [
          51344,
          407,
          286,
          528,
          281,
          992,
          493,
          257,
          7955,
          322,
          452,
          7154,
          13,
          51536
        ]
      },
      {
        "avg_logprob": -0.19853950949276195,
        "compression_ratio": 1.6963562753036436,
        "end": 83.75999999999999,
        "id": 32,
        "no_speech_prob": 0.00009027975465869531,
        "seek": 5816,
        "start": 81.6,
        "temperature": 0,
        "text": " So the way that I do that is by specifying",
        "tokens": [
          51536,
          407,
          264,
          636,
          300,
          286,
          360,
          300,
          307,
          538,
          1608,
          5489,
          51644
        ]
      },
      {
        "avg_logprob": -0.19853950949276195,
        "compression_ratio": 1.6963562753036436,
        "end": 87.42,
        "id": 33,
        "no_speech_prob": 0.00009027975465869531,
        "seek": 5816,
        "start": 83.75999999999999,
        "temperature": 0,
        "text": " this particular route will either be a GET or a POST.",
        "tokens": [
          51644,
          341,
          1729,
          7955,
          486,
          2139,
          312,
          257,
          28091,
          420,
          257,
          430,
          28067,
          13,
          51827
        ]
      },
      {
        "avg_logprob": -0.20356500749107745,
        "compression_ratio": 1.7452471482889733,
        "end": 90.10000000000001,
        "id": 34,
        "no_speech_prob": 0.00002212542312918231,
        "seek": 8742,
        "start": 87.42,
        "temperature": 0,
        "text": " In this case, I expect a POST request.",
        "tokens": [
          50364,
          682,
          341,
          1389,
          11,
          286,
          2066,
          257,
          430,
          28067,
          5308,
          13,
          50498
        ]
      },
      {
        "avg_logprob": -0.20356500749107745,
        "compression_ratio": 1.7452471482889733,
        "end": 93.98,
        "id": 35,
        "no_speech_prob": 0.00002212542312918231,
        "seek": 8742,
        "start": 90.10000000000001,
        "temperature": 0,
        "text": " So I am going to go and say app.post.",
        "tokens": [
          50498,
          407,
          286,
          669,
          516,
          281,
          352,
          293,
          584,
          724,
          13,
          23744,
          13,
          50692
        ]
      },
      {
        "avg_logprob": -0.20356500749107745,
        "compression_ratio": 1.7452471482889733,
        "end": 99.74000000000001,
        "id": 36,
        "no_speech_prob": 0.00002212542312918231,
        "seek": 8742,
        "start": 93.98,
        "temperature": 0,
        "text": " Now, once I have a POST, I want to specify both the address,",
        "tokens": [
          50692,
          823,
          11,
          1564,
          286,
          362,
          257,
          430,
          28067,
          11,
          286,
          528,
          281,
          16500,
          1293,
          264,
          2985,
          11,
          50980
        ]
      },
      {
        "avg_logprob": -0.20356500749107745,
        "compression_ratio": 1.7452471482889733,
        "end": 101.46000000000001,
        "id": 37,
        "no_speech_prob": 0.00002212542312918231,
        "seek": 8742,
        "start": 99.74000000000001,
        "temperature": 0,
        "text": " where I want to receive that POST,",
        "tokens": [
          50980,
          689,
          286,
          528,
          281,
          4774,
          300,
          430,
          28067,
          11,
          51066
        ]
      },
      {
        "avg_logprob": -0.20356500749107745,
        "compression_ratio": 1.7452471482889733,
        "end": 103.82,
        "id": 38,
        "no_speech_prob": 0.00002212542312918231,
        "seek": 8742,
        "start": 101.46000000000001,
        "temperature": 0,
        "text": " as well as a callback function where",
        "tokens": [
          51066,
          382,
          731,
          382,
          257,
          818,
          3207,
          2445,
          689,
          51184
        ]
      },
      {
        "avg_logprob": -0.20356500749107745,
        "compression_ratio": 1.7452471482889733,
        "end": 105.7,
        "id": 39,
        "no_speech_prob": 0.00002212542312918231,
        "seek": 8742,
        "start": 103.82,
        "temperature": 0,
        "text": " I'm going to look at the information coming in",
        "tokens": [
          51184,
          286,
          478,
          516,
          281,
          574,
          412,
          264,
          1589,
          1348,
          294,
          51278
        ]
      },
      {
        "avg_logprob": -0.20356500749107745,
        "compression_ratio": 1.7452471482889733,
        "end": 107.42,
        "id": 40,
        "no_speech_prob": 0.00002212542312918231,
        "seek": 8742,
        "start": 105.7,
        "temperature": 0,
        "text": " and send a response back.",
        "tokens": [
          51278,
          293,
          2845,
          257,
          4134,
          646,
          13,
          51364
        ]
      },
      {
        "avg_logprob": -0.20356500749107745,
        "compression_ratio": 1.7452471482889733,
        "end": 108.94,
        "id": 41,
        "no_speech_prob": 0.00002212542312918231,
        "seek": 8742,
        "start": 107.42,
        "temperature": 0,
        "text": " So let's set up that address.",
        "tokens": [
          51364,
          407,
          718,
          311,
          992,
          493,
          300,
          2985,
          13,
          51440
        ]
      },
      {
        "avg_logprob": -0.20356500749107745,
        "compression_ratio": 1.7452471482889733,
        "end": 111.58,
        "id": 42,
        "no_speech_prob": 0.00002212542312918231,
        "seek": 8742,
        "start": 108.94,
        "temperature": 0,
        "text": " Let's set up the endpoint for this particular route,",
        "tokens": [
          51440,
          961,
          311,
          992,
          493,
          264,
          35795,
          337,
          341,
          1729,
          7955,
          11,
          51572
        ]
      },
      {
        "avg_logprob": -0.20356500749107745,
        "compression_ratio": 1.7452471482889733,
        "end": 113.78,
        "id": 43,
        "no_speech_prob": 0.00002212542312918231,
        "seek": 8742,
        "start": 111.58,
        "temperature": 0,
        "text": " where I want to receive the POST.",
        "tokens": [
          51572,
          689,
          286,
          528,
          281,
          4774,
          264,
          430,
          28067,
          13,
          51682
        ]
      },
      {
        "avg_logprob": -0.20356500749107745,
        "compression_ratio": 1.7452471482889733,
        "end": 116.7,
        "id": 44,
        "no_speech_prob": 0.00002212542312918231,
        "seek": 8742,
        "start": 113.78,
        "temperature": 0,
        "text": " And I could call it anything I want, like unicorn, cupcake,",
        "tokens": [
          51682,
          400,
          286,
          727,
          818,
          309,
          1340,
          286,
          528,
          11,
          411,
          28122,
          11,
          42153,
          11,
          51828
        ]
      },
      {
        "avg_logprob": -0.2428057403564453,
        "compression_ratio": 1.591760299625468,
        "end": 118.22,
        "id": 45,
        "no_speech_prob": 0.0000660507648717612,
        "seek": 11670,
        "start": 116.78,
        "temperature": 0,
        "text": " rainbow.",
        "tokens": [
          50368,
          18526,
          13,
          50440
        ]
      },
      {
        "avg_logprob": -0.2428057403564453,
        "compression_ratio": 1.591760299625468,
        "end": 120.06,
        "id": 46,
        "no_speech_prob": 0.0000660507648717612,
        "seek": 11670,
        "start": 118.22,
        "temperature": 0,
        "text": " Let's call it rainbow.",
        "tokens": [
          50440,
          961,
          311,
          818,
          309,
          18526,
          13,
          50532
        ]
      },
      {
        "avg_logprob": -0.2428057403564453,
        "compression_ratio": 1.591760299625468,
        "end": 122.52000000000001,
        "id": 47,
        "no_speech_prob": 0.0000660507648717612,
        "seek": 11670,
        "start": 120.06,
        "temperature": 0,
        "text": " All right, I'll give it a name that's maybe more appropriate.",
        "tokens": [
          50532,
          1057,
          558,
          11,
          286,
          603,
          976,
          309,
          257,
          1315,
          300,
          311,
          1310,
          544,
          6854,
          13,
          50655
        ]
      },
      {
        "avg_logprob": -0.2428057403564453,
        "compression_ratio": 1.591760299625468,
        "end": 123.9,
        "id": 48,
        "no_speech_prob": 0.0000660507648717612,
        "seek": 11670,
        "start": 122.52000000000001,
        "temperature": 0,
        "text": " I'm going to call it API.",
        "tokens": [
          50655,
          286,
          478,
          516,
          281,
          818,
          309,
          9362,
          13,
          50724
        ]
      },
      {
        "avg_logprob": -0.2428057403564453,
        "compression_ratio": 1.591760299625468,
        "end": 126.18,
        "id": 49,
        "no_speech_prob": 0.0000660507648717612,
        "seek": 11670,
        "start": 123.9,
        "temperature": 0,
        "text": " API, because what I'm really doing here",
        "tokens": [
          50724,
          9362,
          11,
          570,
          437,
          286,
          478,
          534,
          884,
          510,
          50838
        ]
      },
      {
        "avg_logprob": -0.2428057403564453,
        "compression_ratio": 1.591760299625468,
        "end": 129.22,
        "id": 50,
        "no_speech_prob": 0.0000660507648717612,
        "seek": 11670,
        "start": 126.18,
        "temperature": 0,
        "text": " is setting up an application programming interface.",
        "tokens": [
          50838,
          307,
          3287,
          493,
          364,
          3861,
          9410,
          9226,
          13,
          50990
        ]
      },
      {
        "avg_logprob": -0.2428057403564453,
        "compression_ratio": 1.591760299625468,
        "end": 132.9,
        "id": 51,
        "no_speech_prob": 0.0000660507648717612,
        "seek": 11670,
        "start": 129.22,
        "temperature": 0,
        "text": " This is my API for clients to send data to me.",
        "tokens": [
          50990,
          639,
          307,
          452,
          9362,
          337,
          6982,
          281,
          2845,
          1412,
          281,
          385,
          13,
          51174
        ]
      },
      {
        "avg_logprob": -0.2428057403564453,
        "compression_ratio": 1.591760299625468,
        "end": 138.02,
        "id": 52,
        "no_speech_prob": 0.0000660507648717612,
        "seek": 11670,
        "start": 132.9,
        "temperature": 0,
        "text": " Next, I'm going to set up a callback request response.",
        "tokens": [
          51174,
          3087,
          11,
          286,
          478,
          516,
          281,
          992,
          493,
          257,
          818,
          3207,
          5308,
          4134,
          13,
          51430
        ]
      },
      {
        "avg_logprob": -0.2428057403564453,
        "compression_ratio": 1.591760299625468,
        "end": 141.78,
        "id": 53,
        "no_speech_prob": 0.0000660507648717612,
        "seek": 11670,
        "start": 138.02,
        "temperature": 0,
        "text": " And I'm using the ES6 JavaScript, ES6 arrow syntax,",
        "tokens": [
          51430,
          400,
          286,
          478,
          1228,
          264,
          12564,
          21,
          15778,
          11,
          12564,
          21,
          11610,
          28431,
          11,
          51618
        ]
      },
      {
        "avg_logprob": -0.2428057403564453,
        "compression_ratio": 1.591760299625468,
        "end": 144.46,
        "id": 54,
        "no_speech_prob": 0.0000660507648717612,
        "seek": 11670,
        "start": 141.78,
        "temperature": 0,
        "text": " which is a sort of nice, clean way of putting this in here.",
        "tokens": [
          51618,
          597,
          307,
          257,
          1333,
          295,
          1481,
          11,
          2541,
          636,
          295,
          3372,
          341,
          294,
          510,
          13,
          51752
        ]
      },
      {
        "avg_logprob": -0.22467639480811963,
        "compression_ratio": 1.8201438848920863,
        "end": 147.22,
        "id": 55,
        "no_speech_prob": 0.000017502718037576415,
        "seek": 14446,
        "start": 144.46,
        "temperature": 0,
        "text": " So the function is here.",
        "tokens": [
          50364,
          407,
          264,
          2445,
          307,
          510,
          13,
          50502
        ]
      },
      {
        "avg_logprob": -0.22467639480811963,
        "compression_ratio": 1.8201438848920863,
        "end": 150.1,
        "id": 56,
        "no_speech_prob": 0.000017502718037576415,
        "seek": 14446,
        "start": 147.22,
        "temperature": 0,
        "text": " And it has two arguments, request and response.",
        "tokens": [
          50502,
          400,
          309,
          575,
          732,
          12869,
          11,
          5308,
          293,
          4134,
          13,
          50646
        ]
      },
      {
        "avg_logprob": -0.22467639480811963,
        "compression_ratio": 1.8201438848920863,
        "end": 152.78,
        "id": 57,
        "no_speech_prob": 0.000017502718037576415,
        "seek": 14446,
        "start": 150.1,
        "temperature": 0,
        "text": " The request variable holds everything",
        "tokens": [
          50646,
          440,
          5308,
          7006,
          9190,
          1203,
          50780
        ]
      },
      {
        "avg_logprob": -0.22467639480811963,
        "compression_ratio": 1.8201438848920863,
        "end": 155.12,
        "id": 58,
        "no_speech_prob": 0.000017502718037576415,
        "seek": 14446,
        "start": 152.78,
        "temperature": 0,
        "text": " that's contained within that request, all the data that's",
        "tokens": [
          50780,
          300,
          311,
          16212,
          1951,
          300,
          5308,
          11,
          439,
          264,
          1412,
          300,
          311,
          50897
        ]
      },
      {
        "avg_logprob": -0.22467639480811963,
        "compression_ratio": 1.8201438848920863,
        "end": 156.82,
        "id": 59,
        "no_speech_prob": 0.000017502718037576415,
        "seek": 14446,
        "start": 155.12,
        "temperature": 0,
        "text": " being sent, any information I need",
        "tokens": [
          50897,
          885,
          2279,
          11,
          604,
          1589,
          286,
          643,
          50982
        ]
      },
      {
        "avg_logprob": -0.22467639480811963,
        "compression_ratio": 1.8201438848920863,
        "end": 158.62,
        "id": 60,
        "no_speech_prob": 0.000017502718037576415,
        "seek": 14446,
        "start": 156.82,
        "temperature": 0,
        "text": " to know about that particular client that's",
        "tokens": [
          50982,
          281,
          458,
          466,
          300,
          1729,
          6423,
          300,
          311,
          51072
        ]
      },
      {
        "avg_logprob": -0.22467639480811963,
        "compression_ratio": 1.8201438848920863,
        "end": 159.86,
        "id": 61,
        "no_speech_prob": 0.000017502718037576415,
        "seek": 14446,
        "start": 158.62,
        "temperature": 0,
        "text": " sending the information.",
        "tokens": [
          51072,
          7750,
          264,
          1589,
          13,
          51134
        ]
      },
      {
        "avg_logprob": -0.22467639480811963,
        "compression_ratio": 1.8201438848920863,
        "end": 163.98000000000002,
        "id": 62,
        "no_speech_prob": 0.000017502718037576415,
        "seek": 14446,
        "start": 159.86,
        "temperature": 0,
        "text": " The response is a variable that I can use to send things back",
        "tokens": [
          51134,
          440,
          4134,
          307,
          257,
          7006,
          300,
          286,
          393,
          764,
          281,
          2845,
          721,
          646,
          51340
        ]
      },
      {
        "avg_logprob": -0.22467639480811963,
        "compression_ratio": 1.8201438848920863,
        "end": 164.62,
        "id": 63,
        "no_speech_prob": 0.000017502718037576415,
        "seek": 14446,
        "start": 163.98000000000002,
        "temperature": 0,
        "text": " to the client.",
        "tokens": [
          51340,
          281,
          264,
          6423,
          13,
          51372
        ]
      },
      {
        "avg_logprob": -0.22467639480811963,
        "compression_ratio": 1.8201438848920863,
        "end": 167.54000000000002,
        "id": 64,
        "no_speech_prob": 0.000017502718037576415,
        "seek": 14446,
        "start": 164.62,
        "temperature": 0,
        "text": " So I'm going to need, ultimately, to do stuff there.",
        "tokens": [
          51372,
          407,
          286,
          478,
          516,
          281,
          643,
          11,
          6284,
          11,
          281,
          360,
          1507,
          456,
          13,
          51518
        ]
      },
      {
        "avg_logprob": -0.22467639480811963,
        "compression_ratio": 1.8201438848920863,
        "end": 170.58,
        "id": 65,
        "no_speech_prob": 0.000017502718037576415,
        "seek": 14446,
        "start": 167.54000000000002,
        "temperature": 0,
        "text": " But for right now, I'm just going to say console.log",
        "tokens": [
          51518,
          583,
          337,
          558,
          586,
          11,
          286,
          478,
          445,
          516,
          281,
          584,
          11076,
          13,
          4987,
          51670
        ]
      },
      {
        "avg_logprob": -0.22467639480811963,
        "compression_ratio": 1.8201438848920863,
        "end": 171.98000000000002,
        "id": 66,
        "no_speech_prob": 0.000017502718037576415,
        "seek": 14446,
        "start": 170.58,
        "temperature": 0,
        "text": " request.",
        "tokens": [
          51670,
          5308,
          13,
          51740
        ]
      },
      {
        "avg_logprob": -0.22467639480811963,
        "compression_ratio": 1.8201438848920863,
        "end": 173.70000000000002,
        "id": 67,
        "no_speech_prob": 0.000017502718037576415,
        "seek": 14446,
        "start": 171.98000000000002,
        "temperature": 0,
        "text": " And I'm just going to leave it like that.",
        "tokens": [
          51740,
          400,
          286,
          478,
          445,
          516,
          281,
          1856,
          309,
          411,
          300,
          13,
          51826
        ]
      },
      {
        "avg_logprob": -0.2104449475932325,
        "compression_ratio": 1.7721518987341771,
        "end": 176.26,
        "id": 68,
        "no_speech_prob": 0.0002131839719368145,
        "seek": 17370,
        "start": 173.7,
        "temperature": 0,
        "text": " Because let's at least see if we can get this working",
        "tokens": [
          50364,
          1436,
          718,
          311,
          412,
          1935,
          536,
          498,
          321,
          393,
          483,
          341,
          1364,
          50492
        ]
      },
      {
        "avg_logprob": -0.2104449475932325,
        "compression_ratio": 1.7721518987341771,
        "end": 178.98,
        "id": 69,
        "no_speech_prob": 0.0002131839719368145,
        "seek": 17370,
        "start": 176.26,
        "temperature": 0,
        "text": " and see if we can see something console.log there.",
        "tokens": [
          50492,
          293,
          536,
          498,
          321,
          393,
          536,
          746,
          11076,
          13,
          4987,
          456,
          13,
          50628
        ]
      },
      {
        "avg_logprob": -0.2104449475932325,
        "compression_ratio": 1.7721518987341771,
        "end": 182.45999999999998,
        "id": 70,
        "no_speech_prob": 0.0002131839719368145,
        "seek": 17370,
        "start": 178.98,
        "temperature": 0,
        "text": " The next step is to have my client send something",
        "tokens": [
          50628,
          440,
          958,
          1823,
          307,
          281,
          362,
          452,
          6423,
          2845,
          746,
          50802
        ]
      },
      {
        "avg_logprob": -0.2104449475932325,
        "compression_ratio": 1.7721518987341771,
        "end": 185.33999999999997,
        "id": 71,
        "no_speech_prob": 0.0002131839719368145,
        "seek": 17370,
        "start": 182.45999999999998,
        "temperature": 0,
        "text": " to this particular endpoint with a post.",
        "tokens": [
          50802,
          281,
          341,
          1729,
          35795,
          365,
          257,
          2183,
          13,
          50946
        ]
      },
      {
        "avg_logprob": -0.2104449475932325,
        "compression_ratio": 1.7721518987341771,
        "end": 187.66,
        "id": 72,
        "no_speech_prob": 0.0002131839719368145,
        "seek": 17370,
        "start": 185.33999999999997,
        "temperature": 0,
        "text": " I can now move over to the client code",
        "tokens": [
          50946,
          286,
          393,
          586,
          1286,
          670,
          281,
          264,
          6423,
          3089,
          51062
        ]
      },
      {
        "avg_logprob": -0.2104449475932325,
        "compression_ratio": 1.7721518987341771,
        "end": 190.78,
        "id": 73,
        "no_speech_prob": 0.0002131839719368145,
        "seek": 17370,
        "start": 187.66,
        "temperature": 0,
        "text": " and set up the post itself using fetch.",
        "tokens": [
          51062,
          293,
          992,
          493,
          264,
          2183,
          2564,
          1228,
          23673,
          13,
          51218
        ]
      },
      {
        "avg_logprob": -0.2104449475932325,
        "compression_ratio": 1.7721518987341771,
        "end": 195.7,
        "id": 74,
        "no_speech_prob": 0.0002131839719368145,
        "seek": 17370,
        "start": 190.78,
        "temperature": 0,
        "text": " So I want to fetch my post over to slash API endpoint.",
        "tokens": [
          51218,
          407,
          286,
          528,
          281,
          23673,
          452,
          2183,
          670,
          281,
          17330,
          9362,
          35795,
          13,
          51464
        ]
      },
      {
        "avg_logprob": -0.2104449475932325,
        "compression_ratio": 1.7721518987341771,
        "end": 198.38,
        "id": 75,
        "no_speech_prob": 0.0002131839719368145,
        "seek": 17370,
        "start": 195.7,
        "temperature": 0,
        "text": " So the client code, remember, this is confusing.",
        "tokens": [
          51464,
          407,
          264,
          6423,
          3089,
          11,
          1604,
          11,
          341,
          307,
          13181,
          13,
          51598
        ]
      },
      {
        "avg_logprob": -0.2104449475932325,
        "compression_ratio": 1.7721518987341771,
        "end": 202.54,
        "id": 76,
        "no_speech_prob": 0.0002131839719368145,
        "seek": 17370,
        "start": 198.38,
        "temperature": 0,
        "text": " Server in index.js, client in index.html.",
        "tokens": [
          51598,
          25684,
          294,
          8186,
          13,
          25530,
          11,
          6423,
          294,
          8186,
          13,
          357,
          15480,
          13,
          51806
        ]
      },
      {
        "avg_logprob": -0.19143409263796923,
        "compression_ratio": 1.9137931034482758,
        "end": 204.34,
        "id": 77,
        "no_speech_prob": 0.00008481018448946998,
        "seek": 20254,
        "start": 202.54,
        "temperature": 0,
        "text": " I'm going to move over here.",
        "tokens": [
          50364,
          286,
          478,
          516,
          281,
          1286,
          670,
          510,
          13,
          50454
        ]
      },
      {
        "avg_logprob": -0.19143409263796923,
        "compression_ratio": 1.9137931034482758,
        "end": 206.7,
        "id": 78,
        "no_speech_prob": 0.00008481018448946998,
        "seek": 20254,
        "start": 204.34,
        "temperature": 0,
        "text": " What I'll do is I'll choose to send this latitude",
        "tokens": [
          50454,
          708,
          286,
          603,
          360,
          307,
          286,
          603,
          2826,
          281,
          2845,
          341,
          45436,
          50572
        ]
      },
      {
        "avg_logprob": -0.19143409263796923,
        "compression_ratio": 1.9137931034482758,
        "end": 208.7,
        "id": 79,
        "no_speech_prob": 0.00008481018448946998,
        "seek": 20254,
        "start": 206.7,
        "temperature": 0,
        "text": " and longitude as soon as I've received it.",
        "tokens": [
          50572,
          293,
          938,
          4377,
          382,
          2321,
          382,
          286,
          600,
          4613,
          309,
          13,
          50672
        ]
      },
      {
        "avg_logprob": -0.19143409263796923,
        "compression_ratio": 1.9137931034482758,
        "end": 211.06,
        "id": 80,
        "no_speech_prob": 0.00008481018448946998,
        "seek": 20254,
        "start": 208.7,
        "temperature": 0,
        "text": " This is what I did in the previous video.",
        "tokens": [
          50672,
          639,
          307,
          437,
          286,
          630,
          294,
          264,
          3894,
          960,
          13,
          50790
        ]
      },
      {
        "avg_logprob": -0.19143409263796923,
        "compression_ratio": 1.9137931034482758,
        "end": 213.1,
        "id": 81,
        "no_speech_prob": 0.00008481018448946998,
        "seek": 20254,
        "start": 211.06,
        "temperature": 0,
        "text": " And so now what I can do is I'll set up.",
        "tokens": [
          50790,
          400,
          370,
          586,
          437,
          286,
          393,
          360,
          307,
          286,
          603,
          992,
          493,
          13,
          50892
        ]
      },
      {
        "avg_logprob": -0.19143409263796923,
        "compression_ratio": 1.9137931034482758,
        "end": 215.62,
        "id": 82,
        "no_speech_prob": 0.00008481018448946998,
        "seek": 20254,
        "start": 213.1,
        "temperature": 0,
        "text": " I'm just going to make up an object called data.",
        "tokens": [
          50892,
          286,
          478,
          445,
          516,
          281,
          652,
          493,
          364,
          2657,
          1219,
          1412,
          13,
          51018
        ]
      },
      {
        "avg_logprob": -0.19143409263796923,
        "compression_ratio": 1.9137931034482758,
        "end": 217.98,
        "id": 83,
        "no_speech_prob": 0.00008481018448946998,
        "seek": 20254,
        "start": 215.62,
        "temperature": 0,
        "text": " And what I want to send is the latitude and longitude",
        "tokens": [
          51018,
          400,
          437,
          286,
          528,
          281,
          2845,
          307,
          264,
          45436,
          293,
          938,
          4377,
          51136
        ]
      },
      {
        "avg_logprob": -0.19143409263796923,
        "compression_ratio": 1.9137931034482758,
        "end": 220.01999999999998,
        "id": 84,
        "no_speech_prob": 0.00008481018448946998,
        "seek": 20254,
        "start": 217.98,
        "temperature": 0,
        "text": " inside of an object.",
        "tokens": [
          51136,
          1854,
          295,
          364,
          2657,
          13,
          51238
        ]
      },
      {
        "avg_logprob": -0.19143409263796923,
        "compression_ratio": 1.9137931034482758,
        "end": 221.38,
        "id": 85,
        "no_speech_prob": 0.00008481018448946998,
        "seek": 20254,
        "start": 220.01999999999998,
        "temperature": 0,
        "text": " So that's what I want to send.",
        "tokens": [
          51238,
          407,
          300,
          311,
          437,
          286,
          528,
          281,
          2845,
          13,
          51306
        ]
      },
      {
        "avg_logprob": -0.19143409263796923,
        "compression_ratio": 1.9137931034482758,
        "end": 223.42,
        "id": 86,
        "no_speech_prob": 0.00008481018448946998,
        "seek": 20254,
        "start": 221.38,
        "temperature": 0,
        "text": " So you might think all I need to do,",
        "tokens": [
          51306,
          407,
          291,
          1062,
          519,
          439,
          286,
          643,
          281,
          360,
          11,
          51408
        ]
      },
      {
        "avg_logprob": -0.19143409263796923,
        "compression_ratio": 1.9137931034482758,
        "end": 225.73999999999998,
        "id": 87,
        "no_speech_prob": 0.00008481018448946998,
        "seek": 20254,
        "start": 223.42,
        "temperature": 0,
        "text": " if I were just doing a regular fetch,",
        "tokens": [
          51408,
          498,
          286,
          645,
          445,
          884,
          257,
          3890,
          23673,
          11,
          51524
        ]
      },
      {
        "avg_logprob": -0.19143409263796923,
        "compression_ratio": 1.9137931034482758,
        "end": 229.1,
        "id": 88,
        "no_speech_prob": 0.00008481018448946998,
        "seek": 20254,
        "start": 225.73999999999998,
        "temperature": 0,
        "text": " I just wanted to fetch something that's a regular get request",
        "tokens": [
          51524,
          286,
          445,
          1415,
          281,
          23673,
          746,
          300,
          311,
          257,
          3890,
          483,
          5308,
          51692
        ]
      },
      {
        "avg_logprob": -0.19143409263796923,
        "compression_ratio": 1.9137931034482758,
        "end": 231.78,
        "id": 89,
        "no_speech_prob": 0.00008481018448946998,
        "seek": 20254,
        "start": 229.1,
        "temperature": 0,
        "text": " like I did in previous videos, I would then just say this.",
        "tokens": [
          51692,
          411,
          286,
          630,
          294,
          3894,
          2145,
          11,
          286,
          576,
          550,
          445,
          584,
          341,
          13,
          51826
        ]
      },
      {
        "avg_logprob": -0.193441874360385,
        "compression_ratio": 1.6513157894736843,
        "end": 237.01999999999998,
        "id": 90,
        "no_speech_prob": 0.00004331857417128049,
        "seek": 23254,
        "start": 233.54,
        "temperature": 0,
        "text": " Hey, fetch me the stuff from slash API.",
        "tokens": [
          50414,
          1911,
          11,
          23673,
          385,
          264,
          1507,
          490,
          17330,
          9362,
          13,
          50588
        ]
      },
      {
        "avg_logprob": -0.193441874360385,
        "compression_ratio": 1.6513157894736843,
        "end": 238.98,
        "id": 91,
        "no_speech_prob": 0.00004331857417128049,
        "seek": 23254,
        "start": 237.01999999999998,
        "temperature": 0,
        "text": " And I might actually do that a little later",
        "tokens": [
          50588,
          400,
          286,
          1062,
          767,
          360,
          300,
          257,
          707,
          1780,
          50686
        ]
      },
      {
        "avg_logprob": -0.193441874360385,
        "compression_ratio": 1.6513157894736843,
        "end": 240.78,
        "id": 92,
        "no_speech_prob": 0.00004331857417128049,
        "seek": 23254,
        "start": 238.98,
        "temperature": 0,
        "text": " when I want to just do a get request",
        "tokens": [
          50686,
          562,
          286,
          528,
          281,
          445,
          360,
          257,
          483,
          5308,
          50776
        ]
      },
      {
        "avg_logprob": -0.193441874360385,
        "compression_ratio": 1.6513157894736843,
        "end": 242.5,
        "id": 93,
        "no_speech_prob": 0.00004331857417128049,
        "seek": 23254,
        "start": 240.78,
        "temperature": 0,
        "text": " and get the data that's in the database.",
        "tokens": [
          50776,
          293,
          483,
          264,
          1412,
          300,
          311,
          294,
          264,
          8149,
          13,
          50862
        ]
      },
      {
        "avg_logprob": -0.193441874360385,
        "compression_ratio": 1.6513157894736843,
        "end": 245.34,
        "id": 94,
        "no_speech_prob": 0.00004331857417128049,
        "seek": 23254,
        "start": 242.5,
        "temperature": 0,
        "text": " But right now, what I want to do is send a post.",
        "tokens": [
          50862,
          583,
          558,
          586,
          11,
          437,
          286,
          528,
          281,
          360,
          307,
          2845,
          257,
          2183,
          13,
          51004
        ]
      },
      {
        "avg_logprob": -0.193441874360385,
        "compression_ratio": 1.6513157894736843,
        "end": 249.38,
        "id": 95,
        "no_speech_prob": 0.00004331857417128049,
        "seek": 23254,
        "start": 245.34,
        "temperature": 0,
        "text": " In order to send a post, I need a second argument here,",
        "tokens": [
          51004,
          682,
          1668,
          281,
          2845,
          257,
          2183,
          11,
          286,
          643,
          257,
          1150,
          6770,
          510,
          11,
          51206
        ]
      },
      {
        "avg_logprob": -0.193441874360385,
        "compression_ratio": 1.6513157894736843,
        "end": 251.01999999999998,
        "id": 96,
        "no_speech_prob": 0.00004331857417128049,
        "seek": 23254,
        "start": 249.38,
        "temperature": 0,
        "text": " which is just a JavaScript object.",
        "tokens": [
          51206,
          597,
          307,
          445,
          257,
          15778,
          2657,
          13,
          51288
        ]
      },
      {
        "avg_logprob": -0.193441874360385,
        "compression_ratio": 1.6513157894736843,
        "end": 252.57999999999998,
        "id": 97,
        "no_speech_prob": 0.00004331857417128049,
        "seek": 23254,
        "start": 251.01999999999998,
        "temperature": 0,
        "text": " I'm going to call it options.",
        "tokens": [
          51288,
          286,
          478,
          516,
          281,
          818,
          309,
          3956,
          13,
          51366
        ]
      },
      {
        "avg_logprob": -0.193441874360385,
        "compression_ratio": 1.6513157894736843,
        "end": 255.14,
        "id": 98,
        "no_speech_prob": 0.00004331857417128049,
        "seek": 23254,
        "start": 252.57999999999998,
        "temperature": 0,
        "text": " And that way, I can set it up separately as a variable",
        "tokens": [
          51366,
          400,
          300,
          636,
          11,
          286,
          393,
          992,
          309,
          493,
          14759,
          382,
          257,
          7006,
          51494
        ]
      },
      {
        "avg_logprob": -0.193441874360385,
        "compression_ratio": 1.6513157894736843,
        "end": 257.21999999999997,
        "id": 99,
        "no_speech_prob": 0.00004331857417128049,
        "seek": 23254,
        "start": 255.14,
        "temperature": 0,
        "text": " and kind of examine it more closely.",
        "tokens": [
          51494,
          293,
          733,
          295,
          17496,
          309,
          544,
          8185,
          13,
          51598
        ]
      },
      {
        "avg_logprob": -0.193441874360385,
        "compression_ratio": 1.6513157894736843,
        "end": 260.1,
        "id": 100,
        "no_speech_prob": 0.00004331857417128049,
        "seek": 23254,
        "start": 257.21999999999997,
        "temperature": 0,
        "text": " The first property that I need to put in options",
        "tokens": [
          51598,
          440,
          700,
          4707,
          300,
          286,
          643,
          281,
          829,
          294,
          3956,
          51742
        ]
      },
      {
        "avg_logprob": -0.193441874360385,
        "compression_ratio": 1.6513157894736843,
        "end": 262.02,
        "id": 101,
        "no_speech_prob": 0.00004331857417128049,
        "seek": 23254,
        "start": 260.1,
        "temperature": 0,
        "text": " is the method that I'm using.",
        "tokens": [
          51742,
          307,
          264,
          3170,
          300,
          286,
          478,
          1228,
          13,
          51838
        ]
      },
      {
        "avg_logprob": -0.214247178058235,
        "compression_ratio": 1.698961937716263,
        "end": 265.26,
        "id": 102,
        "no_speech_prob": 0.000503318035043776,
        "seek": 26202,
        "start": 262.02,
        "temperature": 0,
        "text": " And that method is a post.",
        "tokens": [
          50364,
          400,
          300,
          3170,
          307,
          257,
          2183,
          13,
          50526
        ]
      },
      {
        "avg_logprob": -0.214247178058235,
        "compression_ratio": 1.698961937716263,
        "end": 266.62,
        "id": 103,
        "no_speech_prob": 0.000503318035043776,
        "seek": 26202,
        "start": 265.26,
        "temperature": 0,
        "text": " Now, there's a lot of information",
        "tokens": [
          50526,
          823,
          11,
          456,
          311,
          257,
          688,
          295,
          1589,
          50594
        ]
      },
      {
        "avg_logprob": -0.214247178058235,
        "compression_ratio": 1.698961937716263,
        "end": 268.7,
        "id": 104,
        "no_speech_prob": 0.000503318035043776,
        "seek": 26202,
        "start": 266.62,
        "temperature": 0,
        "text": " that you could put here under options.",
        "tokens": [
          50594,
          300,
          291,
          727,
          829,
          510,
          833,
          3956,
          13,
          50698
        ]
      },
      {
        "avg_logprob": -0.214247178058235,
        "compression_ratio": 1.698961937716263,
        "end": 270.78,
        "id": 105,
        "no_speech_prob": 0.000503318035043776,
        "seek": 26202,
        "start": 268.7,
        "temperature": 0,
        "text": " And if you want to look at all the possibilities,",
        "tokens": [
          50698,
          400,
          498,
          291,
          528,
          281,
          574,
          412,
          439,
          264,
          12178,
          11,
          50802
        ]
      },
      {
        "avg_logprob": -0.214247178058235,
        "compression_ratio": 1.698961937716263,
        "end": 273.18,
        "id": 106,
        "no_speech_prob": 0.000503318035043776,
        "seek": 26202,
        "start": 270.78,
        "temperature": 0,
        "text": " I refer you back to the MDN web docs.",
        "tokens": [
          50802,
          286,
          2864,
          291,
          646,
          281,
          264,
          22521,
          45,
          3670,
          45623,
          13,
          50922
        ]
      },
      {
        "avg_logprob": -0.214247178058235,
        "compression_ratio": 1.698961937716263,
        "end": 275.5,
        "id": 107,
        "no_speech_prob": 0.000503318035043776,
        "seek": 26202,
        "start": 273.18,
        "temperature": 0,
        "text": " There's a nice page here about fetch.",
        "tokens": [
          50922,
          821,
          311,
          257,
          1481,
          3028,
          510,
          466,
          23673,
          13,
          51038
        ]
      },
      {
        "avg_logprob": -0.214247178058235,
        "compression_ratio": 1.698961937716263,
        "end": 277.14,
        "id": 108,
        "no_speech_prob": 0.000503318035043776,
        "seek": 26202,
        "start": 275.5,
        "temperature": 0,
        "text": " And we can kind of see all this stuff.",
        "tokens": [
          51038,
          400,
          321,
          393,
          733,
          295,
          536,
          439,
          341,
          1507,
          13,
          51120
        ]
      },
      {
        "avg_logprob": -0.214247178058235,
        "compression_ratio": 1.698961937716263,
        "end": 279.97999999999996,
        "id": 109,
        "no_speech_prob": 0.000503318035043776,
        "seek": 26202,
        "start": 277.14,
        "temperature": 0,
        "text": " OK, what mode and cache and credentials.",
        "tokens": [
          51120,
          2264,
          11,
          437,
          4391,
          293,
          19459,
          293,
          27404,
          13,
          51262
        ]
      },
      {
        "avg_logprob": -0.214247178058235,
        "compression_ratio": 1.698961937716263,
        "end": 283.34,
        "id": 110,
        "no_speech_prob": 0.000503318035043776,
        "seek": 26202,
        "start": 279.97999999999996,
        "temperature": 0,
        "text": " But really what I need, what I want, the key thing that I want",
        "tokens": [
          51262,
          583,
          534,
          437,
          286,
          643,
          11,
          437,
          286,
          528,
          11,
          264,
          2141,
          551,
          300,
          286,
          528,
          51430
        ]
      },
      {
        "avg_logprob": -0.214247178058235,
        "compression_ratio": 1.698961937716263,
        "end": 284.18,
        "id": 111,
        "no_speech_prob": 0.000503318035043776,
        "seek": 26202,
        "start": 283.34,
        "temperature": 0,
        "text": " is body.",
        "tokens": [
          51430,
          307,
          1772,
          13,
          51472
        ]
      },
      {
        "avg_logprob": -0.214247178058235,
        "compression_ratio": 1.698961937716263,
        "end": 287.18,
        "id": 112,
        "no_speech_prob": 0.000503318035043776,
        "seek": 26202,
        "start": 284.18,
        "temperature": 0,
        "text": " So the body of the post request is where",
        "tokens": [
          51472,
          407,
          264,
          1772,
          295,
          264,
          2183,
          5308,
          307,
          689,
          51622
        ]
      },
      {
        "avg_logprob": -0.214247178058235,
        "compression_ratio": 1.698961937716263,
        "end": 289.85999999999996,
        "id": 113,
        "no_speech_prob": 0.000503318035043776,
        "seek": 26202,
        "start": 287.18,
        "temperature": 0,
        "text": " I'm packaging up all of my data.",
        "tokens": [
          51622,
          286,
          478,
          16836,
          493,
          439,
          295,
          452,
          1412,
          13,
          51756
        ]
      },
      {
        "avg_logprob": -0.214247178058235,
        "compression_ratio": 1.698961937716263,
        "end": 291.53999999999996,
        "id": 114,
        "no_speech_prob": 0.000503318035043776,
        "seek": 26202,
        "start": 289.85999999999996,
        "temperature": 0,
        "text": " And you can see here, even though there",
        "tokens": [
          51756,
          400,
          291,
          393,
          536,
          510,
          11,
          754,
          1673,
          456,
          51840
        ]
      },
      {
        "avg_logprob": -0.19576481089872472,
        "compression_ratio": 1.581151832460733,
        "end": 294.42,
        "id": 115,
        "no_speech_prob": 0.00011235283454880118,
        "seek": 29154,
        "start": 291.54,
        "temperature": 0,
        "text": " are different ways to send the data, that what I want to do",
        "tokens": [
          50364,
          366,
          819,
          2098,
          281,
          2845,
          264,
          1412,
          11,
          300,
          437,
          286,
          528,
          281,
          360,
          50508
        ]
      },
      {
        "avg_logprob": -0.19576481089872472,
        "compression_ratio": 1.581151832460733,
        "end": 297.06,
        "id": 116,
        "no_speech_prob": 0.00011235283454880118,
        "seek": 29154,
        "start": 294.42,
        "temperature": 0,
        "text": " is send it as a string text.",
        "tokens": [
          50508,
          307,
          2845,
          309,
          382,
          257,
          6798,
          2487,
          13,
          50640
        ]
      },
      {
        "avg_logprob": -0.19576481089872472,
        "compression_ratio": 1.581151832460733,
        "end": 301.42,
        "id": 117,
        "no_speech_prob": 0.00011235283454880118,
        "seek": 29154,
        "start": 297.06,
        "temperature": 0,
        "text": " I want to stringify, take the JavaScript object data,",
        "tokens": [
          50640,
          286,
          528,
          281,
          6798,
          2505,
          11,
          747,
          264,
          15778,
          2657,
          1412,
          11,
          50858
        ]
      },
      {
        "avg_logprob": -0.19576481089872472,
        "compression_ratio": 1.581151832460733,
        "end": 303.74,
        "id": 118,
        "no_speech_prob": 0.00011235283454880118,
        "seek": 29154,
        "start": 301.42,
        "temperature": 0,
        "text": " and make it into a JSON string.",
        "tokens": [
          50858,
          293,
          652,
          309,
          666,
          257,
          31828,
          6798,
          13,
          50974
        ]
      },
      {
        "avg_logprob": -0.19576481089872472,
        "compression_ratio": 1.581151832460733,
        "end": 306.58000000000004,
        "id": 119,
        "no_speech_prob": 0.00011235283454880118,
        "seek": 29154,
        "start": 303.74,
        "temperature": 0,
        "text": " So I want exactly this right here.",
        "tokens": [
          50974,
          407,
          286,
          528,
          2293,
          341,
          558,
          510,
          13,
          51116
        ]
      },
      {
        "avg_logprob": -0.19576481089872472,
        "compression_ratio": 1.581151832460733,
        "end": 313.62,
        "id": 120,
        "no_speech_prob": 0.00011235283454880118,
        "seek": 29154,
        "start": 310.62,
        "temperature": 0,
        "text": " And there's one other piece that's important.",
        "tokens": [
          51318,
          400,
          456,
          311,
          472,
          661,
          2522,
          300,
          311,
          1021,
          13,
          51468
        ]
      },
      {
        "avg_logprob": -0.19576481089872472,
        "compression_ratio": 1.581151832460733,
        "end": 318.14000000000004,
        "id": 121,
        "no_speech_prob": 0.00011235283454880118,
        "seek": 29154,
        "start": 313.62,
        "temperature": 0,
        "text": " I am specifically sending data in JSON format.",
        "tokens": [
          51468,
          286,
          669,
          4682,
          7750,
          1412,
          294,
          31828,
          7877,
          13,
          51694
        ]
      },
      {
        "avg_logprob": -0.19935106852697948,
        "compression_ratio": 1.6215277777777777,
        "end": 322.26,
        "id": 122,
        "no_speech_prob": 0.0000681482270010747,
        "seek": 31814,
        "start": 318.14,
        "temperature": 0,
        "text": " So it's useful to specify that in something called a header.",
        "tokens": [
          50364,
          407,
          309,
          311,
          4420,
          281,
          16500,
          300,
          294,
          746,
          1219,
          257,
          23117,
          13,
          50570
        ]
      },
      {
        "avg_logprob": -0.19935106852697948,
        "compression_ratio": 1.6215277777777777,
        "end": 325.62,
        "id": 123,
        "no_speech_prob": 0.0000681482270010747,
        "seek": 31814,
        "start": 322.26,
        "temperature": 0,
        "text": " A header is something that can be packaged along",
        "tokens": [
          50570,
          316,
          23117,
          307,
          746,
          300,
          393,
          312,
          38162,
          2051,
          50738
        ]
      },
      {
        "avg_logprob": -0.19935106852697948,
        "compression_ratio": 1.6215277777777777,
        "end": 327.5,
        "id": 124,
        "no_speech_prob": 0.0000681482270010747,
        "seek": 31814,
        "start": 325.62,
        "temperature": 0,
        "text": " with any post or get request that's",
        "tokens": [
          50738,
          365,
          604,
          2183,
          420,
          483,
          5308,
          300,
          311,
          50832
        ]
      },
      {
        "avg_logprob": -0.19935106852697948,
        "compression_ratio": 1.6215277777777777,
        "end": 329.3,
        "id": 125,
        "no_speech_prob": 0.0000681482270010747,
        "seek": 31814,
        "start": 327.5,
        "temperature": 0,
        "text": " moving between client and server.",
        "tokens": [
          50832,
          2684,
          1296,
          6423,
          293,
          7154,
          13,
          50922
        ]
      },
      {
        "avg_logprob": -0.19935106852697948,
        "compression_ratio": 1.6215277777777777,
        "end": 331.65999999999997,
        "id": 126,
        "no_speech_prob": 0.0000681482270010747,
        "seek": 31814,
        "start": 329.3,
        "temperature": 0,
        "text": " And it's a way of adding some additional meta information.",
        "tokens": [
          50922,
          400,
          309,
          311,
          257,
          636,
          295,
          5127,
          512,
          4497,
          19616,
          1589,
          13,
          51040
        ]
      },
      {
        "avg_logprob": -0.19935106852697948,
        "compression_ratio": 1.6215277777777777,
        "end": 336.02,
        "id": 127,
        "no_speech_prob": 0.0000681482270010747,
        "seek": 31814,
        "start": 331.65999999999997,
        "temperature": 0,
        "text": " You can read more about this also on the Mozilla docs.",
        "tokens": [
          51040,
          509,
          393,
          1401,
          544,
          466,
          341,
          611,
          322,
          264,
          3335,
          26403,
          45623,
          13,
          51258
        ]
      },
      {
        "avg_logprob": -0.19935106852697948,
        "compression_ratio": 1.6215277777777777,
        "end": 338.58,
        "id": 128,
        "no_speech_prob": 0.0000681482270010747,
        "seek": 31814,
        "start": 336.02,
        "temperature": 0,
        "text": " Headers have a name, a colon, and a value.",
        "tokens": [
          51258,
          11398,
          433,
          362,
          257,
          1315,
          11,
          257,
          8255,
          11,
          293,
          257,
          2158,
          13,
          51386
        ]
      },
      {
        "avg_logprob": -0.19935106852697948,
        "compression_ratio": 1.6215277777777777,
        "end": 340.78,
        "id": 129,
        "no_speech_prob": 0.0000681482270010747,
        "seek": 31814,
        "start": 338.58,
        "temperature": 0,
        "text": " And so the one that I want to use here,",
        "tokens": [
          51386,
          400,
          370,
          264,
          472,
          300,
          286,
          528,
          281,
          764,
          510,
          11,
          51496
        ]
      },
      {
        "avg_logprob": -0.19935106852697948,
        "compression_ratio": 1.6215277777777777,
        "end": 342.3,
        "id": 130,
        "no_speech_prob": 0.0000681482270010747,
        "seek": 31814,
        "start": 340.78,
        "temperature": 0,
        "text": " I can actually just go and grab it",
        "tokens": [
          51496,
          286,
          393,
          767,
          445,
          352,
          293,
          4444,
          309,
          51572
        ]
      },
      {
        "avg_logprob": -0.19935106852697948,
        "compression_ratio": 1.6215277777777777,
        "end": 346.82,
        "id": 131,
        "no_speech_prob": 0.0000681482270010747,
        "seek": 31814,
        "start": 342.3,
        "temperature": 0,
        "text": " from here, which is this content type application JSON.",
        "tokens": [
          51572,
          490,
          510,
          11,
          597,
          307,
          341,
          2701,
          2010,
          3861,
          31828,
          13,
          51798
        ]
      },
      {
        "avg_logprob": -0.17690342030626663,
        "compression_ratio": 1.7044534412955465,
        "end": 348.7,
        "id": 132,
        "no_speech_prob": 0.00004400102261570282,
        "seek": 34682,
        "start": 346.86,
        "temperature": 0,
        "text": " So I'm going to grab this.",
        "tokens": [
          50366,
          407,
          286,
          478,
          516,
          281,
          4444,
          341,
          13,
          50458
        ]
      },
      {
        "avg_logprob": -0.17690342030626663,
        "compression_ratio": 1.7044534412955465,
        "end": 352.09999999999997,
        "id": 133,
        "no_speech_prob": 0.00004400102261570282,
        "seek": 34682,
        "start": 348.7,
        "temperature": 0,
        "text": " And I'm also going to put this here in my code now.",
        "tokens": [
          50458,
          400,
          286,
          478,
          611,
          516,
          281,
          829,
          341,
          510,
          294,
          452,
          3089,
          586,
          13,
          50628
        ]
      },
      {
        "avg_logprob": -0.17690342030626663,
        "compression_ratio": 1.7044534412955465,
        "end": 354.06,
        "id": 134,
        "no_speech_prob": 0.00004400102261570282,
        "seek": 34682,
        "start": 352.09999999999997,
        "temperature": 0,
        "text": " And I don't need this.",
        "tokens": [
          50628,
          400,
          286,
          500,
          380,
          643,
          341,
          13,
          50726
        ]
      },
      {
        "avg_logprob": -0.17690342030626663,
        "compression_ratio": 1.7044534412955465,
        "end": 357.58,
        "id": 135,
        "no_speech_prob": 0.00004400102261570282,
        "seek": 34682,
        "start": 354.06,
        "temperature": 0,
        "text": " So even though there's more that I could put,",
        "tokens": [
          50726,
          407,
          754,
          1673,
          456,
          311,
          544,
          300,
          286,
          727,
          829,
          11,
          50902
        ]
      },
      {
        "avg_logprob": -0.17690342030626663,
        "compression_ratio": 1.7044534412955465,
        "end": 360.18,
        "id": 136,
        "no_speech_prob": 0.00004400102261570282,
        "seek": 34682,
        "start": 357.58,
        "temperature": 0,
        "text": " the basic pieces that I need are, hey,",
        "tokens": [
          50902,
          264,
          3875,
          3755,
          300,
          286,
          643,
          366,
          11,
          4177,
          11,
          51032
        ]
      },
      {
        "avg_logprob": -0.17690342030626663,
        "compression_ratio": 1.7044534412955465,
        "end": 363.65999999999997,
        "id": 137,
        "no_speech_prob": 0.00004400102261570282,
        "seek": 34682,
        "start": 360.18,
        "temperature": 0,
        "text": " I want this data to get sent as JSON.",
        "tokens": [
          51032,
          286,
          528,
          341,
          1412,
          281,
          483,
          2279,
          382,
          31828,
          13,
          51206
        ]
      },
      {
        "avg_logprob": -0.17690342030626663,
        "compression_ratio": 1.7044534412955465,
        "end": 365.86,
        "id": 138,
        "no_speech_prob": 0.00004400102261570282,
        "seek": 34682,
        "start": 363.65999999999997,
        "temperature": 0,
        "text": " I want to tell you that it's going to be JSON.",
        "tokens": [
          51206,
          286,
          528,
          281,
          980,
          291,
          300,
          309,
          311,
          516,
          281,
          312,
          31828,
          13,
          51316
        ]
      },
      {
        "avg_logprob": -0.17690342030626663,
        "compression_ratio": 1.7044534412955465,
        "end": 368.06,
        "id": 139,
        "no_speech_prob": 0.00004400102261570282,
        "seek": 34682,
        "start": 365.86,
        "temperature": 0,
        "text": " And I want to post it to the API.",
        "tokens": [
          51316,
          400,
          286,
          528,
          281,
          2183,
          309,
          281,
          264,
          9362,
          13,
          51426
        ]
      },
      {
        "avg_logprob": -0.17690342030626663,
        "compression_ratio": 1.7044534412955465,
        "end": 369.74,
        "id": 140,
        "no_speech_prob": 0.00004400102261570282,
        "seek": 34682,
        "start": 368.06,
        "temperature": 0,
        "text": " So now we're good, right?",
        "tokens": [
          51426,
          407,
          586,
          321,
          434,
          665,
          11,
          558,
          30,
          51510
        ]
      },
      {
        "avg_logprob": -0.17690342030626663,
        "compression_ratio": 1.7044534412955465,
        "end": 372.21999999999997,
        "id": 141,
        "no_speech_prob": 0.00004400102261570282,
        "seek": 34682,
        "start": 369.74,
        "temperature": 0,
        "text": " So I get the geolocation data.",
        "tokens": [
          51510,
          407,
          286,
          483,
          264,
          1519,
          401,
          27943,
          1412,
          13,
          51634
        ]
      },
      {
        "avg_logprob": -0.17690342030626663,
        "compression_ratio": 1.7044534412955465,
        "end": 374.53999999999996,
        "id": 142,
        "no_speech_prob": 0.00004400102261570282,
        "seek": 34682,
        "start": 372.21999999999997,
        "temperature": 0,
        "text": " I put it into a JavaScript object.",
        "tokens": [
          51634,
          286,
          829,
          309,
          666,
          257,
          15778,
          2657,
          13,
          51750
        ]
      },
      {
        "avg_logprob": -0.17690342030626663,
        "compression_ratio": 1.7044534412955465,
        "end": 375.78,
        "id": 143,
        "no_speech_prob": 0.00004400102261570282,
        "seek": 34682,
        "start": 374.53999999999996,
        "temperature": 0,
        "text": " I package it as a post.",
        "tokens": [
          51750,
          286,
          7372,
          309,
          382,
          257,
          2183,
          13,
          51812
        ]
      },
      {
        "avg_logprob": -0.2486324041662082,
        "compression_ratio": 1.834008097165992,
        "end": 377.73999999999995,
        "id": 144,
        "no_speech_prob": 0.0004044781962875277,
        "seek": 37578,
        "start": 375.78,
        "temperature": 0,
        "text": " And I send it to my endpoint.",
        "tokens": [
          50364,
          400,
          286,
          2845,
          309,
          281,
          452,
          35795,
          13,
          50462
        ]
      },
      {
        "avg_logprob": -0.2486324041662082,
        "compression_ratio": 1.834008097165992,
        "end": 381.5,
        "id": 145,
        "no_speech_prob": 0.0004044781962875277,
        "seek": 37578,
        "start": 377.73999999999995,
        "temperature": 0,
        "text": " And in the server, I receive and console log it.",
        "tokens": [
          50462,
          400,
          294,
          264,
          7154,
          11,
          286,
          4774,
          293,
          11076,
          3565,
          309,
          13,
          50650
        ]
      },
      {
        "avg_logprob": -0.2486324041662082,
        "compression_ratio": 1.834008097165992,
        "end": 382.7,
        "id": 146,
        "no_speech_prob": 0.0004044781962875277,
        "seek": 37578,
        "start": 381.5,
        "temperature": 0,
        "text": " Let's see what happens.",
        "tokens": [
          50650,
          961,
          311,
          536,
          437,
          2314,
          13,
          50710
        ]
      },
      {
        "avg_logprob": -0.2486324041662082,
        "compression_ratio": 1.834008097165992,
        "end": 385.94,
        "id": 147,
        "no_speech_prob": 0.0004044781962875277,
        "seek": 37578,
        "start": 382.7,
        "temperature": 0,
        "text": " So I'm going to say node index.js to run the server.",
        "tokens": [
          50710,
          407,
          286,
          478,
          516,
          281,
          584,
          9984,
          8186,
          13,
          25530,
          281,
          1190,
          264,
          7154,
          13,
          50872
        ]
      },
      {
        "avg_logprob": -0.2486324041662082,
        "compression_ratio": 1.834008097165992,
        "end": 387.58,
        "id": 148,
        "no_speech_prob": 0.0004044781962875277,
        "seek": 37578,
        "start": 385.94,
        "temperature": 0,
        "text": " I'm going to go back to the browser.",
        "tokens": [
          50872,
          286,
          478,
          516,
          281,
          352,
          646,
          281,
          264,
          11185,
          13,
          50954
        ]
      },
      {
        "avg_logprob": -0.2486324041662082,
        "compression_ratio": 1.834008097165992,
        "end": 389.78,
        "id": 149,
        "no_speech_prob": 0.0004044781962875277,
        "seek": 37578,
        "start": 387.58,
        "temperature": 0,
        "text": " I'm going to refresh my page.",
        "tokens": [
          50954,
          286,
          478,
          516,
          281,
          15134,
          452,
          3028,
          13,
          51064
        ]
      },
      {
        "avg_logprob": -0.2486324041662082,
        "compression_ratio": 1.834008097165992,
        "end": 392.85999999999996,
        "id": 150,
        "no_speech_prob": 0.0004044781962875277,
        "seek": 37578,
        "start": 389.78,
        "temperature": 0,
        "text": " I probably put some console logging in the client",
        "tokens": [
          51064,
          286,
          1391,
          829,
          512,
          11076,
          27991,
          294,
          264,
          6423,
          51218
        ]
      },
      {
        "avg_logprob": -0.2486324041662082,
        "compression_ratio": 1.834008097165992,
        "end": 393.65999999999997,
        "id": 151,
        "no_speech_prob": 0.0004044781962875277,
        "seek": 37578,
        "start": 392.85999999999996,
        "temperature": 0,
        "text": " to see what's there.",
        "tokens": [
          51218,
          281,
          536,
          437,
          311,
          456,
          13,
          51258
        ]
      },
      {
        "avg_logprob": -0.2486324041662082,
        "compression_ratio": 1.834008097165992,
        "end": 395.82,
        "id": 152,
        "no_speech_prob": 0.0004044781962875277,
        "seek": 37578,
        "start": 393.65999999999997,
        "temperature": 0,
        "text": " But presumably now it's sensing the server.",
        "tokens": [
          51258,
          583,
          26742,
          586,
          309,
          311,
          30654,
          264,
          7154,
          13,
          51366
        ]
      },
      {
        "avg_logprob": -0.2486324041662082,
        "compression_ratio": 1.834008097165992,
        "end": 397.14,
        "id": 153,
        "no_speech_prob": 0.0004044781962875277,
        "seek": 37578,
        "start": 395.82,
        "temperature": 0,
        "text": " I don't see an error here.",
        "tokens": [
          51366,
          286,
          500,
          380,
          536,
          364,
          6713,
          510,
          13,
          51432
        ]
      },
      {
        "avg_logprob": -0.2486324041662082,
        "compression_ratio": 1.834008097165992,
        "end": 399.58,
        "id": 154,
        "no_speech_prob": 0.0004044781962875277,
        "seek": 37578,
        "start": 397.14,
        "temperature": 0,
        "text": " If I go back to the server, oh, whoa.",
        "tokens": [
          51432,
          759,
          286,
          352,
          646,
          281,
          264,
          7154,
          11,
          1954,
          11,
          13310,
          13,
          51554
        ]
      },
      {
        "avg_logprob": -0.2486324041662082,
        "compression_ratio": 1.834008097165992,
        "end": 401.21999999999997,
        "id": 155,
        "no_speech_prob": 0.0004044781962875277,
        "seek": 37578,
        "start": 399.58,
        "temperature": 0,
        "text": " So this is good news.",
        "tokens": [
          51554,
          407,
          341,
          307,
          665,
          2583,
          13,
          51636
        ]
      },
      {
        "avg_logprob": -0.2486324041662082,
        "compression_ratio": 1.834008097165992,
        "end": 405.29999999999995,
        "id": 156,
        "no_speech_prob": 0.0004044781962875277,
        "seek": 37578,
        "start": 401.21999999999997,
        "temperature": 0,
        "text": " It console logged something.",
        "tokens": [
          51636,
          467,
          11076,
          27231,
          746,
          13,
          51840
        ]
      },
      {
        "avg_logprob": -0.21574387430142955,
        "compression_ratio": 1.6053639846743295,
        "end": 407.02000000000004,
        "id": 157,
        "no_speech_prob": 0.000219952009501867,
        "seek": 40530,
        "start": 405.3,
        "temperature": 0,
        "text": " As you might have noticed, there's",
        "tokens": [
          50364,
          1018,
          291,
          1062,
          362,
          5694,
          11,
          456,
          311,
          50450
        ]
      },
      {
        "avg_logprob": -0.21574387430142955,
        "compression_ratio": 1.6053639846743295,
        "end": 411.06,
        "id": 158,
        "no_speech_prob": 0.000219952009501867,
        "seek": 40530,
        "start": 407.02000000000004,
        "temperature": 0,
        "text": " a ton of information here as I scroll through this crazily.",
        "tokens": [
          50450,
          257,
          2952,
          295,
          1589,
          510,
          382,
          286,
          11369,
          807,
          341,
          46348,
          953,
          13,
          50652
        ]
      },
      {
        "avg_logprob": -0.21574387430142955,
        "compression_ratio": 1.6053639846743295,
        "end": 413.3,
        "id": 159,
        "no_speech_prob": 0.000219952009501867,
        "seek": 40530,
        "start": 411.06,
        "temperature": 0,
        "text": " While there might be points at which",
        "tokens": [
          50652,
          3987,
          456,
          1062,
          312,
          2793,
          412,
          597,
          50764
        ]
      },
      {
        "avg_logprob": -0.21574387430142955,
        "compression_ratio": 1.6053639846743295,
        "end": 417.34000000000003,
        "id": 160,
        "no_speech_prob": 0.000219952009501867,
        "seek": 40530,
        "start": 413.3,
        "temperature": 0,
        "text": " I need to examine certain aspects of all of the metadata",
        "tokens": [
          50764,
          286,
          643,
          281,
          17496,
          1629,
          7270,
          295,
          439,
          295,
          264,
          26603,
          50966
        ]
      },
      {
        "avg_logprob": -0.21574387430142955,
        "compression_ratio": 1.6053639846743295,
        "end": 419.78000000000003,
        "id": 161,
        "no_speech_prob": 0.000219952009501867,
        "seek": 40530,
        "start": 417.34000000000003,
        "temperature": 0,
        "text": " that's part of the request, all I really want to look at",
        "tokens": [
          50966,
          300,
          311,
          644,
          295,
          264,
          5308,
          11,
          439,
          286,
          534,
          528,
          281,
          574,
          412,
          51088
        ]
      },
      {
        "avg_logprob": -0.21574387430142955,
        "compression_ratio": 1.6053639846743295,
        "end": 420.90000000000003,
        "id": 162,
        "no_speech_prob": 0.000219952009501867,
        "seek": 40530,
        "start": 419.78000000000003,
        "temperature": 0,
        "text": " is the body.",
        "tokens": [
          51088,
          307,
          264,
          1772,
          13,
          51144
        ]
      },
      {
        "avg_logprob": -0.21574387430142955,
        "compression_ratio": 1.6053639846743295,
        "end": 425.66,
        "id": 163,
        "no_speech_prob": 0.000219952009501867,
        "seek": 40530,
        "start": 420.90000000000003,
        "temperature": 0,
        "text": " So let me now say console log request.body.",
        "tokens": [
          51144,
          407,
          718,
          385,
          586,
          584,
          11076,
          3565,
          5308,
          13,
          1067,
          13,
          51382
        ]
      },
      {
        "avg_logprob": -0.21574387430142955,
        "compression_ratio": 1.6053639846743295,
        "end": 430.22,
        "id": 164,
        "no_speech_prob": 0.000219952009501867,
        "seek": 40530,
        "start": 428.18,
        "temperature": 0,
        "text": " Now, one thing I should mention, by the way,",
        "tokens": [
          51508,
          823,
          11,
          472,
          551,
          286,
          820,
          2152,
          11,
          538,
          264,
          636,
          11,
          51610
        ]
      },
      {
        "avg_logprob": -0.21574387430142955,
        "compression_ratio": 1.6053639846743295,
        "end": 434.14,
        "id": 165,
        "no_speech_prob": 0.000219952009501867,
        "seek": 40530,
        "start": 430.22,
        "temperature": 0,
        "text": " is notice how I have to rerun the server every single time",
        "tokens": [
          51610,
          307,
          3449,
          577,
          286,
          362,
          281,
          43819,
          409,
          264,
          7154,
          633,
          2167,
          565,
          51806
        ]
      },
      {
        "avg_logprob": -0.21574387430142955,
        "compression_ratio": 1.6053639846743295,
        "end": 434.94,
        "id": 166,
        "no_speech_prob": 0.000219952009501867,
        "seek": 40530,
        "start": 434.14,
        "temperature": 0,
        "text": " I change it.",
        "tokens": [
          51806,
          286,
          1319,
          309,
          13,
          51846
        ]
      },
      {
        "avg_logprob": -0.2916760714548939,
        "compression_ratio": 1.9039548022598871,
        "end": 438.58,
        "id": 167,
        "no_speech_prob": 0.000005682420578523306,
        "seek": 43494,
        "start": 435.06,
        "temperature": 0,
        "text": " There is a utility that I could use called nodemon or node",
        "tokens": [
          50370,
          821,
          307,
          257,
          14877,
          300,
          286,
          727,
          764,
          1219,
          9984,
          3317,
          420,
          9984,
          50546
        ]
      },
      {
        "avg_logprob": -0.2916760714548939,
        "compression_ratio": 1.9039548022598871,
        "end": 439.58,
        "id": 168,
        "no_speech_prob": 0.000005682420578523306,
        "seek": 43494,
        "start": 438.58,
        "temperature": 0,
        "text": " monitor.",
        "tokens": [
          50546,
          6002,
          13,
          50596
        ]
      },
      {
        "avg_logprob": -0.2916760714548939,
        "compression_ratio": 1.9039548022598871,
        "end": 443.1,
        "id": 169,
        "no_speech_prob": 0.000005682420578523306,
        "seek": 43494,
        "start": 439.58,
        "temperature": 0,
        "text": " And I could install it as a global node module.",
        "tokens": [
          50596,
          400,
          286,
          727,
          3625,
          309,
          382,
          257,
          4338,
          9984,
          10088,
          13,
          50772
        ]
      },
      {
        "avg_logprob": -0.2916760714548939,
        "compression_ratio": 1.9039548022598871,
        "end": 451.18,
        "id": 170,
        "no_speech_prob": 0.000005682420578523306,
        "seek": 43494,
        "start": 446.7,
        "temperature": 0,
        "text": " And if I choose to use it, I can say now nodemon for nodemonitor",
        "tokens": [
          50952,
          400,
          498,
          286,
          2826,
          281,
          764,
          309,
          11,
          286,
          393,
          584,
          586,
          9984,
          3317,
          337,
          9984,
          3317,
          3029,
          51176
        ]
      },
      {
        "avg_logprob": -0.2916760714548939,
        "compression_ratio": 1.9039548022598871,
        "end": 452.38,
        "id": 171,
        "no_speech_prob": 0.000005682420578523306,
        "seek": 43494,
        "start": 451.18,
        "temperature": 0,
        "text": " index.js.",
        "tokens": [
          51176,
          8186,
          13,
          25530,
          13,
          51236
        ]
      },
      {
        "avg_logprob": -0.2916760714548939,
        "compression_ratio": 1.9039548022598871,
        "end": 454.86,
        "id": 172,
        "no_speech_prob": 0.000005682420578523306,
        "seek": 43494,
        "start": 452.38,
        "temperature": 0,
        "text": " And any time I change the code, it's",
        "tokens": [
          51236,
          400,
          604,
          565,
          286,
          1319,
          264,
          3089,
          11,
          309,
          311,
          51360
        ]
      },
      {
        "avg_logprob": -0.2916760714548939,
        "compression_ratio": 1.9039548022598871,
        "end": 456.74,
        "id": 173,
        "no_speech_prob": 0.000005682420578523306,
        "seek": 43494,
        "start": 454.86,
        "temperature": 0,
        "text": " going to rewrite the server.",
        "tokens": [
          51360,
          516,
          281,
          28132,
          264,
          7154,
          13,
          51454
        ]
      },
      {
        "avg_logprob": -0.2916760714548939,
        "compression_ratio": 1.9039548022598871,
        "end": 457.7,
        "id": 174,
        "no_speech_prob": 0.000005682420578523306,
        "seek": 43494,
        "start": 456.74,
        "temperature": 0,
        "text": " It's going to start.",
        "tokens": [
          51454,
          467,
          311,
          516,
          281,
          722,
          13,
          51502
        ]
      },
      {
        "avg_logprob": -0.2916760714548939,
        "compression_ratio": 1.9039548022598871,
        "end": 460.58,
        "id": 175,
        "no_speech_prob": 0.000005682420578523306,
        "seek": 43494,
        "start": 457.7,
        "temperature": 0,
        "text": " Any time I change the code, it's going to rerun the server.",
        "tokens": [
          51502,
          2639,
          565,
          286,
          1319,
          264,
          3089,
          11,
          309,
          311,
          516,
          281,
          43819,
          409,
          264,
          7154,
          13,
          51646
        ]
      },
      {
        "avg_logprob": -0.23279081845114416,
        "compression_ratio": 1.7220216606498195,
        "end": 466.26,
        "id": 176,
        "no_speech_prob": 0.000047575806092936546,
        "seek": 46058,
        "start": 460.62,
        "temperature": 0,
        "text": " So for example, I'm going to go here and say console.log,",
        "tokens": [
          50366,
          407,
          337,
          1365,
          11,
          286,
          478,
          516,
          281,
          352,
          510,
          293,
          584,
          11076,
          13,
          4987,
          11,
          50648
        ]
      },
      {
        "avg_logprob": -0.23279081845114416,
        "compression_ratio": 1.7220216606498195,
        "end": 469.26,
        "id": 177,
        "no_speech_prob": 0.000047575806092936546,
        "seek": 46058,
        "start": 466.26,
        "temperature": 0,
        "text": " I got a request.",
        "tokens": [
          50648,
          286,
          658,
          257,
          5308,
          13,
          50798
        ]
      },
      {
        "avg_logprob": -0.23279081845114416,
        "compression_ratio": 1.7220216606498195,
        "end": 471.94,
        "id": 178,
        "no_speech_prob": 0.000047575806092936546,
        "seek": 46058,
        "start": 469.26,
        "temperature": 0,
        "text": " And over here, you'll notice it restarted the server.",
        "tokens": [
          50798,
          400,
          670,
          510,
          11,
          291,
          603,
          3449,
          309,
          21022,
          292,
          264,
          7154,
          13,
          50932
        ]
      },
      {
        "avg_logprob": -0.23279081845114416,
        "compression_ratio": 1.7220216606498195,
        "end": 473.46,
        "id": 179,
        "no_speech_prob": 0.000047575806092936546,
        "seek": 46058,
        "start": 471.94,
        "temperature": 0,
        "text": " Sometimes you can run into trouble",
        "tokens": [
          50932,
          4803,
          291,
          393,
          1190,
          666,
          5253,
          51008
        ]
      },
      {
        "avg_logprob": -0.23279081845114416,
        "compression_ratio": 1.7220216606498195,
        "end": 475.78,
        "id": 180,
        "no_speech_prob": 0.000047575806092936546,
        "seek": 46058,
        "start": 473.46,
        "temperature": 0,
        "text": " with this if you're doing a lot of things at once.",
        "tokens": [
          51008,
          365,
          341,
          498,
          291,
          434,
          884,
          257,
          688,
          295,
          721,
          412,
          1564,
          13,
          51124
        ]
      },
      {
        "avg_logprob": -0.23279081845114416,
        "compression_ratio": 1.7220216606498195,
        "end": 478.53999999999996,
        "id": 181,
        "no_speech_prob": 0.000047575806092936546,
        "seek": 46058,
        "start": 475.78,
        "temperature": 0,
        "text": " And I often prefer to manually stop and start the server.",
        "tokens": [
          51124,
          400,
          286,
          2049,
          4382,
          281,
          16945,
          1590,
          293,
          722,
          264,
          7154,
          13,
          51262
        ]
      },
      {
        "avg_logprob": -0.23279081845114416,
        "compression_ratio": 1.7220216606498195,
        "end": 480.5,
        "id": 182,
        "no_speech_prob": 0.000047575806092936546,
        "seek": 46058,
        "start": 478.53999999999996,
        "temperature": 0,
        "text": " But for right now, this might be useful to see.",
        "tokens": [
          51262,
          583,
          337,
          558,
          586,
          11,
          341,
          1062,
          312,
          4420,
          281,
          536,
          13,
          51360
        ]
      },
      {
        "avg_logprob": -0.23279081845114416,
        "compression_ratio": 1.7220216606498195,
        "end": 482.5,
        "id": 183,
        "no_speech_prob": 0.000047575806092936546,
        "seek": 46058,
        "start": 480.5,
        "temperature": 0,
        "text": " So let me leave that there for right now.",
        "tokens": [
          51360,
          407,
          718,
          385,
          1856,
          300,
          456,
          337,
          558,
          586,
          13,
          51460
        ]
      },
      {
        "avg_logprob": -0.23279081845114416,
        "compression_ratio": 1.7220216606498195,
        "end": 485.09999999999997,
        "id": 184,
        "no_speech_prob": 0.000047575806092936546,
        "seek": 46058,
        "start": 482.5,
        "temperature": 0,
        "text": " Then I'm going to go over here, hit refresh.",
        "tokens": [
          51460,
          1396,
          286,
          478,
          516,
          281,
          352,
          670,
          510,
          11,
          2045,
          15134,
          13,
          51590
        ]
      },
      {
        "avg_logprob": -0.23279081845114416,
        "compression_ratio": 1.7220216606498195,
        "end": 488.65999999999997,
        "id": 185,
        "no_speech_prob": 0.000047575806092936546,
        "seek": 46058,
        "start": 485.09999999999997,
        "temperature": 0,
        "text": " And presumably, did it send the data?",
        "tokens": [
          51590,
          400,
          26742,
          11,
          630,
          309,
          2845,
          264,
          1412,
          30,
          51768
        ]
      },
      {
        "avg_logprob": -0.23279081845114416,
        "compression_ratio": 1.7220216606498195,
        "end": 489.97999999999996,
        "id": 186,
        "no_speech_prob": 0.000047575806092936546,
        "seek": 46058,
        "start": 488.65999999999997,
        "temperature": 0,
        "text": " Oh, it must have sent the data.",
        "tokens": [
          51768,
          876,
          11,
          309,
          1633,
          362,
          2279,
          264,
          1412,
          13,
          51834
        ]
      },
      {
        "avg_logprob": -0.21656726269011803,
        "compression_ratio": 1.6603053435114503,
        "end": 490.94,
        "id": 187,
        "no_speech_prob": 0.0008693571435287595,
        "seek": 48998,
        "start": 489.98,
        "temperature": 0,
        "text": " Presumably, it sent the data.",
        "tokens": [
          50364,
          2718,
          449,
          1188,
          11,
          309,
          2279,
          264,
          1412,
          13,
          50412
        ]
      },
      {
        "avg_logprob": -0.21656726269011803,
        "compression_ratio": 1.6603053435114503,
        "end": 492.06,
        "id": 188,
        "no_speech_prob": 0.0008693571435287595,
        "seek": 48998,
        "start": 490.94,
        "temperature": 0,
        "text": " I'll go back to the server.",
        "tokens": [
          50412,
          286,
          603,
          352,
          646,
          281,
          264,
          7154,
          13,
          50468
        ]
      },
      {
        "avg_logprob": -0.21656726269011803,
        "compression_ratio": 1.6603053435114503,
        "end": 492.74,
        "id": 189,
        "no_speech_prob": 0.0008693571435287595,
        "seek": 48998,
        "start": 492.06,
        "temperature": 0,
        "text": " I got a request.",
        "tokens": [
          50468,
          286,
          658,
          257,
          5308,
          13,
          50502
        ]
      },
      {
        "avg_logprob": -0.21656726269011803,
        "compression_ratio": 1.6603053435114503,
        "end": 494.06,
        "id": 190,
        "no_speech_prob": 0.0008693571435287595,
        "seek": 48998,
        "start": 492.74,
        "temperature": 0,
        "text": " Undefined.",
        "tokens": [
          50502,
          2719,
          5666,
          2001,
          13,
          50568
        ]
      },
      {
        "avg_logprob": -0.21656726269011803,
        "compression_ratio": 1.6603053435114503,
        "end": 494.98,
        "id": 191,
        "no_speech_prob": 0.0008693571435287595,
        "seek": 48998,
        "start": 494.06,
        "temperature": 0,
        "text": " Undefined.",
        "tokens": [
          50568,
          2719,
          5666,
          2001,
          13,
          50614
        ]
      },
      {
        "avg_logprob": -0.21656726269011803,
        "compression_ratio": 1.6603053435114503,
        "end": 496.26,
        "id": 192,
        "no_speech_prob": 0.0008693571435287595,
        "seek": 48998,
        "start": 494.98,
        "temperature": 0,
        "text": " Why is it undefined?",
        "tokens": [
          50614,
          1545,
          307,
          309,
          674,
          5666,
          2001,
          30,
          50678
        ]
      },
      {
        "avg_logprob": -0.21656726269011803,
        "compression_ratio": 1.6603053435114503,
        "end": 498.5,
        "id": 193,
        "no_speech_prob": 0.0008693571435287595,
        "seek": 48998,
        "start": 496.26,
        "temperature": 0,
        "text": " I forgot one of the steps that I told you about.",
        "tokens": [
          50678,
          286,
          5298,
          472,
          295,
          264,
          4439,
          300,
          286,
          1907,
          291,
          466,
          13,
          50790
        ]
      },
      {
        "avg_logprob": -0.21656726269011803,
        "compression_ratio": 1.6603053435114503,
        "end": 500.06,
        "id": 194,
        "no_speech_prob": 0.0008693571435287595,
        "seek": 48998,
        "start": 498.5,
        "temperature": 0,
        "text": " So I did two of them.",
        "tokens": [
          50790,
          407,
          286,
          630,
          732,
          295,
          552,
          13,
          50868
        ]
      },
      {
        "avg_logprob": -0.21656726269011803,
        "compression_ratio": 1.6603053435114503,
        "end": 504.86,
        "id": 195,
        "no_speech_prob": 0.0008693571435287595,
        "seek": 48998,
        "start": 500.06,
        "temperature": 0,
        "text": " I set up my route called API to receive the request.",
        "tokens": [
          50868,
          286,
          992,
          493,
          452,
          7955,
          1219,
          9362,
          281,
          4774,
          264,
          5308,
          13,
          51108
        ]
      },
      {
        "avg_logprob": -0.21656726269011803,
        "compression_ratio": 1.6603053435114503,
        "end": 507.42,
        "id": 196,
        "no_speech_prob": 0.0008693571435287595,
        "seek": 48998,
        "start": 504.86,
        "temperature": 0,
        "text": " I also modified the fetch function",
        "tokens": [
          51108,
          286,
          611,
          15873,
          264,
          23673,
          2445,
          51236
        ]
      },
      {
        "avg_logprob": -0.21656726269011803,
        "compression_ratio": 1.6603053435114503,
        "end": 509.54,
        "id": 197,
        "no_speech_prob": 0.0008693571435287595,
        "seek": 48998,
        "start": 507.42,
        "temperature": 0,
        "text": " to send data as a post.",
        "tokens": [
          51236,
          281,
          2845,
          1412,
          382,
          257,
          2183,
          13,
          51342
        ]
      },
      {
        "avg_logprob": -0.21656726269011803,
        "compression_ratio": 1.6603053435114503,
        "end": 512.98,
        "id": 198,
        "no_speech_prob": 0.0008693571435287595,
        "seek": 48998,
        "start": 509.54,
        "temperature": 0,
        "text": " What I did not do was add to my server",
        "tokens": [
          51342,
          708,
          286,
          630,
          406,
          360,
          390,
          909,
          281,
          452,
          7154,
          51514
        ]
      },
      {
        "avg_logprob": -0.21656726269011803,
        "compression_ratio": 1.6603053435114503,
        "end": 517.02,
        "id": 199,
        "no_speech_prob": 0.0008693571435287595,
        "seek": 48998,
        "start": 512.98,
        "temperature": 0,
        "text": " the ability to parse any incoming data as JSON.",
        "tokens": [
          51514,
          264,
          3485,
          281,
          48377,
          604,
          22341,
          1412,
          382,
          31828,
          13,
          51716
        ]
      },
      {
        "avg_logprob": -0.21656726269011803,
        "compression_ratio": 1.6603053435114503,
        "end": 519.62,
        "id": 200,
        "no_speech_prob": 0.0008693571435287595,
        "seek": 48998,
        "start": 517.02,
        "temperature": 0,
        "text": " The way to do that is with the express function",
        "tokens": [
          51716,
          440,
          636,
          281,
          360,
          300,
          307,
          365,
          264,
          5109,
          2445,
          51846
        ]
      },
      {
        "avg_logprob": -0.21814773877461752,
        "compression_ratio": 1.7887323943661972,
        "end": 520.86,
        "id": 201,
        "no_speech_prob": 0.0011878764489665627,
        "seek": 51962,
        "start": 519.74,
        "temperature": 0,
        "text": " express.json.",
        "tokens": [
          50370,
          5109,
          13,
          73,
          3015,
          13,
          50426
        ]
      },
      {
        "avg_logprob": -0.21814773877461752,
        "compression_ratio": 1.7887323943661972,
        "end": 524.7,
        "id": 202,
        "no_speech_prob": 0.0011878764489665627,
        "seek": 51962,
        "start": 520.86,
        "temperature": 0,
        "text": " So similarly to how I used express.static to host,",
        "tokens": [
          50426,
          407,
          14138,
          281,
          577,
          286,
          1143,
          5109,
          13,
          34632,
          281,
          3975,
          11,
          50618
        ]
      },
      {
        "avg_logprob": -0.21814773877461752,
        "compression_ratio": 1.7887323943661972,
        "end": 528.18,
        "id": 203,
        "no_speech_prob": 0.0011878764489665627,
        "seek": 51962,
        "start": 524.7,
        "temperature": 0,
        "text": " say, I want this server to host static files,",
        "tokens": [
          50618,
          584,
          11,
          286,
          528,
          341,
          7154,
          281,
          3975,
          13437,
          7098,
          11,
          50792
        ]
      },
      {
        "avg_logprob": -0.21814773877461752,
        "compression_ratio": 1.7887323943661972,
        "end": 530.34,
        "id": 204,
        "no_speech_prob": 0.0011878764489665627,
        "seek": 51962,
        "start": 528.18,
        "temperature": 0,
        "text": " I'm going to say express.json because I",
        "tokens": [
          50792,
          286,
          478,
          516,
          281,
          584,
          5109,
          13,
          73,
          3015,
          570,
          286,
          50900
        ]
      },
      {
        "avg_logprob": -0.21814773877461752,
        "compression_ratio": 1.7887323943661972,
        "end": 532.22,
        "id": 205,
        "no_speech_prob": 0.0011878764489665627,
        "seek": 51962,
        "start": 530.34,
        "temperature": 0,
        "text": " want this server to be able to understand",
        "tokens": [
          50900,
          528,
          341,
          7154,
          281,
          312,
          1075,
          281,
          1223,
          50994
        ]
      },
      {
        "avg_logprob": -0.21814773877461752,
        "compression_ratio": 1.7887323943661972,
        "end": 535.1,
        "id": 206,
        "no_speech_prob": 0.0011878764489665627,
        "seek": 51962,
        "start": 532.22,
        "temperature": 0,
        "text": " incoming data as JSON.",
        "tokens": [
          50994,
          22341,
          1412,
          382,
          31828,
          13,
          51138
        ]
      },
      {
        "avg_logprob": -0.21814773877461752,
        "compression_ratio": 1.7887323943661972,
        "end": 538.62,
        "id": 207,
        "no_speech_prob": 0.0011878764489665627,
        "seek": 51962,
        "start": 535.1,
        "temperature": 0,
        "text": " So in the code, I want to do just like I did up here,",
        "tokens": [
          51138,
          407,
          294,
          264,
          3089,
          11,
          286,
          528,
          281,
          360,
          445,
          411,
          286,
          630,
          493,
          510,
          11,
          51314
        ]
      },
      {
        "avg_logprob": -0.21814773877461752,
        "compression_ratio": 1.7887323943661972,
        "end": 540.46,
        "id": 208,
        "no_speech_prob": 0.0011878764489665627,
        "seek": 51962,
        "start": 538.62,
        "temperature": 0,
        "text": " app.use express.static.",
        "tokens": [
          51314,
          724,
          13,
          438,
          5109,
          13,
          34632,
          13,
          51406
        ]
      },
      {
        "avg_logprob": -0.21814773877461752,
        "compression_ratio": 1.7887323943661972,
        "end": 544.78,
        "id": 209,
        "no_speech_prob": 0.0011878764489665627,
        "seek": 51962,
        "start": 540.46,
        "temperature": 0,
        "text": " I'm going to say app.use express.json.",
        "tokens": [
          51406,
          286,
          478,
          516,
          281,
          584,
          724,
          13,
          438,
          5109,
          13,
          73,
          3015,
          13,
          51622
        ]
      },
      {
        "avg_logprob": -0.21814773877461752,
        "compression_ratio": 1.7887323943661972,
        "end": 547.66,
        "id": 210,
        "no_speech_prob": 0.0011878764489665627,
        "seek": 51962,
        "start": 544.78,
        "temperature": 0,
        "text": " And then I can actually put some options in here",
        "tokens": [
          51622,
          400,
          550,
          286,
          393,
          767,
          829,
          512,
          3956,
          294,
          510,
          51766
        ]
      },
      {
        "avg_logprob": -0.2395318861930601,
        "compression_ratio": 1.6630824372759856,
        "end": 550.9399999999999,
        "id": 211,
        "no_speech_prob": 0.000041334962588734925,
        "seek": 54766,
        "start": 547.6999999999999,
        "temperature": 0,
        "text": " to control or limit what is possible",
        "tokens": [
          50366,
          281,
          1969,
          420,
          4948,
          437,
          307,
          1944,
          50528
        ]
      },
      {
        "avg_logprob": -0.2395318861930601,
        "compression_ratio": 1.6630824372759856,
        "end": 552.3399999999999,
        "id": 212,
        "no_speech_prob": 0.000041334962588734925,
        "seek": 54766,
        "start": 550.9399999999999,
        "temperature": 0,
        "text": " in terms of receiving data.",
        "tokens": [
          50528,
          294,
          2115,
          295,
          10040,
          1412,
          13,
          50598
        ]
      },
      {
        "avg_logprob": -0.2395318861930601,
        "compression_ratio": 1.6630824372759856,
        "end": 556.54,
        "id": 213,
        "no_speech_prob": 0.000041334962588734925,
        "seek": 54766,
        "start": 552.3399999999999,
        "temperature": 0,
        "text": " So all of that is documented here on the Express website.",
        "tokens": [
          50598,
          407,
          439,
          295,
          300,
          307,
          23007,
          510,
          322,
          264,
          20212,
          3144,
          13,
          50808
        ]
      },
      {
        "avg_logprob": -0.2395318861930601,
        "compression_ratio": 1.6630824372759856,
        "end": 559.5,
        "id": 214,
        "no_speech_prob": 0.000041334962588734925,
        "seek": 54766,
        "start": 556.54,
        "temperature": 0,
        "text": " Let's add just as an example this limit.",
        "tokens": [
          50808,
          961,
          311,
          909,
          445,
          382,
          364,
          1365,
          341,
          4948,
          13,
          50956
        ]
      },
      {
        "avg_logprob": -0.2395318861930601,
        "compression_ratio": 1.6630824372759856,
        "end": 563.9399999999999,
        "id": 215,
        "no_speech_prob": 0.000041334962588734925,
        "seek": 54766,
        "start": 559.5,
        "temperature": 0,
        "text": " So limit allows me to specify the maximum size of any body",
        "tokens": [
          50956,
          407,
          4948,
          4045,
          385,
          281,
          16500,
          264,
          6674,
          2744,
          295,
          604,
          1772,
          51178
        ]
      },
      {
        "avg_logprob": -0.2395318861930601,
        "compression_ratio": 1.6630824372759856,
        "end": 564.9399999999999,
        "id": 216,
        "no_speech_prob": 0.000041334962588734925,
        "seek": 54766,
        "start": 563.9399999999999,
        "temperature": 0,
        "text": " that's coming in.",
        "tokens": [
          51178,
          300,
          311,
          1348,
          294,
          13,
          51228
        ]
      },
      {
        "avg_logprob": -0.2395318861930601,
        "compression_ratio": 1.6630824372759856,
        "end": 567.78,
        "id": 217,
        "no_speech_prob": 0.000041334962588734925,
        "seek": 54766,
        "start": 564.9399999999999,
        "temperature": 0,
        "text": " So I'll say limit, let's just say,",
        "tokens": [
          51228,
          407,
          286,
          603,
          584,
          4948,
          11,
          718,
          311,
          445,
          584,
          11,
          51370
        ]
      },
      {
        "avg_logprob": -0.2395318861930601,
        "compression_ratio": 1.6630824372759856,
        "end": 570.18,
        "id": 218,
        "no_speech_prob": 0.000041334962588734925,
        "seek": 54766,
        "start": 567.78,
        "temperature": 0,
        "text": " one megabyte as a kind of starting point.",
        "tokens": [
          51370,
          472,
          10816,
          34529,
          382,
          257,
          733,
          295,
          2891,
          935,
          13,
          51490
        ]
      },
      {
        "avg_logprob": -0.2395318861930601,
        "compression_ratio": 1.6630824372759856,
        "end": 572.4599999999999,
        "id": 219,
        "no_speech_prob": 0.000041334962588734925,
        "seek": 54766,
        "start": 570.18,
        "temperature": 0,
        "text": " So this protects against someone flooding my server",
        "tokens": [
          51490,
          407,
          341,
          22583,
          1970,
          1580,
          24132,
          452,
          7154,
          51604
        ]
      },
      {
        "avg_logprob": -0.2395318861930601,
        "compression_ratio": 1.6630824372759856,
        "end": 574.5,
        "id": 220,
        "no_speech_prob": 0.000041334962588734925,
        "seek": 54766,
        "start": 572.4599999999999,
        "temperature": 0,
        "text": " with huge amounts of data, potentially.",
        "tokens": [
          51604,
          365,
          2603,
          11663,
          295,
          1412,
          11,
          7263,
          13,
          51706
        ]
      },
      {
        "avg_logprob": -0.2395318861930601,
        "compression_ratio": 1.6630824372759856,
        "end": 577.38,
        "id": 221,
        "no_speech_prob": 0.000041334962588734925,
        "seek": 54766,
        "start": 574.5,
        "temperature": 0,
        "text": " There's more to protecting your server than just that,",
        "tokens": [
          51706,
          821,
          311,
          544,
          281,
          12316,
          428,
          7154,
          813,
          445,
          300,
          11,
          51850
        ]
      },
      {
        "avg_logprob": -0.2691404355452365,
        "compression_ratio": 1.7391304347826086,
        "end": 579.98,
        "id": 222,
        "no_speech_prob": 0.000024682822186150588,
        "seek": 57738,
        "start": 577.6,
        "temperature": 0,
        "text": " but we're seeing an inkling of this here.",
        "tokens": [
          50375,
          457,
          321,
          434,
          2577,
          364,
          11276,
          1688,
          295,
          341,
          510,
          13,
          50494
        ]
      },
      {
        "avg_logprob": -0.2691404355452365,
        "compression_ratio": 1.7391304347826086,
        "end": 582.74,
        "id": 223,
        "no_speech_prob": 0.000024682822186150588,
        "seek": 57738,
        "start": 579.98,
        "temperature": 0,
        "text": " All right, so now we can see Node has,",
        "tokens": [
          50494,
          1057,
          558,
          11,
          370,
          586,
          321,
          393,
          536,
          38640,
          575,
          11,
          50632
        ]
      },
      {
        "avg_logprob": -0.2691404355452365,
        "compression_ratio": 1.7391304347826086,
        "end": 585.42,
        "id": 224,
        "no_speech_prob": 0.000024682822186150588,
        "seek": 57738,
        "start": 582.74,
        "temperature": 0,
        "text": " NodeMon has restarted my server a bunch of times,",
        "tokens": [
          50632,
          38640,
          32498,
          575,
          21022,
          292,
          452,
          7154,
          257,
          3840,
          295,
          1413,
          11,
          50766
        ]
      },
      {
        "avg_logprob": -0.2691404355452365,
        "compression_ratio": 1.7391304347826086,
        "end": 588.26,
        "id": 225,
        "no_speech_prob": 0.000024682822186150588,
        "seek": 57738,
        "start": 585.42,
        "temperature": 0,
        "text": " and I should be able to go back to the web page",
        "tokens": [
          50766,
          293,
          286,
          820,
          312,
          1075,
          281,
          352,
          646,
          281,
          264,
          3670,
          3028,
          50908
        ]
      },
      {
        "avg_logprob": -0.2691404355452365,
        "compression_ratio": 1.7391304347826086,
        "end": 591.26,
        "id": 226,
        "no_speech_prob": 0.000024682822186150588,
        "seek": 57738,
        "start": 588.26,
        "temperature": 0,
        "text": " and hit refresh.",
        "tokens": [
          50908,
          293,
          2045,
          15134,
          13,
          51058
        ]
      },
      {
        "avg_logprob": -0.2691404355452365,
        "compression_ratio": 1.7391304347826086,
        "end": 593.34,
        "id": 227,
        "no_speech_prob": 0.000024682822186150588,
        "seek": 57738,
        "start": 591.26,
        "temperature": 0,
        "text": " And it's going to get the latitude and longitude.",
        "tokens": [
          51058,
          400,
          309,
          311,
          516,
          281,
          483,
          264,
          45436,
          293,
          938,
          4377,
          13,
          51162
        ]
      },
      {
        "avg_logprob": -0.2691404355452365,
        "compression_ratio": 1.7391304347826086,
        "end": 597.06,
        "id": 228,
        "no_speech_prob": 0.000024682822186150588,
        "seek": 57738,
        "start": 593.34,
        "temperature": 0,
        "text": " And now I should see in the server, ha ha, I got a request.",
        "tokens": [
          51162,
          400,
          586,
          286,
          820,
          536,
          294,
          264,
          7154,
          11,
          324,
          324,
          11,
          286,
          658,
          257,
          5308,
          13,
          51348
        ]
      },
      {
        "avg_logprob": -0.2691404355452365,
        "compression_ratio": 1.7391304347826086,
        "end": 598.7,
        "id": 229,
        "no_speech_prob": 0.000024682822186150588,
        "seek": 57738,
        "start": 597.06,
        "temperature": 0,
        "text": " And I got the latitude and longitude.",
        "tokens": [
          51348,
          400,
          286,
          658,
          264,
          45436,
          293,
          938,
          4377,
          13,
          51430
        ]
      },
      {
        "avg_logprob": -0.2691404355452365,
        "compression_ratio": 1.7391304347826086,
        "end": 602.9,
        "id": 230,
        "no_speech_prob": 0.000024682822186150588,
        "seek": 57738,
        "start": 598.7,
        "temperature": 0,
        "text": " So the server has successfully received the data.",
        "tokens": [
          51430,
          407,
          264,
          7154,
          575,
          10727,
          4613,
          264,
          1412,
          13,
          51640
        ]
      },
      {
        "avg_logprob": -0.2691404355452365,
        "compression_ratio": 1.7391304347826086,
        "end": 605.02,
        "id": 231,
        "no_speech_prob": 0.000024682822186150588,
        "seek": 57738,
        "start": 602.9,
        "temperature": 0,
        "text": " This is what I wanted to do in this video.",
        "tokens": [
          51640,
          639,
          307,
          437,
          286,
          1415,
          281,
          360,
          294,
          341,
          960,
          13,
          51746
        ]
      },
      {
        "avg_logprob": -0.2691404355452365,
        "compression_ratio": 1.7391304347826086,
        "end": 606.06,
        "id": 232,
        "no_speech_prob": 0.000024682822186150588,
        "seek": 57738,
        "start": 605.02,
        "temperature": 0,
        "text": " So in a way, I'm done.",
        "tokens": [
          51746,
          407,
          294,
          257,
          636,
          11,
          286,
          478,
          1096,
          13,
          51798
        ]
      },
      {
        "avg_logprob": -0.2691404355452365,
        "compression_ratio": 1.7391304347826086,
        "end": 607.1,
        "id": 233,
        "no_speech_prob": 0.000024682822186150588,
        "seek": 57738,
        "start": 606.06,
        "temperature": 0,
        "text": " Oh, that's exciting.",
        "tokens": [
          51798,
          876,
          11,
          300,
          311,
          4670,
          13,
          51850
        ]
      },
      {
        "avg_logprob": -0.19697697482891938,
        "compression_ratio": 1.810077519379845,
        "end": 609.78,
        "id": 234,
        "no_speech_prob": 0.000049084355850936845,
        "seek": 60710,
        "start": 607.82,
        "temperature": 0,
        "text": " What I've actually done here is pretty terrible",
        "tokens": [
          50400,
          708,
          286,
          600,
          767,
          1096,
          510,
          307,
          1238,
          6237,
          50498
        ]
      },
      {
        "avg_logprob": -0.19697697482891938,
        "compression_ratio": 1.810077519379845,
        "end": 614.78,
        "id": 235,
        "no_speech_prob": 0.000049084355850936845,
        "seek": 60710,
        "start": 609.78,
        "temperature": 0,
        "text": " in that it is required that you complete a request.",
        "tokens": [
          50498,
          294,
          300,
          309,
          307,
          4739,
          300,
          291,
          3566,
          257,
          5308,
          13,
          50748
        ]
      },
      {
        "avg_logprob": -0.19697697482891938,
        "compression_ratio": 1.810077519379845,
        "end": 617.4200000000001,
        "id": 236,
        "no_speech_prob": 0.000049084355850936845,
        "seek": 60710,
        "start": 615.14,
        "temperature": 0,
        "text": " So if a get request or a post request is coming",
        "tokens": [
          50766,
          407,
          498,
          257,
          483,
          5308,
          420,
          257,
          2183,
          5308,
          307,
          1348,
          50880
        ]
      },
      {
        "avg_logprob": -0.19697697482891938,
        "compression_ratio": 1.810077519379845,
        "end": 620.26,
        "id": 237,
        "no_speech_prob": 0.000049084355850936845,
        "seek": 60710,
        "start": 617.4200000000001,
        "temperature": 0,
        "text": " into the server, I need to do something to complete it.",
        "tokens": [
          50880,
          666,
          264,
          7154,
          11,
          286,
          643,
          281,
          360,
          746,
          281,
          3566,
          309,
          13,
          51022
        ]
      },
      {
        "avg_logprob": -0.19697697482891938,
        "compression_ratio": 1.810077519379845,
        "end": 622.96,
        "id": 238,
        "no_speech_prob": 0.000049084355850936845,
        "seek": 60710,
        "start": 620.26,
        "temperature": 0,
        "text": " The nicest thing to do, if I'm trying to be polite here,",
        "tokens": [
          51022,
          440,
          45516,
          551,
          281,
          360,
          11,
          498,
          286,
          478,
          1382,
          281,
          312,
          25171,
          510,
          11,
          51157
        ]
      },
      {
        "avg_logprob": -0.19697697482891938,
        "compression_ratio": 1.810077519379845,
        "end": 625.58,
        "id": 239,
        "no_speech_prob": 0.000049084355850936845,
        "seek": 60710,
        "start": 622.96,
        "temperature": 0,
        "text": " which is, you know, it's good for your server to be polite,",
        "tokens": [
          51157,
          597,
          307,
          11,
          291,
          458,
          11,
          309,
          311,
          665,
          337,
          428,
          7154,
          281,
          312,
          25171,
          11,
          51288
        ]
      },
      {
        "avg_logprob": -0.19697697482891938,
        "compression_ratio": 1.810077519379845,
        "end": 627.0600000000001,
        "id": 240,
        "no_speech_prob": 0.000049084355850936845,
        "seek": 60710,
        "start": 625.58,
        "temperature": 0,
        "text": " is send a response back.",
        "tokens": [
          51288,
          307,
          2845,
          257,
          4134,
          646,
          13,
          51362
        ]
      },
      {
        "avg_logprob": -0.19697697482891938,
        "compression_ratio": 1.810077519379845,
        "end": 630.32,
        "id": 241,
        "no_speech_prob": 0.000049084355850936845,
        "seek": 60710,
        "start": 627.0600000000001,
        "temperature": 0,
        "text": " But at a minimum, I can just say response.end.",
        "tokens": [
          51362,
          583,
          412,
          257,
          7285,
          11,
          286,
          393,
          445,
          584,
          4134,
          13,
          521,
          13,
          51525
        ]
      },
      {
        "avg_logprob": -0.19697697482891938,
        "compression_ratio": 1.810077519379845,
        "end": 632.86,
        "id": 242,
        "no_speech_prob": 0.000049084355850936845,
        "seek": 60710,
        "start": 630.32,
        "temperature": 0,
        "text": " So this is required to do something.",
        "tokens": [
          51525,
          407,
          341,
          307,
          4739,
          281,
          360,
          746,
          13,
          51652
        ]
      },
      {
        "avg_logprob": -0.19697697482891938,
        "compression_ratio": 1.810077519379845,
        "end": 634.5400000000001,
        "id": 243,
        "no_speech_prob": 0.000049084355850936845,
        "seek": 60710,
        "start": 632.86,
        "temperature": 0,
        "text": " What I think that I would like to do,",
        "tokens": [
          51652,
          708,
          286,
          519,
          300,
          286,
          576,
          411,
          281,
          360,
          11,
          51736
        ]
      },
      {
        "avg_logprob": -0.2172969788995408,
        "compression_ratio": 1.7061224489795919,
        "end": 639.54,
        "id": 244,
        "no_speech_prob": 0.00005144091846887022,
        "seek": 63454,
        "start": 634.54,
        "temperature": 0,
        "text": " which makes more sense, is say response.json,",
        "tokens": [
          50364,
          597,
          1669,
          544,
          2020,
          11,
          307,
          584,
          4134,
          13,
          73,
          3015,
          11,
          50614
        ]
      },
      {
        "avg_logprob": -0.2172969788995408,
        "compression_ratio": 1.7061224489795919,
        "end": 641.3,
        "id": 245,
        "no_speech_prob": 0.00005144091846887022,
        "seek": 63454,
        "start": 639.5799999999999,
        "temperature": 0,
        "text": " which is a function that will send data.",
        "tokens": [
          50616,
          597,
          307,
          257,
          2445,
          300,
          486,
          2845,
          1412,
          13,
          50702
        ]
      },
      {
        "avg_logprob": -0.2172969788995408,
        "compression_ratio": 1.7061224489795919,
        "end": 642.8199999999999,
        "id": 246,
        "no_speech_prob": 0.00005144091846887022,
        "seek": 63454,
        "start": 641.3,
        "temperature": 0,
        "text": " I could also say response.send",
        "tokens": [
          50702,
          286,
          727,
          611,
          584,
          4134,
          13,
          82,
          521,
          50778
        ]
      },
      {
        "avg_logprob": -0.2172969788995408,
        "compression_ratio": 1.7061224489795919,
        "end": 644.86,
        "id": 247,
        "no_speech_prob": 0.00005144091846887022,
        "seek": 63454,
        "start": 642.8199999999999,
        "temperature": 0,
        "text": " for just a short message as a string.",
        "tokens": [
          50778,
          337,
          445,
          257,
          2099,
          3636,
          382,
          257,
          6798,
          13,
          50880
        ]
      },
      {
        "avg_logprob": -0.2172969788995408,
        "compression_ratio": 1.7061224489795919,
        "end": 647.04,
        "id": 248,
        "no_speech_prob": 0.00005144091846887022,
        "seek": 63454,
        "start": 644.86,
        "temperature": 0,
        "text": " But I want to send maybe an object back",
        "tokens": [
          50880,
          583,
          286,
          528,
          281,
          2845,
          1310,
          364,
          2657,
          646,
          50989
        ]
      },
      {
        "avg_logprob": -0.2172969788995408,
        "compression_ratio": 1.7061224489795919,
        "end": 648.0999999999999,
        "id": 249,
        "no_speech_prob": 0.00005144091846887022,
        "seek": 63454,
        "start": 647.04,
        "temperature": 0,
        "text": " with some data in it.",
        "tokens": [
          50989,
          365,
          512,
          1412,
          294,
          309,
          13,
          51042
        ]
      },
      {
        "avg_logprob": -0.2172969788995408,
        "compression_ratio": 1.7061224489795919,
        "end": 649.98,
        "id": 250,
        "no_speech_prob": 0.00005144091846887022,
        "seek": 63454,
        "start": 648.0999999999999,
        "temperature": 0,
        "text": " So I'm going to do response.json.",
        "tokens": [
          51042,
          407,
          286,
          478,
          516,
          281,
          360,
          4134,
          13,
          73,
          3015,
          13,
          51136
        ]
      },
      {
        "avg_logprob": -0.2172969788995408,
        "compression_ratio": 1.7061224489795919,
        "end": 654.3,
        "id": 251,
        "no_speech_prob": 0.00005144091846887022,
        "seek": 63454,
        "start": 649.98,
        "temperature": 0,
        "text": " And then I would say, I could say like status.",
        "tokens": [
          51136,
          400,
          550,
          286,
          576,
          584,
          11,
          286,
          727,
          584,
          411,
          6558,
          13,
          51352
        ]
      },
      {
        "avg_logprob": -0.2172969788995408,
        "compression_ratio": 1.7061224489795919,
        "end": 656.26,
        "id": 252,
        "no_speech_prob": 0.00005144091846887022,
        "seek": 63454,
        "start": 654.3,
        "temperature": 0,
        "text": " I'm just making this up.",
        "tokens": [
          51352,
          286,
          478,
          445,
          1455,
          341,
          493,
          13,
          51450
        ]
      },
      {
        "avg_logprob": -0.2172969788995408,
        "compression_ratio": 1.7061224489795919,
        "end": 657.5,
        "id": 253,
        "no_speech_prob": 0.00005144091846887022,
        "seek": 63454,
        "start": 656.26,
        "temperature": 0,
        "text": " Success.",
        "tokens": [
          51450,
          23669,
          13,
          51512
        ]
      },
      {
        "avg_logprob": -0.2172969788995408,
        "compression_ratio": 1.7061224489795919,
        "end": 662.4599999999999,
        "id": 254,
        "no_speech_prob": 0.00005144091846887022,
        "seek": 63454,
        "start": 657.5,
        "temperature": 0,
        "text": " And then maybe I'll just repeat back what was sent to me.",
        "tokens": [
          51512,
          400,
          550,
          1310,
          286,
          603,
          445,
          7149,
          646,
          437,
          390,
          2279,
          281,
          385,
          13,
          51760
        ]
      },
      {
        "avg_logprob": -0.2172969788995408,
        "compression_ratio": 1.7061224489795919,
        "end": 663.98,
        "id": 255,
        "no_speech_prob": 0.00005144091846887022,
        "seek": 63454,
        "start": 662.4599999999999,
        "temperature": 0,
        "text": " This is the therapy server.",
        "tokens": [
          51760,
          639,
          307,
          264,
          9492,
          7154,
          13,
          51836
        ]
      },
      {
        "avg_logprob": -0.22665682378804908,
        "compression_ratio": 1.608294930875576,
        "end": 666.4200000000001,
        "id": 256,
        "no_speech_prob": 0.0000888804643182084,
        "seek": 66398,
        "start": 664.38,
        "temperature": 0,
        "text": " It listens and then it just repeats it back to you.",
        "tokens": [
          50384,
          467,
          35959,
          293,
          550,
          309,
          445,
          35038,
          309,
          646,
          281,
          291,
          13,
          50486
        ]
      },
      {
        "avg_logprob": -0.22665682378804908,
        "compression_ratio": 1.608294930875576,
        "end": 670.74,
        "id": 257,
        "no_speech_prob": 0.0000888804643182084,
        "seek": 66398,
        "start": 666.4200000000001,
        "temperature": 0,
        "text": " So I'll say latitude, longitude, long.",
        "tokens": [
          50486,
          407,
          286,
          603,
          584,
          45436,
          11,
          938,
          4377,
          11,
          938,
          13,
          50702
        ]
      },
      {
        "avg_logprob": -0.22665682378804908,
        "compression_ratio": 1.608294930875576,
        "end": 678.1800000000001,
        "id": 258,
        "no_speech_prob": 0.0000888804643182084,
        "seek": 66398,
        "start": 674.14,
        "temperature": 0,
        "text": " So this is what I want to send back now as JavaScript.",
        "tokens": [
          50872,
          407,
          341,
          307,
          437,
          286,
          528,
          281,
          2845,
          646,
          586,
          382,
          15778,
          13,
          51074
        ]
      },
      {
        "avg_logprob": -0.22665682378804908,
        "compression_ratio": 1.608294930875576,
        "end": 681.0600000000001,
        "id": 259,
        "no_speech_prob": 0.0000888804643182084,
        "seek": 66398,
        "start": 678.1800000000001,
        "temperature": 0,
        "text": " Okay, so then in the client,",
        "tokens": [
          51074,
          1033,
          11,
          370,
          550,
          294,
          264,
          6423,
          11,
          51218
        ]
      },
      {
        "avg_logprob": -0.22665682378804908,
        "compression_ratio": 1.608294930875576,
        "end": 683.5,
        "id": 260,
        "no_speech_prob": 0.0000888804643182084,
        "seek": 66398,
        "start": 681.0600000000001,
        "temperature": 0,
        "text": " I need to do something to receive this.",
        "tokens": [
          51218,
          286,
          643,
          281,
          360,
          746,
          281,
          4774,
          341,
          13,
          51340
        ]
      },
      {
        "avg_logprob": -0.22665682378804908,
        "compression_ratio": 1.608294930875576,
        "end": 685.94,
        "id": 261,
        "no_speech_prob": 0.0000888804643182084,
        "seek": 66398,
        "start": 683.5,
        "temperature": 0,
        "text": " So if I go back to index.html,",
        "tokens": [
          51340,
          407,
          498,
          286,
          352,
          646,
          281,
          8186,
          13,
          357,
          15480,
          11,
          51462
        ]
      },
      {
        "avg_logprob": -0.22665682378804908,
        "compression_ratio": 1.608294930875576,
        "end": 688.54,
        "id": 262,
        "no_speech_prob": 0.0000888804643182084,
        "seek": 66398,
        "start": 685.94,
        "temperature": 0,
        "text": " what I have here calling the fetch function",
        "tokens": [
          51462,
          437,
          286,
          362,
          510,
          5141,
          264,
          23673,
          2445,
          51592
        ]
      },
      {
        "avg_logprob": -0.22665682378804908,
        "compression_ratio": 1.608294930875576,
        "end": 689.82,
        "id": 263,
        "no_speech_prob": 0.0000888804643182084,
        "seek": 66398,
        "start": 688.54,
        "temperature": 0,
        "text": " is where I'm sending the data,",
        "tokens": [
          51592,
          307,
          689,
          286,
          478,
          7750,
          264,
          1412,
          11,
          51656
        ]
      },
      {
        "avg_logprob": -0.22665682378804908,
        "compression_ratio": 1.608294930875576,
        "end": 691.5,
        "id": 264,
        "no_speech_prob": 0.0000888804643182084,
        "seek": 66398,
        "start": 689.82,
        "temperature": 0,
        "text": " and fetch returns a promise.",
        "tokens": [
          51656,
          293,
          23673,
          11247,
          257,
          6228,
          13,
          51740
        ]
      },
      {
        "avg_logprob": -0.1857245872760641,
        "compression_ratio": 1.7255813953488373,
        "end": 695.78,
        "id": 265,
        "no_speech_prob": 0.0000038449315979960375,
        "seek": 69150,
        "start": 691.5,
        "temperature": 0,
        "text": " So I can just say.then and then handle the response.",
        "tokens": [
          50364,
          407,
          286,
          393,
          445,
          584,
          2411,
          19096,
          293,
          550,
          4813,
          264,
          4134,
          13,
          50578
        ]
      },
      {
        "avg_logprob": -0.1857245872760641,
        "compression_ratio": 1.7255813953488373,
        "end": 699.52,
        "id": 266,
        "no_speech_prob": 0.0000038449315979960375,
        "seek": 69150,
        "start": 695.78,
        "temperature": 0,
        "text": " Response console.log that response.",
        "tokens": [
          50578,
          43937,
          11076,
          13,
          4987,
          300,
          4134,
          13,
          50765
        ]
      },
      {
        "avg_logprob": -0.1857245872760641,
        "compression_ratio": 1.7255813953488373,
        "end": 705.62,
        "id": 267,
        "no_speech_prob": 0.0000038449315979960375,
        "seek": 69150,
        "start": 702.22,
        "temperature": 0,
        "text": " All right, so let's take a look at this in the browser now.",
        "tokens": [
          50900,
          1057,
          558,
          11,
          370,
          718,
          311,
          747,
          257,
          574,
          412,
          341,
          294,
          264,
          11185,
          586,
          13,
          51070
        ]
      },
      {
        "avg_logprob": -0.1857245872760641,
        "compression_ratio": 1.7255813953488373,
        "end": 707.3,
        "id": 268,
        "no_speech_prob": 0.0000038449315979960375,
        "seek": 69150,
        "start": 705.62,
        "temperature": 0,
        "text": " I'm going to hit refresh.",
        "tokens": [
          51070,
          286,
          478,
          516,
          281,
          2045,
          15134,
          13,
          51154
        ]
      },
      {
        "avg_logprob": -0.1857245872760641,
        "compression_ratio": 1.7255813953488373,
        "end": 709.38,
        "id": 269,
        "no_speech_prob": 0.0000038449315979960375,
        "seek": 69150,
        "start": 707.3,
        "temperature": 0,
        "text": " It's retrieving my latitude and longitude.",
        "tokens": [
          51154,
          467,
          311,
          19817,
          798,
          452,
          45436,
          293,
          938,
          4377,
          13,
          51258
        ]
      },
      {
        "avg_logprob": -0.1857245872760641,
        "compression_ratio": 1.7255813953488373,
        "end": 711.36,
        "id": 270,
        "no_speech_prob": 0.0000038449315979960375,
        "seek": 69150,
        "start": 709.38,
        "temperature": 0,
        "text": " It's going to send to the server.",
        "tokens": [
          51258,
          467,
          311,
          516,
          281,
          2845,
          281,
          264,
          7154,
          13,
          51357
        ]
      },
      {
        "avg_logprob": -0.1857245872760641,
        "compression_ratio": 1.7255813953488373,
        "end": 715.66,
        "id": 271,
        "no_speech_prob": 0.0000038449315979960375,
        "seek": 69150,
        "start": 711.36,
        "temperature": 0,
        "text": " And then here we are, the response.",
        "tokens": [
          51357,
          400,
          550,
          510,
          321,
          366,
          11,
          264,
          4134,
          13,
          51572
        ]
      },
      {
        "avg_logprob": -0.1857245872760641,
        "compression_ratio": 1.7255813953488373,
        "end": 716.7,
        "id": 272,
        "no_speech_prob": 0.0000038449315979960375,
        "seek": 69150,
        "start": 715.66,
        "temperature": 0,
        "text": " Oh boy, look at this.",
        "tokens": [
          51572,
          876,
          3237,
          11,
          574,
          412,
          341,
          13,
          51624
        ]
      },
      {
        "avg_logprob": -0.1857245872760641,
        "compression_ratio": 1.7255813953488373,
        "end": 718.06,
        "id": 273,
        "no_speech_prob": 0.0000038449315979960375,
        "seek": 69150,
        "start": 716.7,
        "temperature": 0,
        "text": " Lat is not defined.",
        "tokens": [
          51624,
          7354,
          307,
          406,
          7642,
          13,
          51692
        ]
      },
      {
        "avg_logprob": -0.1857245872760641,
        "compression_ratio": 1.7255813953488373,
        "end": 720.68,
        "id": 274,
        "no_speech_prob": 0.0000038449315979960375,
        "seek": 69150,
        "start": 718.06,
        "temperature": 0,
        "text": " Let's look at my code back on the server.",
        "tokens": [
          51692,
          961,
          311,
          574,
          412,
          452,
          3089,
          646,
          322,
          264,
          7154,
          13,
          51823
        ]
      },
      {
        "avg_logprob": -0.2034067674116655,
        "compression_ratio": 1.6836363636363636,
        "end": 722.88,
        "id": 275,
        "no_speech_prob": 0.000020785091692232527,
        "seek": 72068,
        "start": 720.68,
        "temperature": 0,
        "text": " Oh yes, these aren't a thing.",
        "tokens": [
          50364,
          876,
          2086,
          11,
          613,
          3212,
          380,
          257,
          551,
          13,
          50474
        ]
      },
      {
        "avg_logprob": -0.2034067674116655,
        "compression_ratio": 1.6836363636363636,
        "end": 725.3199999999999,
        "id": 276,
        "no_speech_prob": 0.000020785091692232527,
        "seek": 72068,
        "start": 722.88,
        "temperature": 0,
        "text": " Remember, everything is in request.body.",
        "tokens": [
          50474,
          5459,
          11,
          1203,
          307,
          294,
          5308,
          13,
          1067,
          13,
          50596
        ]
      },
      {
        "avg_logprob": -0.2034067674116655,
        "compression_ratio": 1.6836363636363636,
        "end": 728.8199999999999,
        "id": 277,
        "no_speech_prob": 0.000020785091692232527,
        "seek": 72068,
        "start": 725.3199999999999,
        "temperature": 0,
        "text": " So what I could do is say const data equals,",
        "tokens": [
          50596,
          407,
          437,
          286,
          727,
          360,
          307,
          584,
          1817,
          1412,
          6915,
          11,
          50771
        ]
      },
      {
        "avg_logprob": -0.2034067674116655,
        "compression_ratio": 1.6836363636363636,
        "end": 732.04,
        "id": 278,
        "no_speech_prob": 0.000020785091692232527,
        "seek": 72068,
        "start": 728.8199999999999,
        "temperature": 0,
        "text": " I could put this in another variable.",
        "tokens": [
          50771,
          286,
          727,
          829,
          341,
          294,
          1071,
          7006,
          13,
          50932
        ]
      },
      {
        "avg_logprob": -0.2034067674116655,
        "compression_ratio": 1.6836363636363636,
        "end": 734.26,
        "id": 279,
        "no_speech_prob": 0.000020785091692232527,
        "seek": 72068,
        "start": 732.04,
        "temperature": 0,
        "text": " And then I could just say data.lat.",
        "tokens": [
          50932,
          400,
          550,
          286,
          727,
          445,
          584,
          1412,
          13,
          14087,
          13,
          51043
        ]
      },
      {
        "avg_logprob": -0.2034067674116655,
        "compression_ratio": 1.6836363636363636,
        "end": 737.04,
        "id": 280,
        "no_speech_prob": 0.000020785091692232527,
        "seek": 72068,
        "start": 734.26,
        "temperature": 0,
        "text": " I could have just said request.body.lat, data.long.",
        "tokens": [
          51043,
          286,
          727,
          362,
          445,
          848,
          5308,
          13,
          1067,
          13,
          14087,
          11,
          1412,
          13,
          13025,
          13,
          51182
        ]
      },
      {
        "avg_logprob": -0.2034067674116655,
        "compression_ratio": 1.6836363636363636,
        "end": 738.3599999999999,
        "id": 281,
        "no_speech_prob": 0.000020785091692232527,
        "seek": 72068,
        "start": 737.04,
        "temperature": 0,
        "text": " So okay, there we go.",
        "tokens": [
          51182,
          407,
          1392,
          11,
          456,
          321,
          352,
          13,
          51248
        ]
      },
      {
        "avg_logprob": -0.2034067674116655,
        "compression_ratio": 1.6836363636363636,
        "end": 739.9599999999999,
        "id": 282,
        "no_speech_prob": 0.000020785091692232527,
        "seek": 72068,
        "start": 738.3599999999999,
        "temperature": 0,
        "text": " Let's try this again.",
        "tokens": [
          51248,
          961,
          311,
          853,
          341,
          797,
          13,
          51328
        ]
      },
      {
        "avg_logprob": -0.2034067674116655,
        "compression_ratio": 1.6836363636363636,
        "end": 742.52,
        "id": 283,
        "no_speech_prob": 0.000020785091692232527,
        "seek": 72068,
        "start": 739.9599999999999,
        "temperature": 0,
        "text": " So the server should be restarting now",
        "tokens": [
          51328,
          407,
          264,
          7154,
          820,
          312,
          21022,
          278,
          586,
          51456
        ]
      },
      {
        "avg_logprob": -0.2034067674116655,
        "compression_ratio": 1.6836363636363636,
        "end": 746.1999999999999,
        "id": 284,
        "no_speech_prob": 0.000020785091692232527,
        "seek": 72068,
        "start": 742.52,
        "temperature": 0,
        "text": " any time I make changes because I'm running NodeMod.",
        "tokens": [
          51456,
          604,
          565,
          286,
          652,
          2962,
          570,
          286,
          478,
          2614,
          38640,
          44,
          378,
          13,
          51640
        ]
      },
      {
        "avg_logprob": -0.2034067674116655,
        "compression_ratio": 1.6836363636363636,
        "end": 749,
        "id": 285,
        "no_speech_prob": 0.000020785091692232527,
        "seek": 72068,
        "start": 746.1999999999999,
        "temperature": 0,
        "text": " Then I can just go back to the client and hit refresh.",
        "tokens": [
          51640,
          1396,
          286,
          393,
          445,
          352,
          646,
          281,
          264,
          6423,
          293,
          2045,
          15134,
          13,
          51780
        ]
      },
      {
        "avg_logprob": -0.2034067674116655,
        "compression_ratio": 1.6836363636363636,
        "end": 750.64,
        "id": 286,
        "no_speech_prob": 0.000020785091692232527,
        "seek": 72068,
        "start": 749,
        "temperature": 0,
        "text": " And there it is, the response.",
        "tokens": [
          51780,
          400,
          456,
          309,
          307,
          11,
          264,
          4134,
          13,
          51862
        ]
      },
      {
        "avg_logprob": -0.22396551717923383,
        "compression_ratio": 1.6491935483870968,
        "end": 752.88,
        "id": 287,
        "no_speech_prob": 0.000016442383639514446,
        "seek": 75064,
        "start": 751.48,
        "temperature": 0,
        "text": " Now this isn't what I was looking for.",
        "tokens": [
          50406,
          823,
          341,
          1943,
          380,
          437,
          286,
          390,
          1237,
          337,
          13,
          50476
        ]
      },
      {
        "avg_logprob": -0.22396551717923383,
        "compression_ratio": 1.6491935483870968,
        "end": 756.4399999999999,
        "id": 288,
        "no_speech_prob": 0.000016442383639514446,
        "seek": 75064,
        "start": 752.88,
        "temperature": 0,
        "text": " And if you recall from when I first looked",
        "tokens": [
          50476,
          400,
          498,
          291,
          9901,
          490,
          562,
          286,
          700,
          2956,
          50654
        ]
      },
      {
        "avg_logprob": -0.22396551717923383,
        "compression_ratio": 1.6491935483870968,
        "end": 758.8,
        "id": 289,
        "no_speech_prob": 0.000016442383639514446,
        "seek": 75064,
        "start": 756.4399999999999,
        "temperature": 0,
        "text": " at the fetch function in this whole series,",
        "tokens": [
          50654,
          412,
          264,
          23673,
          2445,
          294,
          341,
          1379,
          2638,
          11,
          50772
        ]
      },
      {
        "avg_logprob": -0.22396551717923383,
        "compression_ratio": 1.6491935483870968,
        "end": 763.4399999999999,
        "id": 290,
        "no_speech_prob": 0.000016442383639514446,
        "seek": 75064,
        "start": 758.8,
        "temperature": 0,
        "text": " when response comes back after a fetch call,",
        "tokens": [
          50772,
          562,
          4134,
          1487,
          646,
          934,
          257,
          23673,
          818,
          11,
          51004
        ]
      },
      {
        "avg_logprob": -0.22396551717923383,
        "compression_ratio": 1.6491935483870968,
        "end": 764.76,
        "id": 291,
        "no_speech_prob": 0.000016442383639514446,
        "seek": 75064,
        "start": 763.4399999999999,
        "temperature": 0,
        "text": " it comes in as a data stream.",
        "tokens": [
          51004,
          309,
          1487,
          294,
          382,
          257,
          1412,
          4309,
          13,
          51070
        ]
      },
      {
        "avg_logprob": -0.22396551717923383,
        "compression_ratio": 1.6491935483870968,
        "end": 766.42,
        "id": 292,
        "no_speech_prob": 0.000016442383639514446,
        "seek": 75064,
        "start": 764.76,
        "temperature": 0,
        "text": " So it's up to you to sort of specify",
        "tokens": [
          51070,
          407,
          309,
          311,
          493,
          281,
          291,
          281,
          1333,
          295,
          16500,
          51153
        ]
      },
      {
        "avg_logprob": -0.22396551717923383,
        "compression_ratio": 1.6491935483870968,
        "end": 767.52,
        "id": 293,
        "no_speech_prob": 0.000016442383639514446,
        "seek": 75064,
        "start": 766.42,
        "temperature": 0,
        "text": " how you want to read that.",
        "tokens": [
          51153,
          577,
          291,
          528,
          281,
          1401,
          300,
          13,
          51208
        ]
      },
      {
        "avg_logprob": -0.22396551717923383,
        "compression_ratio": 1.6491935483870968,
        "end": 769.64,
        "id": 294,
        "no_speech_prob": 0.000016442383639514446,
        "seek": 75064,
        "start": 767.52,
        "temperature": 0,
        "text": " Is it text, is it a blob, is it JSON?",
        "tokens": [
          51208,
          1119,
          309,
          2487,
          11,
          307,
          309,
          257,
          46115,
          11,
          307,
          309,
          31828,
          30,
          51314
        ]
      },
      {
        "avg_logprob": -0.22396551717923383,
        "compression_ratio": 1.6491935483870968,
        "end": 771.36,
        "id": 295,
        "no_speech_prob": 0.000016442383639514446,
        "seek": 75064,
        "start": 769.64,
        "temperature": 0,
        "text": " And I want to read it as JSON.",
        "tokens": [
          51314,
          400,
          286,
          528,
          281,
          1401,
          309,
          382,
          31828,
          13,
          51400
        ]
      },
      {
        "avg_logprob": -0.22396551717923383,
        "compression_ratio": 1.6491935483870968,
        "end": 774.04,
        "id": 296,
        "no_speech_prob": 0.000016442383639514446,
        "seek": 75064,
        "start": 771.36,
        "temperature": 0,
        "text": " So I've got to handle that in the client.",
        "tokens": [
          51400,
          407,
          286,
          600,
          658,
          281,
          4813,
          300,
          294,
          264,
          6423,
          13,
          51534
        ]
      },
      {
        "avg_logprob": -0.22396551717923383,
        "compression_ratio": 1.6491935483870968,
        "end": 777.68,
        "id": 297,
        "no_speech_prob": 0.000016442383639514446,
        "seek": 75064,
        "start": 774.04,
        "temperature": 0,
        "text": " And I could use another dot then.",
        "tokens": [
          51534,
          400,
          286,
          727,
          764,
          1071,
          5893,
          550,
          13,
          51716
        ]
      },
      {
        "avg_logprob": -0.2807814588824522,
        "compression_ratio": 1.5972850678733033,
        "end": 781.64,
        "id": 298,
        "no_speech_prob": 0.00001384598363074474,
        "seek": 77768,
        "start": 777.68,
        "temperature": 0,
        "text": " But it might be nice to make this an async function.",
        "tokens": [
          50364,
          583,
          309,
          1062,
          312,
          1481,
          281,
          652,
          341,
          364,
          382,
          34015,
          2445,
          13,
          50562
        ]
      },
      {
        "avg_logprob": -0.2807814588824522,
        "compression_ratio": 1.5972850678733033,
        "end": 784.12,
        "id": 299,
        "no_speech_prob": 0.00001384598363074474,
        "seek": 77768,
        "start": 781.64,
        "temperature": 0,
        "text": " This callback inside of getCurrentPosition",
        "tokens": [
          50562,
          639,
          818,
          3207,
          1854,
          295,
          483,
          34,
          374,
          1753,
          47,
          5830,
          50686
        ]
      },
      {
        "avg_logprob": -0.2807814588824522,
        "compression_ratio": 1.5972850678733033,
        "end": 786.9599999999999,
        "id": 300,
        "no_speech_prob": 0.00001384598363074474,
        "seek": 77768,
        "start": 784.12,
        "temperature": 0,
        "text": " can actually be an async function with an async keyword.",
        "tokens": [
          50686,
          393,
          767,
          312,
          364,
          382,
          34015,
          2445,
          365,
          364,
          382,
          34015,
          20428,
          13,
          50828
        ]
      },
      {
        "avg_logprob": -0.2807814588824522,
        "compression_ratio": 1.5972850678733033,
        "end": 788.56,
        "id": 301,
        "no_speech_prob": 0.00001384598363074474,
        "seek": 77768,
        "start": 786.9599999999999,
        "temperature": 0,
        "text": " And then I could use await.",
        "tokens": [
          50828,
          400,
          550,
          286,
          727,
          764,
          19670,
          13,
          50908
        ]
      },
      {
        "avg_logprob": -0.2807814588824522,
        "compression_ratio": 1.5972850678733033,
        "end": 791.5999999999999,
        "id": 302,
        "no_speech_prob": 0.00001384598363074474,
        "seek": 77768,
        "start": 788.56,
        "temperature": 0,
        "text": " So I'm going to say const response equals await",
        "tokens": [
          50908,
          407,
          286,
          478,
          516,
          281,
          584,
          1817,
          4134,
          6915,
          19670,
          51060
        ]
      },
      {
        "avg_logprob": -0.2807814588824522,
        "compression_ratio": 1.5972850678733033,
        "end": 793.6999999999999,
        "id": 303,
        "no_speech_prob": 0.00001384598363074474,
        "seek": 77768,
        "start": 791.5999999999999,
        "temperature": 0,
        "text": " fetch API options.",
        "tokens": [
          51060,
          23673,
          9362,
          3956,
          13,
          51165
        ]
      },
      {
        "avg_logprob": -0.2807814588824522,
        "compression_ratio": 1.5972850678733033,
        "end": 798.9599999999999,
        "id": 304,
        "no_speech_prob": 0.00001384598363074474,
        "seek": 77768,
        "start": 794.76,
        "temperature": 0,
        "text": " Then const JSON, maybe I'll call it data.",
        "tokens": [
          51218,
          1396,
          1817,
          31828,
          11,
          1310,
          286,
          603,
          818,
          309,
          1412,
          13,
          51428
        ]
      },
      {
        "avg_logprob": -0.2807814588824522,
        "compression_ratio": 1.5972850678733033,
        "end": 801.92,
        "id": 305,
        "no_speech_prob": 0.00001384598363074474,
        "seek": 77768,
        "start": 798.9599999999999,
        "temperature": 0,
        "text": " It was await response dot JSON.",
        "tokens": [
          51428,
          467,
          390,
          19670,
          4134,
          5893,
          31828,
          13,
          51576
        ]
      },
      {
        "avg_logprob": -0.2807814588824522,
        "compression_ratio": 1.5972850678733033,
        "end": 804.78,
        "id": 306,
        "no_speech_prob": 0.00001384598363074474,
        "seek": 77768,
        "start": 801.92,
        "temperature": 0,
        "text": " And then console log that data.",
        "tokens": [
          51576,
          400,
          550,
          11076,
          3565,
          300,
          1412,
          13,
          51719
        ]
      },
      {
        "avg_logprob": -0.23307510605432036,
        "compression_ratio": 1.626865671641791,
        "end": 806.06,
        "id": 307,
        "no_speech_prob": 0.000018342841940466315,
        "seek": 80478,
        "start": 804.78,
        "temperature": 0,
        "text": " Okay, now I don't have to,",
        "tokens": [
          50364,
          1033,
          11,
          586,
          286,
          500,
          380,
          362,
          281,
          11,
          50428
        ]
      },
      {
        "avg_logprob": -0.23307510605432036,
        "compression_ratio": 1.626865671641791,
        "end": 807.78,
        "id": 308,
        "no_speech_prob": 0.000018342841940466315,
        "seek": 80478,
        "start": 806.06,
        "temperature": 0,
        "text": " I didn't change anything in the server.",
        "tokens": [
          50428,
          286,
          994,
          380,
          1319,
          1340,
          294,
          264,
          7154,
          13,
          50514
        ]
      },
      {
        "avg_logprob": -0.23307510605432036,
        "compression_ratio": 1.626865671641791,
        "end": 809.8199999999999,
        "id": 309,
        "no_speech_prob": 0.000018342841940466315,
        "seek": 80478,
        "start": 807.78,
        "temperature": 0,
        "text": " But even if I did, NodeMod's going to rerun it for me.",
        "tokens": [
          50514,
          583,
          754,
          498,
          286,
          630,
          11,
          38640,
          44,
          378,
          311,
          516,
          281,
          43819,
          409,
          309,
          337,
          385,
          13,
          50616
        ]
      },
      {
        "avg_logprob": -0.23307510605432036,
        "compression_ratio": 1.626865671641791,
        "end": 812.38,
        "id": 310,
        "no_speech_prob": 0.000018342841940466315,
        "seek": 80478,
        "start": 809.8199999999999,
        "temperature": 0,
        "text": " Now I should be able to refresh here.",
        "tokens": [
          50616,
          823,
          286,
          820,
          312,
          1075,
          281,
          15134,
          510,
          13,
          50744
        ]
      },
      {
        "avg_logprob": -0.23307510605432036,
        "compression_ratio": 1.626865671641791,
        "end": 815.22,
        "id": 311,
        "no_speech_prob": 0.000018342841940466315,
        "seek": 80478,
        "start": 812.38,
        "temperature": 0,
        "text": " Ah, data has already been declared.",
        "tokens": [
          50744,
          2438,
          11,
          1412,
          575,
          1217,
          668,
          15489,
          13,
          50886
        ]
      },
      {
        "avg_logprob": -0.23307510605432036,
        "compression_ratio": 1.626865671641791,
        "end": 817.4599999999999,
        "id": 312,
        "no_speech_prob": 0.000018342841940466315,
        "seek": 80478,
        "start": 815.22,
        "temperature": 0,
        "text": " I'm reusing, I already used data up here.",
        "tokens": [
          50886,
          286,
          478,
          319,
          7981,
          11,
          286,
          1217,
          1143,
          1412,
          493,
          510,
          13,
          50998
        ]
      },
      {
        "avg_logprob": -0.23307510605432036,
        "compression_ratio": 1.626865671641791,
        "end": 819.12,
        "id": 313,
        "no_speech_prob": 0.000018342841940466315,
        "seek": 80478,
        "start": 817.4599999999999,
        "temperature": 0,
        "text": " So let's just call this JSON.",
        "tokens": [
          50998,
          407,
          718,
          311,
          445,
          818,
          341,
          31828,
          13,
          51081
        ]
      },
      {
        "avg_logprob": -0.23307510605432036,
        "compression_ratio": 1.626865671641791,
        "end": 823.8199999999999,
        "id": 314,
        "no_speech_prob": 0.000018342841940466315,
        "seek": 80478,
        "start": 820.72,
        "temperature": 0,
        "text": " One of the nice things about using const or let",
        "tokens": [
          51161,
          1485,
          295,
          264,
          1481,
          721,
          466,
          1228,
          1817,
          420,
          718,
          51316
        ]
      },
      {
        "avg_logprob": -0.23307510605432036,
        "compression_ratio": 1.626865671641791,
        "end": 826.14,
        "id": 315,
        "no_speech_prob": 0.000018342841940466315,
        "seek": 80478,
        "start": 823.8199999999999,
        "temperature": 0,
        "text": " is it's going to enforce these kind of things",
        "tokens": [
          51316,
          307,
          309,
          311,
          516,
          281,
          24825,
          613,
          733,
          295,
          721,
          51432
        ]
      },
      {
        "avg_logprob": -0.23307510605432036,
        "compression_ratio": 1.626865671641791,
        "end": 828.24,
        "id": 316,
        "no_speech_prob": 0.000018342841940466315,
        "seek": 80478,
        "start": 826.14,
        "temperature": 0,
        "text": " to not by accident use the same variable.",
        "tokens": [
          51432,
          281,
          406,
          538,
          6398,
          764,
          264,
          912,
          7006,
          13,
          51537
        ]
      },
      {
        "avg_logprob": -0.23307510605432036,
        "compression_ratio": 1.626865671641791,
        "end": 834.5,
        "id": 317,
        "no_speech_prob": 0.000018342841940466315,
        "seek": 80478,
        "start": 829.5,
        "temperature": 0,
        "text": " And there it is, status success.",
        "tokens": [
          51600,
          400,
          456,
          309,
          307,
          11,
          6558,
          2245,
          13,
          51850
        ]
      },
      {
        "avg_logprob": -0.21077635481550888,
        "compression_ratio": 1.695852534562212,
        "end": 838.06,
        "id": 318,
        "no_speech_prob": 0.0007672888459637761,
        "seek": 83478,
        "start": 835.3399999999999,
        "temperature": 0,
        "text": " And latitude and longitude.",
        "tokens": [
          50392,
          400,
          45436,
          293,
          938,
          4377,
          13,
          50528
        ]
      },
      {
        "avg_logprob": -0.21077635481550888,
        "compression_ratio": 1.695852534562212,
        "end": 840.26,
        "id": 319,
        "no_speech_prob": 0.0007672888459637761,
        "seek": 83478,
        "start": 838.06,
        "temperature": 0,
        "text": " We have completed this feature.",
        "tokens": [
          50528,
          492,
          362,
          7365,
          341,
          4111,
          13,
          50638
        ]
      },
      {
        "avg_logprob": -0.21077635481550888,
        "compression_ratio": 1.695852534562212,
        "end": 845.26,
        "id": 320,
        "no_speech_prob": 0.0007672888459637761,
        "seek": 83478,
        "start": 840.26,
        "temperature": 0,
        "text": " To review, we have now a server that hosts a static file.",
        "tokens": [
          50638,
          1407,
          3131,
          11,
          321,
          362,
          586,
          257,
          7154,
          300,
          21573,
          257,
          13437,
          3991,
          13,
          50888
        ]
      },
      {
        "avg_logprob": -0.21077635481550888,
        "compression_ratio": 1.695852534562212,
        "end": 850.5799999999999,
        "id": 321,
        "no_speech_prob": 0.0007672888459637761,
        "seek": 83478,
        "start": 846.9,
        "temperature": 0,
        "text": " So when I go and load the server into my browser,",
        "tokens": [
          50970,
          407,
          562,
          286,
          352,
          293,
          3677,
          264,
          7154,
          666,
          452,
          11185,
          11,
          51154
        ]
      },
      {
        "avg_logprob": -0.21077635481550888,
        "compression_ratio": 1.695852534562212,
        "end": 852.5,
        "id": 322,
        "no_speech_prob": 0.0007672888459637761,
        "seek": 83478,
        "start": 850.5799999999999,
        "temperature": 0,
        "text": " I see index.html.",
        "tokens": [
          51154,
          286,
          536,
          8186,
          13,
          357,
          15480,
          13,
          51250
        ]
      },
      {
        "avg_logprob": -0.21077635481550888,
        "compression_ratio": 1.695852534562212,
        "end": 855.14,
        "id": 323,
        "no_speech_prob": 0.0007672888459637761,
        "seek": 83478,
        "start": 852.5,
        "temperature": 0,
        "text": " The JavaScript index.html geolocates",
        "tokens": [
          51250,
          440,
          15778,
          8186,
          13,
          357,
          15480,
          1519,
          401,
          905,
          1024,
          51382
        ]
      },
      {
        "avg_logprob": -0.21077635481550888,
        "compression_ratio": 1.695852534562212,
        "end": 856.22,
        "id": 324,
        "no_speech_prob": 0.0007672888459637761,
        "seek": 83478,
        "start": 855.14,
        "temperature": 0,
        "text": " the latitude and longitude,",
        "tokens": [
          51382,
          264,
          45436,
          293,
          938,
          4377,
          11,
          51436
        ]
      },
      {
        "avg_logprob": -0.21077635481550888,
        "compression_ratio": 1.695852534562212,
        "end": 858.78,
        "id": 325,
        "no_speech_prob": 0.0007672888459637761,
        "seek": 83478,
        "start": 856.22,
        "temperature": 0,
        "text": " sends that with a post to the server.",
        "tokens": [
          51436,
          14790,
          300,
          365,
          257,
          2183,
          281,
          264,
          7154,
          13,
          51564
        ]
      },
      {
        "avg_logprob": -0.21077635481550888,
        "compression_ratio": 1.695852534562212,
        "end": 861.76,
        "id": 326,
        "no_speech_prob": 0.0007672888459637761,
        "seek": 83478,
        "start": 858.78,
        "temperature": 0,
        "text": " The server receives it, console logs it out,",
        "tokens": [
          51564,
          440,
          7154,
          20717,
          309,
          11,
          11076,
          20820,
          309,
          484,
          11,
          51713
        ]
      },
      {
        "avg_logprob": -0.21077635481550888,
        "compression_ratio": 1.695852534562212,
        "end": 864.6999999999999,
        "id": 327,
        "no_speech_prob": 0.0007672888459637761,
        "seek": 83478,
        "start": 861.76,
        "temperature": 0,
        "text": " and sends it back saying I got it.",
        "tokens": [
          51713,
          293,
          14790,
          309,
          646,
          1566,
          286,
          658,
          309,
          13,
          51860
        ]
      },
      {
        "avg_logprob": -0.18800499208511845,
        "compression_ratio": 1.8662207357859533,
        "end": 868.26,
        "id": 328,
        "no_speech_prob": 0.00008092733332887292,
        "seek": 86470,
        "start": 865.5,
        "temperature": 0,
        "text": " That handshake is complete, the data is exchanged.",
        "tokens": [
          50404,
          663,
          2377,
          34593,
          307,
          3566,
          11,
          264,
          1412,
          307,
          38378,
          13,
          50542
        ]
      },
      {
        "avg_logprob": -0.18800499208511845,
        "compression_ratio": 1.8662207357859533,
        "end": 869.74,
        "id": 329,
        "no_speech_prob": 0.00008092733332887292,
        "seek": 86470,
        "start": 868.26,
        "temperature": 0,
        "text": " Usually when I finish these videos,",
        "tokens": [
          50542,
          11419,
          562,
          286,
          2413,
          613,
          2145,
          11,
          50616
        ]
      },
      {
        "avg_logprob": -0.18800499208511845,
        "compression_ratio": 1.8662207357859533,
        "end": 871.34,
        "id": 330,
        "no_speech_prob": 0.00008092733332887292,
        "seek": 86470,
        "start": 869.74,
        "temperature": 0,
        "text": " I like to give you a little exercise to do.",
        "tokens": [
          50616,
          286,
          411,
          281,
          976,
          291,
          257,
          707,
          5380,
          281,
          360,
          13,
          50696
        ]
      },
      {
        "avg_logprob": -0.18800499208511845,
        "compression_ratio": 1.8662207357859533,
        "end": 872.5400000000001,
        "id": 331,
        "no_speech_prob": 0.00008092733332887292,
        "seek": 86470,
        "start": 871.34,
        "temperature": 0,
        "text": " So here's one.",
        "tokens": [
          50696,
          407,
          510,
          311,
          472,
          13,
          50756
        ]
      },
      {
        "avg_logprob": -0.18800499208511845,
        "compression_ratio": 1.8662207357859533,
        "end": 874.5,
        "id": 332,
        "no_speech_prob": 0.00008092733332887292,
        "seek": 86470,
        "start": 872.5400000000001,
        "temperature": 0,
        "text": " The goal, what I'm going to do in the next video",
        "tokens": [
          50756,
          440,
          3387,
          11,
          437,
          286,
          478,
          516,
          281,
          360,
          294,
          264,
          958,
          960,
          50854
        ]
      },
      {
        "avg_logprob": -0.18800499208511845,
        "compression_ratio": 1.8662207357859533,
        "end": 876.76,
        "id": 333,
        "no_speech_prob": 0.00008092733332887292,
        "seek": 86470,
        "start": 874.5,
        "temperature": 0,
        "text": " is actually save the latitude and longitude",
        "tokens": [
          50854,
          307,
          767,
          3155,
          264,
          45436,
          293,
          938,
          4377,
          50967
        ]
      },
      {
        "avg_logprob": -0.18800499208511845,
        "compression_ratio": 1.8662207357859533,
        "end": 878.62,
        "id": 334,
        "no_speech_prob": 0.00008092733332887292,
        "seek": 86470,
        "start": 876.76,
        "temperature": 0,
        "text": " along with a timestamp to a database",
        "tokens": [
          50967,
          2051,
          365,
          257,
          49108,
          1215,
          281,
          257,
          8149,
          51060
        ]
      },
      {
        "avg_logprob": -0.18800499208511845,
        "compression_ratio": 1.8662207357859533,
        "end": 880.5,
        "id": 335,
        "no_speech_prob": 0.00008092733332887292,
        "seek": 86470,
        "start": 878.62,
        "temperature": 0,
        "text": " so that I can keep this history",
        "tokens": [
          51060,
          370,
          300,
          286,
          393,
          1066,
          341,
          2503,
          51154
        ]
      },
      {
        "avg_logprob": -0.18800499208511845,
        "compression_ratio": 1.8662207357859533,
        "end": 882.8000000000001,
        "id": 336,
        "no_speech_prob": 0.00008092733332887292,
        "seek": 86470,
        "start": 880.5,
        "temperature": 0,
        "text": " of all of my latitude and longitudes over time.",
        "tokens": [
          51154,
          295,
          439,
          295,
          452,
          45436,
          293,
          938,
          16451,
          670,
          565,
          13,
          51269
        ]
      },
      {
        "avg_logprob": -0.18800499208511845,
        "compression_ratio": 1.8662207357859533,
        "end": 884.62,
        "id": 337,
        "no_speech_prob": 0.00008092733332887292,
        "seek": 86470,
        "start": 882.8000000000001,
        "temperature": 0,
        "text": " Two steps along the way to using a database",
        "tokens": [
          51269,
          4453,
          4439,
          2051,
          264,
          636,
          281,
          1228,
          257,
          8149,
          51360
        ]
      },
      {
        "avg_logprob": -0.18800499208511845,
        "compression_ratio": 1.8662207357859533,
        "end": 886.9200000000001,
        "id": 338,
        "no_speech_prob": 0.00008092733332887292,
        "seek": 86470,
        "start": 884.62,
        "temperature": 0,
        "text": " is number one, just make a variable.",
        "tokens": [
          51360,
          307,
          1230,
          472,
          11,
          445,
          652,
          257,
          7006,
          13,
          51475
        ]
      },
      {
        "avg_logprob": -0.18800499208511845,
        "compression_ratio": 1.8662207357859533,
        "end": 888.26,
        "id": 339,
        "no_speech_prob": 0.00008092733332887292,
        "seek": 86470,
        "start": 886.9200000000001,
        "temperature": 0,
        "text": " A variable like an array.",
        "tokens": [
          51475,
          316,
          7006,
          411,
          364,
          10225,
          13,
          51542
        ]
      },
      {
        "avg_logprob": -0.18800499208511845,
        "compression_ratio": 1.8662207357859533,
        "end": 890.74,
        "id": 340,
        "no_speech_prob": 0.00008092733332887292,
        "seek": 86470,
        "start": 888.26,
        "temperature": 0,
        "text": " Every time you get that latitude and longitude,",
        "tokens": [
          51542,
          2048,
          565,
          291,
          483,
          300,
          45436,
          293,
          938,
          4377,
          11,
          51666
        ]
      },
      {
        "avg_logprob": -0.18800499208511845,
        "compression_ratio": 1.8662207357859533,
        "end": 893.58,
        "id": 341,
        "no_speech_prob": 0.00008092733332887292,
        "seek": 86470,
        "start": 890.74,
        "temperature": 0,
        "text": " save that latitude and longitude into an array.",
        "tokens": [
          51666,
          3155,
          300,
          45436,
          293,
          938,
          4377,
          666,
          364,
          10225,
          13,
          51808
        ]
      },
      {
        "avg_logprob": -0.19362315586635045,
        "compression_ratio": 1.8848484848484848,
        "end": 894.94,
        "id": 342,
        "no_speech_prob": 0.00024923120508901775,
        "seek": 89358,
        "start": 893.58,
        "temperature": 0,
        "text": " That is persistence.",
        "tokens": [
          50364,
          663,
          307,
          37617,
          13,
          50432
        ]
      },
      {
        "avg_logprob": -0.19362315586635045,
        "compression_ratio": 1.8848484848484848,
        "end": 896.5,
        "id": 343,
        "no_speech_prob": 0.00024923120508901775,
        "seek": 89358,
        "start": 894.94,
        "temperature": 0,
        "text": " Of course, once you quit the server",
        "tokens": [
          50432,
          2720,
          1164,
          11,
          1564,
          291,
          10366,
          264,
          7154,
          50510
        ]
      },
      {
        "avg_logprob": -0.19362315586635045,
        "compression_ratio": 1.8848484848484848,
        "end": 898.5400000000001,
        "id": 344,
        "no_speech_prob": 0.00024923120508901775,
        "seek": 89358,
        "start": 896.5,
        "temperature": 0,
        "text": " and restart the server, it's lost.",
        "tokens": [
          50510,
          293,
          21022,
          264,
          7154,
          11,
          309,
          311,
          2731,
          13,
          50612
        ]
      },
      {
        "avg_logprob": -0.19362315586635045,
        "compression_ratio": 1.8848484848484848,
        "end": 900.22,
        "id": 345,
        "no_speech_prob": 0.00024923120508901775,
        "seek": 89358,
        "start": 898.5400000000001,
        "temperature": 0,
        "text": " So another thing you could try to do,",
        "tokens": [
          50612,
          407,
          1071,
          551,
          291,
          727,
          853,
          281,
          360,
          11,
          50696
        ]
      },
      {
        "avg_logprob": -0.19362315586635045,
        "compression_ratio": 1.8848484848484848,
        "end": 901.74,
        "id": 346,
        "no_speech_prob": 0.00024923120508901775,
        "seek": 89358,
        "start": 900.22,
        "temperature": 0,
        "text": " and this requires some detective work,",
        "tokens": [
          50696,
          293,
          341,
          7029,
          512,
          25571,
          589,
          11,
          50772
        ]
      },
      {
        "avg_logprob": -0.19362315586635045,
        "compression_ratio": 1.8848484848484848,
        "end": 904.58,
        "id": 347,
        "no_speech_prob": 0.00024923120508901775,
        "seek": 89358,
        "start": 901.74,
        "temperature": 0,
        "text": " is look up the node file system package.",
        "tokens": [
          50772,
          307,
          574,
          493,
          264,
          9984,
          3991,
          1185,
          7372,
          13,
          50914
        ]
      },
      {
        "avg_logprob": -0.19362315586635045,
        "compression_ratio": 1.8848484848484848,
        "end": 906.7800000000001,
        "id": 348,
        "no_speech_prob": 0.00024923120508901775,
        "seek": 89358,
        "start": 904.58,
        "temperature": 0,
        "text": " Maybe you could use the node file system package",
        "tokens": [
          50914,
          2704,
          291,
          727,
          764,
          264,
          9984,
          3991,
          1185,
          7372,
          51024
        ]
      },
      {
        "avg_logprob": -0.19362315586635045,
        "compression_ratio": 1.8848484848484848,
        "end": 909.82,
        "id": 349,
        "no_speech_prob": 0.00024923120508901775,
        "seek": 89358,
        "start": 906.7800000000001,
        "temperature": 0,
        "text": " to save all of the stuff in that array to a text file.",
        "tokens": [
          51024,
          281,
          3155,
          439,
          295,
          264,
          1507,
          294,
          300,
          10225,
          281,
          257,
          2487,
          3991,
          13,
          51176
        ]
      },
      {
        "avg_logprob": -0.19362315586635045,
        "compression_ratio": 1.8848484848484848,
        "end": 911.1800000000001,
        "id": 350,
        "no_speech_prob": 0.00024923120508901775,
        "seek": 89358,
        "start": 909.82,
        "temperature": 0,
        "text": " And you could always load that text file.",
        "tokens": [
          51176,
          400,
          291,
          727,
          1009,
          3677,
          300,
          2487,
          3991,
          13,
          51244
        ]
      },
      {
        "avg_logprob": -0.19362315586635045,
        "compression_ratio": 1.8848484848484848,
        "end": 913.22,
        "id": 351,
        "no_speech_prob": 0.00024923120508901775,
        "seek": 89358,
        "start": 911.1800000000001,
        "temperature": 0,
        "text": " Maybe it's JSON format, maybe it's plain text,",
        "tokens": [
          51244,
          2704,
          309,
          311,
          31828,
          7877,
          11,
          1310,
          309,
          311,
          11121,
          2487,
          11,
          51346
        ]
      },
      {
        "avg_logprob": -0.19362315586635045,
        "compression_ratio": 1.8848484848484848,
        "end": 915.0600000000001,
        "id": 352,
        "no_speech_prob": 0.00024923120508901775,
        "seek": 89358,
        "start": 913.22,
        "temperature": 0,
        "text": " maybe it's even CSV.",
        "tokens": [
          51346,
          1310,
          309,
          311,
          754,
          48814,
          13,
          51438
        ]
      },
      {
        "avg_logprob": -0.19362315586635045,
        "compression_ratio": 1.8848484848484848,
        "end": 916.1,
        "id": 353,
        "no_speech_prob": 0.00024923120508901775,
        "seek": 89358,
        "start": 915.0600000000001,
        "temperature": 0,
        "text": " Also, while you're doing this,",
        "tokens": [
          51438,
          2743,
          11,
          1339,
          291,
          434,
          884,
          341,
          11,
          51490
        ]
      },
      {
        "avg_logprob": -0.19362315586635045,
        "compression_ratio": 1.8848484848484848,
        "end": 918.22,
        "id": 354,
        "no_speech_prob": 0.00024923120508901775,
        "seek": 89358,
        "start": 916.1,
        "temperature": 0,
        "text": " whether you're saving the latitude and longitude",
        "tokens": [
          51490,
          1968,
          291,
          434,
          6816,
          264,
          45436,
          293,
          938,
          4377,
          51596
        ]
      },
      {
        "avg_logprob": -0.19362315586635045,
        "compression_ratio": 1.8848484848484848,
        "end": 919.46,
        "id": 355,
        "no_speech_prob": 0.00024923120508901775,
        "seek": 89358,
        "start": 918.22,
        "temperature": 0,
        "text": " to an array or a text file,",
        "tokens": [
          51596,
          281,
          364,
          10225,
          420,
          257,
          2487,
          3991,
          11,
          51658
        ]
      },
      {
        "avg_logprob": -0.19362315586635045,
        "compression_ratio": 1.8848484848484848,
        "end": 921.6,
        "id": 356,
        "no_speech_prob": 0.00024923120508901775,
        "seek": 89358,
        "start": 919.46,
        "temperature": 0,
        "text": " you might want to add a button to your HTML page",
        "tokens": [
          51658,
          291,
          1062,
          528,
          281,
          909,
          257,
          2960,
          281,
          428,
          17995,
          3028,
          51765
        ]
      },
      {
        "avg_logprob": -0.19362315586635045,
        "compression_ratio": 1.8848484848484848,
        "end": 923.4200000000001,
        "id": 357,
        "no_speech_prob": 0.00024923120508901775,
        "seek": 89358,
        "start": 921.6,
        "temperature": 0,
        "text": " so that every time you click that button,",
        "tokens": [
          51765,
          370,
          300,
          633,
          565,
          291,
          2052,
          300,
          2960,
          11,
          51856
        ]
      },
      {
        "avg_logprob": -0.22742315633407492,
        "compression_ratio": 1.7623762376237624,
        "end": 925.66,
        "id": 358,
        "no_speech_prob": 0.000053910694987280294,
        "seek": 92342,
        "start": 924.26,
        "temperature": 0,
        "text": " that's when you send the latitude and longitude",
        "tokens": [
          50406,
          300,
          311,
          562,
          291,
          2845,
          264,
          45436,
          293,
          938,
          4377,
          50476
        ]
      },
      {
        "avg_logprob": -0.22742315633407492,
        "compression_ratio": 1.7623762376237624,
        "end": 927.3399999999999,
        "id": 359,
        "no_speech_prob": 0.000053910694987280294,
        "seek": 92342,
        "start": 925.66,
        "temperature": 0,
        "text": " to the server itself.",
        "tokens": [
          50476,
          281,
          264,
          7154,
          2564,
          13,
          50560
        ]
      },
      {
        "avg_logprob": -0.22742315633407492,
        "compression_ratio": 1.7623762376237624,
        "end": 929.6999999999999,
        "id": 360,
        "no_speech_prob": 0.000053910694987280294,
        "seek": 92342,
        "start": 927.3399999999999,
        "temperature": 0,
        "text": " So this is something that you can really start with,",
        "tokens": [
          50560,
          407,
          341,
          307,
          746,
          300,
          291,
          393,
          534,
          722,
          365,
          11,
          50678
        ]
      },
      {
        "avg_logprob": -0.22742315633407492,
        "compression_ratio": 1.7623762376237624,
        "end": 931.02,
        "id": 361,
        "no_speech_prob": 0.000053910694987280294,
        "seek": 92342,
        "start": 929.6999999999999,
        "temperature": 0,
        "text": " and I will actually implement this",
        "tokens": [
          50678,
          293,
          286,
          486,
          767,
          4445,
          341,
          50744
        ]
      },
      {
        "avg_logprob": -0.22742315633407492,
        "compression_ratio": 1.7623762376237624,
        "end": 932.6999999999999,
        "id": 362,
        "no_speech_prob": 0.000053910694987280294,
        "seek": 92342,
        "start": 931.02,
        "temperature": 0,
        "text": " and have two examples that show it,",
        "tokens": [
          50744,
          293,
          362,
          732,
          5110,
          300,
          855,
          309,
          11,
          50828
        ]
      },
      {
        "avg_logprob": -0.22742315633407492,
        "compression_ratio": 1.7623762376237624,
        "end": 934.18,
        "id": 363,
        "no_speech_prob": 0.000053910694987280294,
        "seek": 92342,
        "start": 932.6999999999999,
        "temperature": 0,
        "text": " just storing it in an array",
        "tokens": [
          50828,
          445,
          26085,
          309,
          294,
          364,
          10225,
          50902
        ]
      },
      {
        "avg_logprob": -0.22742315633407492,
        "compression_ratio": 1.7623762376237624,
        "end": 936.64,
        "id": 364,
        "no_speech_prob": 0.000053910694987280294,
        "seek": 92342,
        "start": 934.18,
        "temperature": 0,
        "text": " and then saving it to a local JSON file.",
        "tokens": [
          50902,
          293,
          550,
          6816,
          309,
          281,
          257,
          2654,
          31828,
          3991,
          13,
          51025
        ]
      },
      {
        "avg_logprob": -0.22742315633407492,
        "compression_ratio": 1.7623762376237624,
        "end": 938.6999999999999,
        "id": 365,
        "no_speech_prob": 0.000053910694987280294,
        "seek": 92342,
        "start": 936.64,
        "temperature": 0,
        "text": " But what I'm going to come back to in the next video",
        "tokens": [
          51025,
          583,
          437,
          286,
          478,
          516,
          281,
          808,
          646,
          281,
          294,
          264,
          958,
          960,
          51128
        ]
      },
      {
        "avg_logprob": -0.22742315633407492,
        "compression_ratio": 1.7623762376237624,
        "end": 939.86,
        "id": 366,
        "no_speech_prob": 0.000053910694987280294,
        "seek": 92342,
        "start": 938.6999999999999,
        "temperature": 0,
        "text": " is a bit more direct.",
        "tokens": [
          51128,
          307,
          257,
          857,
          544,
          2047,
          13,
          51186
        ]
      },
      {
        "avg_logprob": -0.22742315633407492,
        "compression_ratio": 1.7623762376237624,
        "end": 943.9799999999999,
        "id": 367,
        "no_speech_prob": 0.000053910694987280294,
        "seek": 92342,
        "start": 939.86,
        "temperature": 0,
        "text": " I'm going to use a database node package called NEDB",
        "tokens": [
          51186,
          286,
          478,
          516,
          281,
          764,
          257,
          8149,
          9984,
          7372,
          1219,
          426,
          4731,
          33,
          51392
        ]
      },
      {
        "avg_logprob": -0.22742315633407492,
        "compression_ratio": 1.7623762376237624,
        "end": 946.38,
        "id": 368,
        "no_speech_prob": 0.000053910694987280294,
        "seek": 92342,
        "start": 943.9799999999999,
        "temperature": 0,
        "text": " to actually save to a database.",
        "tokens": [
          51392,
          281,
          767,
          3155,
          281,
          257,
          8149,
          13,
          51512
        ]
      },
      {
        "avg_logprob": -0.22742315633407492,
        "compression_ratio": 1.7623762376237624,
        "end": 947.74,
        "id": 369,
        "no_speech_prob": 0.000053910694987280294,
        "seek": 92342,
        "start": 946.38,
        "temperature": 0,
        "text": " And I'll talk about what a database is",
        "tokens": [
          51512,
          400,
          286,
          603,
          751,
          466,
          437,
          257,
          8149,
          307,
          51580
        ]
      },
      {
        "avg_logprob": -0.22742315633407492,
        "compression_ratio": 1.7623762376237624,
        "end": 949.98,
        "id": 370,
        "no_speech_prob": 0.000053910694987280294,
        "seek": 92342,
        "start": 947.74,
        "temperature": 0,
        "text": " and what some of the advantages are in the next video.",
        "tokens": [
          51580,
          293,
          437,
          512,
          295,
          264,
          14906,
          366,
          294,
          264,
          958,
          960,
          13,
          51692
        ]
      },
      {
        "avg_logprob": -0.22742315633407492,
        "compression_ratio": 1.7623762376237624,
        "end": 951.2199999999999,
        "id": 371,
        "no_speech_prob": 0.000053910694987280294,
        "seek": 92342,
        "start": 949.98,
        "temperature": 0,
        "text": " So see you there.",
        "tokens": [
          51692,
          407,
          536,
          291,
          456,
          13,
          51754
        ]
      },
      {
        "avg_logprob": -0.7926456744854267,
        "compression_ratio": 0.6428571428571429,
        "end": 955.0600000000001,
        "id": 372,
        "no_speech_prob": 0.4587705731391907,
        "seek": 95122,
        "start": 951.22,
        "temperature": 0,
        "text": " ♪♪♪",
        "tokens": [
          50406,
          220,
          158,
          247,
          103,
          158,
          247,
          103,
          158,
          247,
          103,
          50556
        ]
      }
    ],
    "transcription": " Welcome back. I don't know what you did since the last video, but I went and got a haircut. Thank you. In this video, I am going to do something important, a key feature here. What I want to do is take data from the client, the latitude and longitude, and send it to the server and have the server receive it. Ultimately, the point of this is for the server to eventually save that to a database. But I'm not there yet. I just want to say the client executes its own client-side JavaScript, gets the latitude and longitude, and sends that data to the server. And the server can just console log it. I'm going to need to look at three things in order to do this. I'm going to need to look at a concept known as routing. How do I set up a route in Express? This is the place, the endpoint for the API, the address at which I will send the data to and later also make a request to receive some data. I need to look at adding JSON parsing to Express. I need the route when it receives data to understand that data as JSON and make it readable in my code. And then I also need to look at adapting the fetch function to specify a POST request, a request that is posting data or sending data to the server. Let's start with the route. So I want to set up a route on my server. So the way that I do that is by specifying this particular route will either be a GET or a POST. In this case, I expect a POST request. So I am going to go and say app.post. Now, once I have a POST, I want to specify both the address, where I want to receive that POST, as well as a callback function where I'm going to look at the information coming in and send a response back. So let's set up that address. Let's set up the endpoint for this particular route, where I want to receive the POST. And I could call it anything I want, like unicorn, cupcake, rainbow. Let's call it rainbow. All right, I'll give it a name that's maybe more appropriate. I'm going to call it API. API, because what I'm really doing here is setting up an application programming interface. This is my API for clients to send data to me. Next, I'm going to set up a callback request response. And I'm using the ES6 JavaScript, ES6 arrow syntax, which is a sort of nice, clean way of putting this in here. So the function is here. And it has two arguments, request and response. The request variable holds everything that's contained within that request, all the data that's being sent, any information I need to know about that particular client that's sending the information. The response is a variable that I can use to send things back to the client. So I'm going to need, ultimately, to do stuff there. But for right now, I'm just going to say console.log request. And I'm just going to leave it like that. Because let's at least see if we can get this working and see if we can see something console.log there. The next step is to have my client send something to this particular endpoint with a post. I can now move over to the client code and set up the post itself using fetch. So I want to fetch my post over to slash API endpoint. So the client code, remember, this is confusing. Server in index.js, client in index.html. I'm going to move over here. What I'll do is I'll choose to send this latitude and longitude as soon as I've received it. This is what I did in the previous video. And so now what I can do is I'll set up. I'm just going to make up an object called data. And what I want to send is the latitude and longitude inside of an object. So that's what I want to send. So you might think all I need to do, if I were just doing a regular fetch, I just wanted to fetch something that's a regular get request like I did in previous videos, I would then just say this. Hey, fetch me the stuff from slash API. And I might actually do that a little later when I want to just do a get request and get the data that's in the database. But right now, what I want to do is send a post. In order to send a post, I need a second argument here, which is just a JavaScript object. I'm going to call it options. And that way, I can set it up separately as a variable and kind of examine it more closely. The first property that I need to put in options is the method that I'm using. And that method is a post. Now, there's a lot of information that you could put here under options. And if you want to look at all the possibilities, I refer you back to the MDN web docs. There's a nice page here about fetch. And we can kind of see all this stuff. OK, what mode and cache and credentials. But really what I need, what I want, the key thing that I want is body. So the body of the post request is where I'm packaging up all of my data. And you can see here, even though there are different ways to send the data, that what I want to do is send it as a string text. I want to stringify, take the JavaScript object data, and make it into a JSON string. So I want exactly this right here. And there's one other piece that's important. I am specifically sending data in JSON format. So it's useful to specify that in something called a header. A header is something that can be packaged along with any post or get request that's moving between client and server. And it's a way of adding some additional meta information. You can read more about this also on the Mozilla docs. Headers have a name, a colon, and a value. And so the one that I want to use here, I can actually just go and grab it from here, which is this content type application JSON. So I'm going to grab this. And I'm also going to put this here in my code now. And I don't need this. So even though there's more that I could put, the basic pieces that I need are, hey, I want this data to get sent as JSON. I want to tell you that it's going to be JSON. And I want to post it to the API. So now we're good, right? So I get the geolocation data. I put it into a JavaScript object. I package it as a post. And I send it to my endpoint. And in the server, I receive and console log it. Let's see what happens. So I'm going to say node index.js to run the server. I'm going to go back to the browser. I'm going to refresh my page. I probably put some console logging in the client to see what's there. But presumably now it's sensing the server. I don't see an error here. If I go back to the server, oh, whoa. So this is good news. It console logged something. As you might have noticed, there's a ton of information here as I scroll through this crazily. While there might be points at which I need to examine certain aspects of all of the metadata that's part of the request, all I really want to look at is the body. So let me now say console log request.body. Now, one thing I should mention, by the way, is notice how I have to rerun the server every single time I change it. There is a utility that I could use called nodemon or node monitor. And I could install it as a global node module. And if I choose to use it, I can say now nodemon for nodemonitor index.js. And any time I change the code, it's going to rewrite the server. It's going to start. Any time I change the code, it's going to rerun the server. So for example, I'm going to go here and say console.log, I got a request. And over here, you'll notice it restarted the server. Sometimes you can run into trouble with this if you're doing a lot of things at once. And I often prefer to manually stop and start the server. But for right now, this might be useful to see. So let me leave that there for right now. Then I'm going to go over here, hit refresh. And presumably, did it send the data? Oh, it must have sent the data. Presumably, it sent the data. I'll go back to the server. I got a request. Undefined. Undefined. Why is it undefined? I forgot one of the steps that I told you about. So I did two of them. I set up my route called API to receive the request. I also modified the fetch function to send data as a post. What I did not do was add to my server the ability to parse any incoming data as JSON. The way to do that is with the express function express.json. So similarly to how I used express.static to host, say, I want this server to host static files, I'm going to say express.json because I want this server to be able to understand incoming data as JSON. So in the code, I want to do just like I did up here, app.use express.static. I'm going to say app.use express.json. And then I can actually put some options in here to control or limit what is possible in terms of receiving data. So all of that is documented here on the Express website. Let's add just as an example this limit. So limit allows me to specify the maximum size of any body that's coming in. So I'll say limit, let's just say, one megabyte as a kind of starting point. So this protects against someone flooding my server with huge amounts of data, potentially. There's more to protecting your server than just that, but we're seeing an inkling of this here. All right, so now we can see Node has, NodeMon has restarted my server a bunch of times, and I should be able to go back to the web page and hit refresh. And it's going to get the latitude and longitude. And now I should see in the server, ha ha, I got a request. And I got the latitude and longitude. So the server has successfully received the data. This is what I wanted to do in this video. So in a way, I'm done. Oh, that's exciting. What I've actually done here is pretty terrible in that it is required that you complete a request. So if a get request or a post request is coming into the server, I need to do something to complete it. The nicest thing to do, if I'm trying to be polite here, which is, you know, it's good for your server to be polite, is send a response back. But at a minimum, I can just say response.end. So this is required to do something. What I think that I would like to do, which makes more sense, is say response.json, which is a function that will send data. I could also say response.send for just a short message as a string. But I want to send maybe an object back with some data in it. So I'm going to do response.json. And then I would say, I could say like status. I'm just making this up. Success. And then maybe I'll just repeat back what was sent to me. This is the therapy server. It listens and then it just repeats it back to you. So I'll say latitude, longitude, long. So this is what I want to send back now as JavaScript. Okay, so then in the client, I need to do something to receive this. So if I go back to index.html, what I have here calling the fetch function is where I'm sending the data, and fetch returns a promise. So I can just say.then and then handle the response. Response console.log that response. All right, so let's take a look at this in the browser now. I'm going to hit refresh. It's retrieving my latitude and longitude. It's going to send to the server. And then here we are, the response. Oh boy, look at this. Lat is not defined. Let's look at my code back on the server. Oh yes, these aren't a thing. Remember, everything is in request.body. So what I could do is say const data equals, I could put this in another variable. And then I could just say data.lat. I could have just said request.body.lat, data.long. So okay, there we go. Let's try this again. So the server should be restarting now any time I make changes because I'm running NodeMod. Then I can just go back to the client and hit refresh. And there it is, the response. Now this isn't what I was looking for. And if you recall from when I first looked at the fetch function in this whole series, when response comes back after a fetch call, it comes in as a data stream. So it's up to you to sort of specify how you want to read that. Is it text, is it a blob, is it JSON? And I want to read it as JSON. So I've got to handle that in the client. And I could use another dot then. But it might be nice to make this an async function. This callback inside of getCurrentPosition can actually be an async function with an async keyword. And then I could use await. So I'm going to say const response equals await fetch API options. Then const JSON, maybe I'll call it data. It was await response dot JSON. And then console log that data. Okay, now I don't have to, I didn't change anything in the server. But even if I did, NodeMod's going to rerun it for me. Now I should be able to refresh here. Ah, data has already been declared. I'm reusing, I already used data up here. So let's just call this JSON. One of the nice things about using const or let is it's going to enforce these kind of things to not by accident use the same variable. And there it is, status success. And latitude and longitude. We have completed this feature. To review, we have now a server that hosts a static file. So when I go and load the server into my browser, I see index.html. The JavaScript index.html geolocates the latitude and longitude, sends that with a post to the server. The server receives it, console logs it out, and sends it back saying I got it. That handshake is complete, the data is exchanged. Usually when I finish these videos, I like to give you a little exercise to do. So here's one. The goal, what I'm going to do in the next video is actually save the latitude and longitude along with a timestamp to a database so that I can keep this history of all of my latitude and longitudes over time. Two steps along the way to using a database is number one, just make a variable. A variable like an array. Every time you get that latitude and longitude, save that latitude and longitude into an array. That is persistence. Of course, once you quit the server and restart the server, it's lost. So another thing you could try to do, and this requires some detective work, is look up the node file system package. Maybe you could use the node file system package to save all of the stuff in that array to a text file. And you could always load that text file. Maybe it's JSON format, maybe it's plain text, maybe it's even CSV. Also, while you're doing this, whether you're saving the latitude and longitude to an array or a text file, you might want to add a button to your HTML page so that every time you click that button, that's when you send the latitude and longitude to the server itself. So this is something that you can really start with, and I will actually implement this and have two examples that show it, just storing it in an array and then saving it to a local JSON file. But what I'm going to come back to in the next video is a bit more direct. I'm going to use a database node package called NEDB to actually save to a database. And I'll talk about what a database is and what some of the advantages are in the next video. So see you there. ♪♪♪",
    "translation": null
  },
  "error": null,
  "status": "succeeded",
  "created_at": "2023-09-26T21:03:42.289428Z",
  "started_at": "2023-09-26T21:16:18.286681Z",
  "completed_at": "2023-09-26T21:21:06.31494Z",
  "webhook": "https://83ceaa0b612c.ngrok.app/?video_id=Kw5tC5nQMRY",
  "webhook_events_filter": [
    "completed"
  ],
  "metrics": {
    "predict_time": 288.028259
  },
  "urls": {
    "cancel": "https://api.replicate.com/v1/predictions/4fzrvzbbiel552lwjcbzl6so5m/cancel",
    "get": "https://api.replicate.com/v1/predictions/4fzrvzbbiel552lwjcbzl6so5m"
  }
}