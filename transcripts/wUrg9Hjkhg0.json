{
  "id": "nqkqfwbbp7impifzbuntg43nha",
  "version": "91ee9c0c3df30478510ff8c8a3a545add1ad0259ad3a9f78fba57fbc05ee64f7",
  "input": {
    "audio": "https://upcdn.io/FW25b4F/raw/coding-train/wUrg9Hjkhg0.m4a"
  },
  "logs": "Transcribe with large-v2 model\nDetected language: English\n  0%|          | 0/65700 [00:00<?, ?frames/s]\n  4%|▍         | 2692/65700 [00:06<02:24, 435.47frames/s]\n  9%|▊         | 5598/65700 [00:15<02:47, 359.89frames/s]\n 13%|█▎        | 8310/65700 [00:24<02:52, 331.90frames/s]\n 17%|█▋        | 11282/65700 [00:34<02:52, 315.69frames/s]\n 22%|██▏       | 14222/65700 [00:39<02:20, 367.27frames/s]\n 26%|██▌       | 17210/65700 [00:46<02:05, 387.90frames/s]\n 30%|███       | 19994/65700 [00:55<02:05, 363.08frames/s]\n 35%|███▍      | 22726/65700 [01:01<01:49, 391.32frames/s]\n 39%|███▉      | 25554/65700 [01:08<01:41, 397.29frames/s]\n 43%|████▎     | 28498/65700 [01:16<01:37, 381.54frames/s]\n 48%|████▊     | 31354/65700 [01:23<01:27, 390.89frames/s]\n 52%|█████▏    | 34206/65700 [01:30<01:19, 397.04frames/s]\n 57%|█████▋    | 37170/65700 [01:39<01:15, 378.46frames/s]\n 61%|██████    | 39966/65700 [01:45<01:06, 385.48frames/s]\n 65%|██████▍   | 42666/65700 [01:53<01:01, 372.59frames/s]\n 69%|██████▉   | 45498/65700 [01:59<00:50, 397.66frames/s]\n 74%|███████▍  | 48474/65700 [02:10<00:48, 353.94frames/s]\n 78%|███████▊  | 51378/65700 [02:18<00:40, 356.96frames/s]\n 82%|████████▏ | 53862/65700 [02:24<00:32, 364.26frames/s]\n 87%|████████▋ | 56862/65700 [02:30<00:21, 408.79frames/s]\n 91%|█████████ | 59658/65700 [02:36<00:14, 422.85frames/s]\n 95%|█████████▌| 62512/65700 [02:44<00:08, 387.27frames/s]\n 99%|█████████▊| 64728/65700 [02:53<00:02, 349.76frames/s]\n100%|██████████| 65700/65700 [02:54<00:00, 370.97frames/s]\n100%|██████████| 65700/65700 [02:54<00:00, 375.93frames/s]\n",
  "output": {
    "detected_language": "english",
    "segments": [
      {
        "avg_logprob": -0.2086286455671364,
        "compression_ratio": 1.575,
        "end": 2.24,
        "id": 0,
        "no_speech_prob": 0.0007096091285347939,
        "seek": 0,
        "start": 0,
        "temperature": 0,
        "text": " And we're back.",
        "tokens": [
          50364,
          400,
          321,
          434,
          646,
          13,
          50476
        ]
      },
      {
        "avg_logprob": -0.2086286455671364,
        "compression_ratio": 1.575,
        "end": 4.92,
        "id": 1,
        "no_speech_prob": 0.0007096091285347939,
        "seek": 0,
        "start": 2.24,
        "temperature": 0,
        "text": " I am ready in this video to show you",
        "tokens": [
          50476,
          286,
          669,
          1919,
          294,
          341,
          960,
          281,
          855,
          291,
          50610
        ]
      },
      {
        "avg_logprob": -0.2086286455671364,
        "compression_ratio": 1.575,
        "end": 8.16,
        "id": 2,
        "no_speech_prob": 0.0007096091285347939,
        "seek": 0,
        "start": 4.92,
        "temperature": 0,
        "text": " how to save the trained model with ml5.",
        "tokens": [
          50610,
          577,
          281,
          3155,
          264,
          8895,
          2316,
          365,
          23271,
          20,
          13,
          50772
        ]
      },
      {
        "avg_logprob": -0.2086286455671364,
        "compression_ratio": 1.575,
        "end": 10.72,
        "id": 3,
        "no_speech_prob": 0.0007096091285347939,
        "seek": 0,
        "start": 8.16,
        "temperature": 0,
        "text": " So if you recall, what I previously just did",
        "tokens": [
          50772,
          407,
          498,
          291,
          9901,
          11,
          437,
          286,
          8046,
          445,
          630,
          50900
        ]
      },
      {
        "avg_logprob": -0.2086286455671364,
        "compression_ratio": 1.575,
        "end": 14.200000000000001,
        "id": 4,
        "no_speech_prob": 0.0007096091285347939,
        "seek": 0,
        "start": 10.72,
        "temperature": 0,
        "text": " in the previous video is I added a feature",
        "tokens": [
          50900,
          294,
          264,
          3894,
          960,
          307,
          286,
          3869,
          257,
          4111,
          51074
        ]
      },
      {
        "avg_logprob": -0.2086286455671364,
        "compression_ratio": 1.575,
        "end": 16.92,
        "id": 5,
        "no_speech_prob": 0.0007096091285347939,
        "seek": 0,
        "start": 14.200000000000001,
        "temperature": 0,
        "text": " to my example which will load a data set",
        "tokens": [
          51074,
          281,
          452,
          1365,
          597,
          486,
          3677,
          257,
          1412,
          992,
          51210
        ]
      },
      {
        "avg_logprob": -0.2086286455671364,
        "compression_ratio": 1.575,
        "end": 18.92,
        "id": 6,
        "no_speech_prob": 0.0007096091285347939,
        "seek": 0,
        "start": 16.92,
        "temperature": 0,
        "text": " and immediately start training the model.",
        "tokens": [
          51210,
          293,
          4258,
          722,
          3097,
          264,
          2316,
          13,
          51310
        ]
      },
      {
        "avg_logprob": -0.2086286455671364,
        "compression_ratio": 1.575,
        "end": 23.6,
        "id": 7,
        "no_speech_prob": 0.0007096091285347939,
        "seek": 0,
        "start": 18.92,
        "temperature": 0,
        "text": " So you can see here a whole bunch of labeled xy points.",
        "tokens": [
          51310,
          407,
          291,
          393,
          536,
          510,
          257,
          1379,
          3840,
          295,
          21335,
          2031,
          88,
          2793,
          13,
          51544
        ]
      },
      {
        "avg_logprob": -0.2086286455671364,
        "compression_ratio": 1.575,
        "end": 26.92,
        "id": 8,
        "no_speech_prob": 0.0007096091285347939,
        "seek": 0,
        "start": 23.6,
        "temperature": 0,
        "text": " The model is now training all the way up until 200 epochs.",
        "tokens": [
          51544,
          440,
          2316,
          307,
          586,
          3097,
          439,
          264,
          636,
          493,
          1826,
          2331,
          30992,
          28346,
          13,
          51710
        ]
      },
      {
        "avg_logprob": -0.2214049268888947,
        "compression_ratio": 1.7467105263157894,
        "end": 31,
        "id": 9,
        "no_speech_prob": 0.00020027266873512417,
        "seek": 2692,
        "start": 26.92,
        "temperature": 0,
        "text": " And when it gets to the end, I've got a trained model.",
        "tokens": [
          50364,
          400,
          562,
          309,
          2170,
          281,
          264,
          917,
          11,
          286,
          600,
          658,
          257,
          8895,
          2316,
          13,
          50568
        ]
      },
      {
        "avg_logprob": -0.2214049268888947,
        "compression_ratio": 1.7467105263157894,
        "end": 32.88,
        "id": 10,
        "no_speech_prob": 0.00020027266873512417,
        "seek": 2692,
        "start": 31,
        "temperature": 0,
        "text": " And I can click around and have the model",
        "tokens": [
          50568,
          400,
          286,
          393,
          2052,
          926,
          293,
          362,
          264,
          2316,
          50662
        ]
      },
      {
        "avg_logprob": -0.2214049268888947,
        "compression_ratio": 1.7467105263157894,
        "end": 35.84,
        "id": 11,
        "no_speech_prob": 0.00020027266873512417,
        "seek": 2692,
        "start": 32.88,
        "temperature": 0,
        "text": " guess a particular note for a particular xy",
        "tokens": [
          50662,
          2041,
          257,
          1729,
          3637,
          337,
          257,
          1729,
          2031,
          88,
          50810
        ]
      },
      {
        "avg_logprob": -0.2214049268888947,
        "compression_ratio": 1.7467105263157894,
        "end": 37.52,
        "id": 12,
        "no_speech_prob": 0.00020027266873512417,
        "seek": 2692,
        "start": 35.84,
        "temperature": 0,
        "text": " coordinate in the canvas.",
        "tokens": [
          50810,
          15670,
          294,
          264,
          16267,
          13,
          50894
        ]
      },
      {
        "avg_logprob": -0.2214049268888947,
        "compression_ratio": 1.7467105263157894,
        "end": 39.82,
        "id": 13,
        "no_speech_prob": 0.00020027266873512417,
        "seek": 2692,
        "start": 37.52,
        "temperature": 0,
        "text": " But if I press Stop and run it again,",
        "tokens": [
          50894,
          583,
          498,
          286,
          1886,
          5535,
          293,
          1190,
          309,
          797,
          11,
          51009
        ]
      },
      {
        "avg_logprob": -0.2214049268888947,
        "compression_ratio": 1.7467105263157894,
        "end": 41.32,
        "id": 14,
        "no_speech_prob": 0.00020027266873512417,
        "seek": 2692,
        "start": 39.82,
        "temperature": 0,
        "text": " I have to retrain the model.",
        "tokens": [
          51009,
          286,
          362,
          281,
          1533,
          7146,
          264,
          2316,
          13,
          51084
        ]
      },
      {
        "avg_logprob": -0.2214049268888947,
        "compression_ratio": 1.7467105263157894,
        "end": 42.900000000000006,
        "id": 15,
        "no_speech_prob": 0.00020027266873512417,
        "seek": 2692,
        "start": 41.32,
        "temperature": 0,
        "text": " And this is incredibly useful because I",
        "tokens": [
          51084,
          400,
          341,
          307,
          6252,
          4420,
          570,
          286,
          51163
        ]
      },
      {
        "avg_logprob": -0.2214049268888947,
        "compression_ratio": 1.7467105263157894,
        "end": 45.44,
        "id": 16,
        "no_speech_prob": 0.00020027266873512417,
        "seek": 2692,
        "start": 42.900000000000006,
        "temperature": 0,
        "text": " might want to try adjusting the data set,",
        "tokens": [
          51163,
          1062,
          528,
          281,
          853,
          23559,
          264,
          1412,
          992,
          11,
          51290
        ]
      },
      {
        "avg_logprob": -0.2214049268888947,
        "compression_ratio": 1.7467105263157894,
        "end": 47.28,
        "id": 17,
        "no_speech_prob": 0.00020027266873512417,
        "seek": 2692,
        "start": 45.44,
        "temperature": 0,
        "text": " recollecting the data, retraining the model,",
        "tokens": [
          51290,
          39495,
          557,
          278,
          264,
          1412,
          11,
          49356,
          1760,
          264,
          2316,
          11,
          51382
        ]
      },
      {
        "avg_logprob": -0.2214049268888947,
        "compression_ratio": 1.7467105263157894,
        "end": 49.68000000000001,
        "id": 18,
        "no_speech_prob": 0.00020027266873512417,
        "seek": 2692,
        "start": 47.28,
        "temperature": 0,
        "text": " trying different parameters, adding more epochs, fewer",
        "tokens": [
          51382,
          1382,
          819,
          9834,
          11,
          5127,
          544,
          30992,
          28346,
          11,
          13366,
          51502
        ]
      },
      {
        "avg_logprob": -0.2214049268888947,
        "compression_ratio": 1.7467105263157894,
        "end": 51.480000000000004,
        "id": 19,
        "no_speech_prob": 0.00020027266873512417,
        "seek": 2692,
        "start": 49.68000000000001,
        "temperature": 0,
        "text": " epochs, all sorts of possibilities.",
        "tokens": [
          51502,
          30992,
          28346,
          11,
          439,
          7527,
          295,
          12178,
          13,
          51592
        ]
      },
      {
        "avg_logprob": -0.2214049268888947,
        "compression_ratio": 1.7467105263157894,
        "end": 53.64,
        "id": 20,
        "no_speech_prob": 0.00020027266873512417,
        "seek": 2692,
        "start": 51.480000000000004,
        "temperature": 0,
        "text": " But once I'm done, once I feel like I'm",
        "tokens": [
          51592,
          583,
          1564,
          286,
          478,
          1096,
          11,
          1564,
          286,
          841,
          411,
          286,
          478,
          51700
        ]
      },
      {
        "avg_logprob": -0.2214049268888947,
        "compression_ratio": 1.7467105263157894,
        "end": 55.980000000000004,
        "id": 21,
        "no_speech_prob": 0.00020027266873512417,
        "seek": 2692,
        "start": 53.64,
        "temperature": 0,
        "text": " satisfied with this model, I would also",
        "tokens": [
          51700,
          11239,
          365,
          341,
          2316,
          11,
          286,
          576,
          611,
          51817
        ]
      },
      {
        "avg_logprob": -0.2379743058160441,
        "compression_ratio": 1.8055555555555556,
        "end": 58.14,
        "id": 22,
        "no_speech_prob": 0.0018675456522032619,
        "seek": 5598,
        "start": 56.04,
        "temperature": 0,
        "text": " like to be able to just save the model and reload it",
        "tokens": [
          50367,
          411,
          281,
          312,
          1075,
          281,
          445,
          3155,
          264,
          2316,
          293,
          25628,
          309,
          50472
        ]
      },
      {
        "avg_logprob": -0.2379743058160441,
        "compression_ratio": 1.8055555555555556,
        "end": 60.86,
        "id": 23,
        "no_speech_prob": 0.0018675456522032619,
        "seek": 5598,
        "start": 58.14,
        "temperature": 0,
        "text": " so that I don't have to do anything with the data again.",
        "tokens": [
          50472,
          370,
          300,
          286,
          500,
          380,
          362,
          281,
          360,
          1340,
          365,
          264,
          1412,
          797,
          13,
          50608
        ]
      },
      {
        "avg_logprob": -0.2379743058160441,
        "compression_ratio": 1.8055555555555556,
        "end": 63.54,
        "id": 24,
        "no_speech_prob": 0.0018675456522032619,
        "seek": 5598,
        "start": 60.86,
        "temperature": 0,
        "text": " In other words, we're finished with these first two steps.",
        "tokens": [
          50608,
          682,
          661,
          2283,
          11,
          321,
          434,
          4335,
          365,
          613,
          700,
          732,
          4439,
          13,
          50742
        ]
      },
      {
        "avg_logprob": -0.2379743058160441,
        "compression_ratio": 1.8055555555555556,
        "end": 65.03999999999999,
        "id": 25,
        "no_speech_prob": 0.0018675456522032619,
        "seek": 5598,
        "start": 63.54,
        "temperature": 0,
        "text": " Collect the data, train the model.",
        "tokens": [
          50742,
          31896,
          264,
          1412,
          11,
          3847,
          264,
          2316,
          13,
          50817
        ]
      },
      {
        "avg_logprob": -0.2379743058160441,
        "compression_ratio": 1.8055555555555556,
        "end": 68.38,
        "id": 26,
        "no_speech_prob": 0.0018675456522032619,
        "seek": 5598,
        "start": 65.03999999999999,
        "temperature": 0,
        "text": " We've used Save Data and Load Data previously.",
        "tokens": [
          50817,
          492,
          600,
          1143,
          15541,
          11888,
          293,
          48408,
          11888,
          8046,
          13,
          50984
        ]
      },
      {
        "avg_logprob": -0.2379743058160441,
        "compression_ratio": 1.8055555555555556,
        "end": 71.94,
        "id": 27,
        "no_speech_prob": 0.0018675456522032619,
        "seek": 5598,
        "start": 68.38,
        "temperature": 0,
        "text": " And now I just want to save the model so that I can deploy it.",
        "tokens": [
          50984,
          400,
          586,
          286,
          445,
          528,
          281,
          3155,
          264,
          2316,
          370,
          300,
          286,
          393,
          7274,
          309,
          13,
          51162
        ]
      },
      {
        "avg_logprob": -0.2379743058160441,
        "compression_ratio": 1.8055555555555556,
        "end": 75.17999999999999,
        "id": 28,
        "no_speech_prob": 0.0018675456522032619,
        "seek": 5598,
        "start": 71.94,
        "temperature": 0,
        "text": " So to do that, there's really just one other function",
        "tokens": [
          51162,
          407,
          281,
          360,
          300,
          11,
          456,
          311,
          534,
          445,
          472,
          661,
          2445,
          51324
        ]
      },
      {
        "avg_logprob": -0.2379743058160441,
        "compression_ratio": 1.8055555555555556,
        "end": 76.3,
        "id": 29,
        "no_speech_prob": 0.0018675456522032619,
        "seek": 5598,
        "start": 75.17999999999999,
        "temperature": 0,
        "text": " that I need, Save.",
        "tokens": [
          51324,
          300,
          286,
          643,
          11,
          15541,
          13,
          51380
        ]
      },
      {
        "avg_logprob": -0.2379743058160441,
        "compression_ratio": 1.8055555555555556,
        "end": 78.82,
        "id": 30,
        "no_speech_prob": 0.0018675456522032619,
        "seek": 5598,
        "start": 76.3,
        "temperature": 0,
        "text": " So if I just say Save, that's the model, not the data.",
        "tokens": [
          51380,
          407,
          498,
          286,
          445,
          584,
          15541,
          11,
          300,
          311,
          264,
          2316,
          11,
          406,
          264,
          1412,
          13,
          51506
        ]
      },
      {
        "avg_logprob": -0.2379743058160441,
        "compression_ratio": 1.8055555555555556,
        "end": 80.25999999999999,
        "id": 31,
        "no_speech_prob": 0.0018675456522032619,
        "seek": 5598,
        "start": 78.82,
        "temperature": 0,
        "text": " And then also, of course, I'm going",
        "tokens": [
          51506,
          400,
          550,
          611,
          11,
          295,
          1164,
          11,
          286,
          478,
          516,
          51578
        ]
      },
      {
        "avg_logprob": -0.2379743058160441,
        "compression_ratio": 1.8055555555555556,
        "end": 83.1,
        "id": 32,
        "no_speech_prob": 0.0018675456522032619,
        "seek": 5598,
        "start": 80.25999999999999,
        "temperature": 0,
        "text": " to make use of this Load function as well.",
        "tokens": [
          51578,
          281,
          652,
          764,
          295,
          341,
          48408,
          2445,
          382,
          731,
          13,
          51720
        ]
      },
      {
        "avg_logprob": -0.2293347719698976,
        "compression_ratio": 1.8269230769230769,
        "end": 86.47999999999999,
        "id": 33,
        "no_speech_prob": 0.00003591296626836993,
        "seek": 8310,
        "start": 83.1,
        "temperature": 0,
        "text": " So let's go back to my trusty keypress interface",
        "tokens": [
          50364,
          407,
          718,
          311,
          352,
          646,
          281,
          452,
          3361,
          88,
          2141,
          11637,
          9226,
          50533
        ]
      },
      {
        "avg_logprob": -0.2293347719698976,
        "compression_ratio": 1.8269230769230769,
        "end": 87.78,
        "id": 34,
        "no_speech_prob": 0.00003591296626836993,
        "seek": 8310,
        "start": 86.47999999999999,
        "temperature": 0,
        "text": " and add one more key.",
        "tokens": [
          50533,
          293,
          909,
          472,
          544,
          2141,
          13,
          50598
        ]
      },
      {
        "avg_logprob": -0.2293347719698976,
        "compression_ratio": 1.8269230769230769,
        "end": 90.33999999999999,
        "id": 35,
        "no_speech_prob": 0.00003591296626836993,
        "seek": 8310,
        "start": 87.78,
        "temperature": 0,
        "text": " Let's use M for saving the model.",
        "tokens": [
          50598,
          961,
          311,
          764,
          376,
          337,
          6816,
          264,
          2316,
          13,
          50726
        ]
      },
      {
        "avg_logprob": -0.2293347719698976,
        "compression_ratio": 1.8269230769230769,
        "end": 93.74,
        "id": 36,
        "no_speech_prob": 0.00003591296626836993,
        "seek": 8310,
        "start": 90.33999999999999,
        "temperature": 0,
        "text": " Change the key to M. Change the function to Save.",
        "tokens": [
          50726,
          15060,
          264,
          2141,
          281,
          376,
          13,
          15060,
          264,
          2445,
          281,
          15541,
          13,
          50896
        ]
      },
      {
        "avg_logprob": -0.2293347719698976,
        "compression_ratio": 1.8269230769230769,
        "end": 95.82,
        "id": 37,
        "no_speech_prob": 0.00003591296626836993,
        "seek": 8310,
        "start": 93.74,
        "temperature": 0,
        "text": " And then I'm going to call this, I'm just",
        "tokens": [
          50896,
          400,
          550,
          286,
          478,
          516,
          281,
          818,
          341,
          11,
          286,
          478,
          445,
          51000
        ]
      },
      {
        "avg_logprob": -0.2293347719698976,
        "compression_ratio": 1.8269230769230769,
        "end": 97.25999999999999,
        "id": 38,
        "no_speech_prob": 0.00003591296626836993,
        "seek": 8310,
        "start": 95.82,
        "temperature": 0,
        "text": " going to call it mouse notes as well.",
        "tokens": [
          51000,
          516,
          281,
          818,
          309,
          9719,
          5570,
          382,
          731,
          13,
          51072
        ]
      },
      {
        "avg_logprob": -0.2293347719698976,
        "compression_ratio": 1.8269230769230769,
        "end": 100.25999999999999,
        "id": 39,
        "no_speech_prob": 0.00003591296626836993,
        "seek": 8310,
        "start": 97.25999999999999,
        "temperature": 0,
        "text": " Actually, let's go, maybe I need some more arguments here.",
        "tokens": [
          51072,
          5135,
          11,
          718,
          311,
          352,
          11,
          1310,
          286,
          643,
          512,
          544,
          12869,
          510,
          13,
          51222
        ]
      },
      {
        "avg_logprob": -0.2293347719698976,
        "compression_ratio": 1.8269230769230769,
        "end": 102.53999999999999,
        "id": 40,
        "no_speech_prob": 0.00003591296626836993,
        "seek": 8310,
        "start": 100.25999999999999,
        "temperature": 0,
        "text": " Let's go and check the ml5 reference.",
        "tokens": [
          51222,
          961,
          311,
          352,
          293,
          1520,
          264,
          23271,
          20,
          6408,
          13,
          51336
        ]
      },
      {
        "avg_logprob": -0.2293347719698976,
        "compression_ratio": 1.8269230769230769,
        "end": 104.22,
        "id": 41,
        "no_speech_prob": 0.00003591296626836993,
        "seek": 8310,
        "start": 102.53999999999999,
        "temperature": 0,
        "text": " So indeed, it's the same as before.",
        "tokens": [
          51336,
          407,
          6451,
          11,
          309,
          311,
          264,
          912,
          382,
          949,
          13,
          51420
        ]
      },
      {
        "avg_logprob": -0.2293347719698976,
        "compression_ratio": 1.8269230769230769,
        "end": 106.5,
        "id": 42,
        "no_speech_prob": 0.00003591296626836993,
        "seek": 8310,
        "start": 104.22,
        "temperature": 0,
        "text": " I need to give it a name and then a callback",
        "tokens": [
          51420,
          286,
          643,
          281,
          976,
          309,
          257,
          1315,
          293,
          550,
          257,
          818,
          3207,
          51534
        ]
      },
      {
        "avg_logprob": -0.2293347719698976,
        "compression_ratio": 1.8269230769230769,
        "end": 107.53999999999999,
        "id": 43,
        "no_speech_prob": 0.00003591296626836993,
        "seek": 8310,
        "start": 106.5,
        "temperature": 0,
        "text": " for when it's completed.",
        "tokens": [
          51534,
          337,
          562,
          309,
          311,
          7365,
          13,
          51586
        ]
      },
      {
        "avg_logprob": -0.2293347719698976,
        "compression_ratio": 1.8269230769230769,
        "end": 109.86,
        "id": 44,
        "no_speech_prob": 0.00003591296626836993,
        "seek": 8310,
        "start": 107.53999999999999,
        "temperature": 0,
        "text": " But in this case, I don't need to know when it's completed",
        "tokens": [
          51586,
          583,
          294,
          341,
          1389,
          11,
          286,
          500,
          380,
          643,
          281,
          458,
          562,
          309,
          311,
          7365,
          51702
        ]
      },
      {
        "avg_logprob": -0.2293347719698976,
        "compression_ratio": 1.8269230769230769,
        "end": 111.94,
        "id": 45,
        "no_speech_prob": 0.00003591296626836993,
        "seek": 8310,
        "start": 109.86,
        "temperature": 0,
        "text": " because I'm just going to see that the files are there.",
        "tokens": [
          51702,
          570,
          286,
          478,
          445,
          516,
          281,
          536,
          300,
          264,
          7098,
          366,
          456,
          13,
          51806
        ]
      },
      {
        "avg_logprob": -0.2293347719698976,
        "compression_ratio": 1.8269230769230769,
        "end": 112.82,
        "id": 46,
        "no_speech_prob": 0.00003591296626836993,
        "seek": 8310,
        "start": 111.94,
        "temperature": 0,
        "text": " OK, let's run it.",
        "tokens": [
          51806,
          2264,
          11,
          718,
          311,
          1190,
          309,
          13,
          51850
        ]
      },
      {
        "avg_logprob": -0.32839083671569824,
        "compression_ratio": 1.505,
        "end": 113.86,
        "id": 47,
        "no_speech_prob": 0.00002468286947987508,
        "seek": 11282,
        "start": 112.82,
        "temperature": 0,
        "text": " Train it and save it.",
        "tokens": [
          50364,
          28029,
          309,
          293,
          3155,
          309,
          13,
          50416
        ]
      },
      {
        "avg_logprob": -0.32839083671569824,
        "compression_ratio": 1.505,
        "end": 129.7,
        "id": 48,
        "no_speech_prob": 0.00002468286947987508,
        "seek": 11282,
        "start": 113.86,
        "temperature": 0,
        "text": " And now I can hit M. Files have been downloaded.",
        "tokens": [
          50416,
          220,
          5289,
          586,
          286,
          393,
          2045,
          376,
          13,
          479,
          4680,
          362,
          668,
          21748,
          13,
          51208
        ]
      },
      {
        "avg_logprob": -0.32839083671569824,
        "compression_ratio": 1.505,
        "end": 132.26,
        "id": 49,
        "no_speech_prob": 0.00002468286947987508,
        "seek": 11282,
        "start": 129.7,
        "temperature": 0,
        "text": " Interestingly, there are now four files",
        "tokens": [
          51208,
          30564,
          11,
          456,
          366,
          586,
          1451,
          7098,
          51336
        ]
      },
      {
        "avg_logprob": -0.32839083671569824,
        "compression_ratio": 1.505,
        "end": 133.82,
        "id": 50,
        "no_speech_prob": 0.00002468286947987508,
        "seek": 11282,
        "start": 132.26,
        "temperature": 0,
        "text": " in my Downloads directory.",
        "tokens": [
          51336,
          294,
          452,
          32282,
          82,
          21120,
          13,
          51414
        ]
      },
      {
        "avg_logprob": -0.32839083671569824,
        "compression_ratio": 1.505,
        "end": 136.64,
        "id": 51,
        "no_speech_prob": 0.00002468286947987508,
        "seek": 11282,
        "start": 133.82,
        "temperature": 0,
        "text": " Now, I should point out that this file is from before.",
        "tokens": [
          51414,
          823,
          11,
          286,
          820,
          935,
          484,
          300,
          341,
          3991,
          307,
          490,
          949,
          13,
          51555
        ]
      },
      {
        "avg_logprob": -0.32839083671569824,
        "compression_ratio": 1.505,
        "end": 137.74,
        "id": 52,
        "no_speech_prob": 0.00002468286947987508,
        "seek": 11282,
        "start": 136.64,
        "temperature": 0,
        "text": " This is the data file.",
        "tokens": [
          51555,
          639,
          307,
          264,
          1412,
          3991,
          13,
          51610
        ]
      },
      {
        "avg_logprob": -0.32839083671569824,
        "compression_ratio": 1.505,
        "end": 140.01999999999998,
        "id": 53,
        "no_speech_prob": 0.00002468286947987508,
        "seek": 11282,
        "start": 137.74,
        "temperature": 0,
        "text": " And I'm just going to delete it right now because there are",
        "tokens": [
          51610,
          400,
          286,
          478,
          445,
          516,
          281,
          12097,
          309,
          558,
          586,
          570,
          456,
          366,
          51724
        ]
      },
      {
        "avg_logprob": -0.32839083671569824,
        "compression_ratio": 1.505,
        "end": 142.22,
        "id": 54,
        "no_speech_prob": 0.00002468286947987508,
        "seek": 11282,
        "start": 140.01999999999998,
        "temperature": 0,
        "text": " actually now three files.",
        "tokens": [
          51724,
          767,
          586,
          1045,
          7098,
          13,
          51834
        ]
      },
      {
        "avg_logprob": -0.2152491904593803,
        "compression_ratio": 1.676595744680851,
        "end": 146.54,
        "id": 55,
        "no_speech_prob": 0.000013845989997207653,
        "seek": 14222,
        "start": 142.22,
        "temperature": 0,
        "text": " So saving the model is different than saving the data.",
        "tokens": [
          50364,
          407,
          6816,
          264,
          2316,
          307,
          819,
          813,
          6816,
          264,
          1412,
          13,
          50580
        ]
      },
      {
        "avg_logprob": -0.2152491904593803,
        "compression_ratio": 1.676595744680851,
        "end": 149.54,
        "id": 56,
        "no_speech_prob": 0.000013845989997207653,
        "seek": 14222,
        "start": 146.54,
        "temperature": 0,
        "text": " With the data, you just have one file,",
        "tokens": [
          50580,
          2022,
          264,
          1412,
          11,
          291,
          445,
          362,
          472,
          3991,
          11,
          50730
        ]
      },
      {
        "avg_logprob": -0.2152491904593803,
        "compression_ratio": 1.676595744680851,
        "end": 152.02,
        "id": 57,
        "no_speech_prob": 0.000013845989997207653,
        "seek": 14222,
        "start": 149.54,
        "temperature": 0,
        "text": " the actual data in JSON format.",
        "tokens": [
          50730,
          264,
          3539,
          1412,
          294,
          31828,
          7877,
          13,
          50854
        ]
      },
      {
        "avg_logprob": -0.2152491904593803,
        "compression_ratio": 1.676595744680851,
        "end": 154.98,
        "id": 58,
        "no_speech_prob": 0.000013845989997207653,
        "seek": 14222,
        "start": 152.02,
        "temperature": 0,
        "text": " When you're saving the model, there are three files.",
        "tokens": [
          50854,
          1133,
          291,
          434,
          6816,
          264,
          2316,
          11,
          456,
          366,
          1045,
          7098,
          13,
          51002
        ]
      },
      {
        "avg_logprob": -0.2152491904593803,
        "compression_ratio": 1.676595744680851,
        "end": 157.42,
        "id": 59,
        "no_speech_prob": 0.000013845989997207653,
        "seek": 14222,
        "start": 154.98,
        "temperature": 0,
        "text": " Let's call one model.json.",
        "tokens": [
          51002,
          961,
          311,
          818,
          472,
          2316,
          13,
          73,
          3015,
          13,
          51124
        ]
      },
      {
        "avg_logprob": -0.2152491904593803,
        "compression_ratio": 1.676595744680851,
        "end": 162.22,
        "id": 60,
        "no_speech_prob": 0.000013845989997207653,
        "seek": 14222,
        "start": 157.42,
        "temperature": 0,
        "text": " The next file is model underscore meta.json.",
        "tokens": [
          51124,
          440,
          958,
          3991,
          307,
          2316,
          37556,
          19616,
          13,
          73,
          3015,
          13,
          51364
        ]
      },
      {
        "avg_logprob": -0.2152491904593803,
        "compression_ratio": 1.676595744680851,
        "end": 167.38,
        "id": 61,
        "no_speech_prob": 0.000013845989997207653,
        "seek": 14222,
        "start": 162.22,
        "temperature": 0,
        "text": " And one more file, model underscore weights.bin.",
        "tokens": [
          51364,
          400,
          472,
          544,
          3991,
          11,
          2316,
          37556,
          17443,
          13,
          13496,
          13,
          51622
        ]
      },
      {
        "avg_logprob": -0.2152491904593803,
        "compression_ratio": 1.676595744680851,
        "end": 169.34,
        "id": 62,
        "no_speech_prob": 0.000013845989997207653,
        "seek": 14222,
        "start": 167.38,
        "temperature": 0,
        "text": " Now, I think while recording this video tutorial,",
        "tokens": [
          51622,
          823,
          11,
          286,
          519,
          1339,
          6613,
          341,
          960,
          7073,
          11,
          51720
        ]
      },
      {
        "avg_logprob": -0.2152491904593803,
        "compression_ratio": 1.676595744680851,
        "end": 172.1,
        "id": 63,
        "no_speech_prob": 0.000013845989997207653,
        "seek": 14222,
        "start": 169.34,
        "temperature": 0,
        "text": " I just discovered a bug in ml5 because these",
        "tokens": [
          51720,
          286,
          445,
          6941,
          257,
          7426,
          294,
          23271,
          20,
          570,
          613,
          51858
        ]
      },
      {
        "avg_logprob": -0.24989165200127494,
        "compression_ratio": 1.77,
        "end": 174.66,
        "id": 64,
        "no_speech_prob": 0.0005442087422125041,
        "seek": 17210,
        "start": 172.18,
        "temperature": 0,
        "text": " should actually be called mouse notes dot json,",
        "tokens": [
          50368,
          820,
          767,
          312,
          1219,
          9719,
          5570,
          5893,
          361,
          3015,
          11,
          50492
        ]
      },
      {
        "avg_logprob": -0.24989165200127494,
        "compression_ratio": 1.77,
        "end": 177.85999999999999,
        "id": 65,
        "no_speech_prob": 0.0005442087422125041,
        "seek": 17210,
        "start": 174.66,
        "temperature": 0,
        "text": " mouse notes underscore meta dot json, mouse notes dot weights",
        "tokens": [
          50492,
          9719,
          5570,
          37556,
          19616,
          5893,
          361,
          3015,
          11,
          9719,
          5570,
          5893,
          17443,
          50652
        ]
      },
      {
        "avg_logprob": -0.24989165200127494,
        "compression_ratio": 1.77,
        "end": 178.54,
        "id": 66,
        "no_speech_prob": 0.0005442087422125041,
        "seek": 17210,
        "start": 177.85999999999999,
        "temperature": 0,
        "text": " dot bin.",
        "tokens": [
          50652,
          5893,
          5171,
          13,
          50686
        ]
      },
      {
        "avg_logprob": -0.24989165200127494,
        "compression_ratio": 1.77,
        "end": 180.45999999999998,
        "id": 67,
        "no_speech_prob": 0.0005442087422125041,
        "seek": 17210,
        "start": 178.54,
        "temperature": 0,
        "text": " But it just used the default naming model.",
        "tokens": [
          50686,
          583,
          309,
          445,
          1143,
          264,
          7576,
          25290,
          2316,
          13,
          50782
        ]
      },
      {
        "avg_logprob": -0.24989165200127494,
        "compression_ratio": 1.77,
        "end": 182.62,
        "id": 68,
        "no_speech_prob": 0.0005442087422125041,
        "seek": 17210,
        "start": 180.45999999999998,
        "temperature": 0,
        "text": " So maybe by the time you're watching this,",
        "tokens": [
          50782,
          407,
          1310,
          538,
          264,
          565,
          291,
          434,
          1976,
          341,
          11,
          50890
        ]
      },
      {
        "avg_logprob": -0.24989165200127494,
        "compression_ratio": 1.77,
        "end": 184.7,
        "id": 69,
        "no_speech_prob": 0.0005442087422125041,
        "seek": 17210,
        "start": 182.62,
        "temperature": 0,
        "text": " there'll be a new version of ml5 that fixes that.",
        "tokens": [
          50890,
          456,
          603,
          312,
          257,
          777,
          3037,
          295,
          23271,
          20,
          300,
          32539,
          300,
          13,
          50994
        ]
      },
      {
        "avg_logprob": -0.24989165200127494,
        "compression_ratio": 1.77,
        "end": 186.85999999999999,
        "id": 70,
        "no_speech_prob": 0.0005442087422125041,
        "seek": 17210,
        "start": 184.7,
        "temperature": 0,
        "text": " But those are the default names.",
        "tokens": [
          50994,
          583,
          729,
          366,
          264,
          7576,
          5288,
          13,
          51102
        ]
      },
      {
        "avg_logprob": -0.24989165200127494,
        "compression_ratio": 1.77,
        "end": 188.66,
        "id": 71,
        "no_speech_prob": 0.0005442087422125041,
        "seek": 17210,
        "start": 186.85999999999999,
        "temperature": 0,
        "text": " And the reason why there are three files",
        "tokens": [
          51102,
          400,
          264,
          1778,
          983,
          456,
          366,
          1045,
          7098,
          51192
        ]
      },
      {
        "avg_logprob": -0.24989165200127494,
        "compression_ratio": 1.77,
        "end": 191.45999999999998,
        "id": 72,
        "no_speech_prob": 0.0005442087422125041,
        "seek": 17210,
        "start": 188.66,
        "temperature": 0,
        "text": " is there's a lot of information to store related",
        "tokens": [
          51192,
          307,
          456,
          311,
          257,
          688,
          295,
          1589,
          281,
          3531,
          4077,
          51332
        ]
      },
      {
        "avg_logprob": -0.24989165200127494,
        "compression_ratio": 1.77,
        "end": 193.29999999999998,
        "id": 73,
        "no_speech_prob": 0.0005442087422125041,
        "seek": 17210,
        "start": 191.45999999999998,
        "temperature": 0,
        "text": " to a machine learning model.",
        "tokens": [
          51332,
          281,
          257,
          3479,
          2539,
          2316,
          13,
          51424
        ]
      },
      {
        "avg_logprob": -0.24989165200127494,
        "compression_ratio": 1.77,
        "end": 195.54,
        "id": 74,
        "no_speech_prob": 0.0005442087422125041,
        "seek": 17210,
        "start": 193.29999999999998,
        "temperature": 0,
        "text": " If we travel back in time for a moment,",
        "tokens": [
          51424,
          759,
          321,
          3147,
          646,
          294,
          565,
          337,
          257,
          1623,
          11,
          51536
        ]
      },
      {
        "avg_logprob": -0.24989165200127494,
        "compression_ratio": 1.77,
        "end": 198.9,
        "id": 75,
        "no_speech_prob": 0.0005442087422125041,
        "seek": 17210,
        "start": 195.54,
        "temperature": 0,
        "text": " you might remember a diagram I drew in this very first video",
        "tokens": [
          51536,
          291,
          1062,
          1604,
          257,
          10686,
          286,
          12804,
          294,
          341,
          588,
          700,
          960,
          51704
        ]
      },
      {
        "avg_logprob": -0.24989165200127494,
        "compression_ratio": 1.77,
        "end": 199.94,
        "id": 76,
        "no_speech_prob": 0.0005442087422125041,
        "seek": 17210,
        "start": 198.9,
        "temperature": 0,
        "text": " as part of this series.",
        "tokens": [
          51704,
          382,
          644,
          295,
          341,
          2638,
          13,
          51756
        ]
      },
      {
        "avg_logprob": -0.25579955897380396,
        "compression_ratio": 1.6603773584905661,
        "end": 205.78,
        "id": 77,
        "no_speech_prob": 0.00001670140409260057,
        "seek": 19994,
        "start": 200.02,
        "temperature": 0,
        "text": " This is the neural network diagram.",
        "tokens": [
          50368,
          639,
          307,
          264,
          18161,
          3209,
          10686,
          13,
          50656
        ]
      },
      {
        "avg_logprob": -0.25579955897380396,
        "compression_ratio": 1.6603773584905661,
        "end": 208.18,
        "id": 78,
        "no_speech_prob": 0.00001670140409260057,
        "seek": 19994,
        "start": 205.78,
        "temperature": 0,
        "text": " Looking at this, we can see the overall architecture",
        "tokens": [
          50656,
          11053,
          412,
          341,
          11,
          321,
          393,
          536,
          264,
          4787,
          9482,
          50776
        ]
      },
      {
        "avg_logprob": -0.25579955897380396,
        "compression_ratio": 1.6603773584905661,
        "end": 209.1,
        "id": 79,
        "no_speech_prob": 0.00001670140409260057,
        "seek": 19994,
        "start": 208.18,
        "temperature": 0,
        "text": " of the network.",
        "tokens": [
          50776,
          295,
          264,
          3209,
          13,
          50822
        ]
      },
      {
        "avg_logprob": -0.25579955897380396,
        "compression_ratio": 1.6603773584905661,
        "end": 210.94,
        "id": 80,
        "no_speech_prob": 0.00001670140409260057,
        "seek": 19994,
        "start": 209.1,
        "temperature": 0,
        "text": " There are two inputs.",
        "tokens": [
          50822,
          821,
          366,
          732,
          15743,
          13,
          50914
        ]
      },
      {
        "avg_logprob": -0.25579955897380396,
        "compression_ratio": 1.6603773584905661,
        "end": 213.02,
        "id": 81,
        "no_speech_prob": 0.00001670140409260057,
        "seek": 19994,
        "start": 210.94,
        "temperature": 0,
        "text": " There is a hidden layer.",
        "tokens": [
          50914,
          821,
          307,
          257,
          7633,
          4583,
          13,
          51018
        ]
      },
      {
        "avg_logprob": -0.25579955897380396,
        "compression_ratio": 1.6603773584905661,
        "end": 215.06,
        "id": 82,
        "no_speech_prob": 0.00001670140409260057,
        "seek": 19994,
        "start": 213.02,
        "temperature": 0,
        "text": " And there is an output layer.",
        "tokens": [
          51018,
          400,
          456,
          307,
          364,
          5598,
          4583,
          13,
          51120
        ]
      },
      {
        "avg_logprob": -0.25579955897380396,
        "compression_ratio": 1.6603773584905661,
        "end": 220.57999999999998,
        "id": 83,
        "no_speech_prob": 0.00001670140409260057,
        "seek": 19994,
        "start": 215.06,
        "temperature": 0,
        "text": " This architecture is described in model dot json.",
        "tokens": [
          51120,
          639,
          9482,
          307,
          7619,
          294,
          2316,
          5893,
          361,
          3015,
          13,
          51396
        ]
      },
      {
        "avg_logprob": -0.25579955897380396,
        "compression_ratio": 1.6603773584905661,
        "end": 223.7,
        "id": 84,
        "no_speech_prob": 0.00001670140409260057,
        "seek": 19994,
        "start": 220.57999999999998,
        "temperature": 0,
        "text": " This is what that model dot json file actually looks like.",
        "tokens": [
          51396,
          639,
          307,
          437,
          300,
          2316,
          5893,
          361,
          3015,
          3991,
          767,
          1542,
          411,
          13,
          51552
        ]
      },
      {
        "avg_logprob": -0.25579955897380396,
        "compression_ratio": 1.6603773584905661,
        "end": 224.74,
        "id": 85,
        "no_speech_prob": 0.00001670140409260057,
        "seek": 19994,
        "start": 223.7,
        "temperature": 0,
        "text": " It's a little terrifying.",
        "tokens": [
          51552,
          467,
          311,
          257,
          707,
          18106,
          13,
          51604
        ]
      },
      {
        "avg_logprob": -0.25579955897380396,
        "compression_ratio": 1.6603773584905661,
        "end": 227.26,
        "id": 86,
        "no_speech_prob": 0.00001670140409260057,
        "seek": 19994,
        "start": 224.74,
        "temperature": 0,
        "text": " There's lots of lower level details",
        "tokens": [
          51604,
          821,
          311,
          3195,
          295,
          3126,
          1496,
          4365,
          51730
        ]
      },
      {
        "avg_logprob": -0.19903146066973287,
        "compression_ratio": 1.8435114503816794,
        "end": 230.62,
        "id": 87,
        "no_speech_prob": 0.0010162398684769869,
        "seek": 22726,
        "start": 227.26,
        "temperature": 0,
        "text": " related to how machine learning and neural networks work.",
        "tokens": [
          50364,
          4077,
          281,
          577,
          3479,
          2539,
          293,
          18161,
          9590,
          589,
          13,
          50532
        ]
      },
      {
        "avg_logprob": -0.19903146066973287,
        "compression_ratio": 1.8435114503816794,
        "end": 232.73999999999998,
        "id": 88,
        "no_speech_prob": 0.0010162398684769869,
        "seek": 22726,
        "start": 230.62,
        "temperature": 0,
        "text": " But we can even start to pick little and choose",
        "tokens": [
          50532,
          583,
          321,
          393,
          754,
          722,
          281,
          1888,
          707,
          293,
          2826,
          50638
        ]
      },
      {
        "avg_logprob": -0.19903146066973287,
        "compression_ratio": 1.8435114503816794,
        "end": 235.1,
        "id": 89,
        "no_speech_prob": 0.0010162398684769869,
        "seek": 22726,
        "start": 232.73999999999998,
        "temperature": 0,
        "text": " little bits here that we can begin to understand.",
        "tokens": [
          50638,
          707,
          9239,
          510,
          300,
          321,
          393,
          1841,
          281,
          1223,
          13,
          50756
        ]
      },
      {
        "avg_logprob": -0.19903146066973287,
        "compression_ratio": 1.8435114503816794,
        "end": 238.38,
        "id": 90,
        "no_speech_prob": 0.0010162398684769869,
        "seek": 22726,
        "start": 235.1,
        "temperature": 0,
        "text": " For example, this is a sequential neural network.",
        "tokens": [
          50756,
          1171,
          1365,
          11,
          341,
          307,
          257,
          42881,
          18161,
          3209,
          13,
          50920
        ]
      },
      {
        "avg_logprob": -0.19903146066973287,
        "compression_ratio": 1.8435114503816794,
        "end": 239.26,
        "id": 91,
        "no_speech_prob": 0.0010162398684769869,
        "seek": 22726,
        "start": 238.38,
        "temperature": 0,
        "text": " There's a sequence.",
        "tokens": [
          50920,
          821,
          311,
          257,
          8310,
          13,
          50964
        ]
      },
      {
        "avg_logprob": -0.19903146066973287,
        "compression_ratio": 1.8435114503816794,
        "end": 242.38,
        "id": 92,
        "no_speech_prob": 0.0010162398684769869,
        "seek": 22726,
        "start": 239.26,
        "temperature": 0,
        "text": " The inputs will come in to the hidden layer to the output.",
        "tokens": [
          50964,
          440,
          15743,
          486,
          808,
          294,
          281,
          264,
          7633,
          4583,
          281,
          264,
          5598,
          13,
          51120
        ]
      },
      {
        "avg_logprob": -0.19903146066973287,
        "compression_ratio": 1.8435114503816794,
        "end": 243.94,
        "id": 93,
        "no_speech_prob": 0.0010162398684769869,
        "seek": 22726,
        "start": 242.38,
        "temperature": 0,
        "text": " Feed forward.",
        "tokens": [
          51120,
          33720,
          2128,
          13,
          51198
        ]
      },
      {
        "avg_logprob": -0.19903146066973287,
        "compression_ratio": 1.8435114503816794,
        "end": 246.1,
        "id": 94,
        "no_speech_prob": 0.0010162398684769869,
        "seek": 22726,
        "start": 243.94,
        "temperature": 0,
        "text": " There is a dense layer.",
        "tokens": [
          51198,
          821,
          307,
          257,
          18011,
          4583,
          13,
          51306
        ]
      },
      {
        "avg_logprob": -0.19903146066973287,
        "compression_ratio": 1.8435114503816794,
        "end": 248.78,
        "id": 95,
        "no_speech_prob": 0.0010162398684769869,
        "seek": 22726,
        "start": 246.1,
        "temperature": 0,
        "text": " A dense layer means every single input",
        "tokens": [
          51306,
          316,
          18011,
          4583,
          1355,
          633,
          2167,
          4846,
          51440
        ]
      },
      {
        "avg_logprob": -0.19903146066973287,
        "compression_ratio": 1.8435114503816794,
        "end": 251.38,
        "id": 96,
        "no_speech_prob": 0.0010162398684769869,
        "seek": 22726,
        "start": 248.78,
        "temperature": 0,
        "text": " is connected to every single hidden node.",
        "tokens": [
          51440,
          307,
          4582,
          281,
          633,
          2167,
          7633,
          9984,
          13,
          51570
        ]
      },
      {
        "avg_logprob": -0.19903146066973287,
        "compression_ratio": 1.8435114503816794,
        "end": 253.44,
        "id": 97,
        "no_speech_prob": 0.0010162398684769869,
        "seek": 22726,
        "start": 251.38,
        "temperature": 0,
        "text": " All of the connections, it's very dense.",
        "tokens": [
          51570,
          1057,
          295,
          264,
          9271,
          11,
          309,
          311,
          588,
          18011,
          13,
          51673
        ]
      },
      {
        "avg_logprob": -0.19903146066973287,
        "compression_ratio": 1.8435114503816794,
        "end": 255.54,
        "id": 98,
        "no_speech_prob": 0.0010162398684769869,
        "seek": 22726,
        "start": 253.44,
        "temperature": 0,
        "text": " Everything is connected to everything.",
        "tokens": [
          51673,
          5471,
          307,
          4582,
          281,
          1203,
          13,
          51778
        ]
      },
      {
        "avg_logprob": -0.18446650818316607,
        "compression_ratio": 1.6996197718631179,
        "end": 258.7,
        "id": 99,
        "no_speech_prob": 0.00014202346210367978,
        "seek": 25554,
        "start": 255.54,
        "temperature": 0,
        "text": " We can see that the input shape has a 2 in it.",
        "tokens": [
          50364,
          492,
          393,
          536,
          300,
          264,
          4846,
          3909,
          575,
          257,
          568,
          294,
          309,
          13,
          50522
        ]
      },
      {
        "avg_logprob": -0.18446650818316607,
        "compression_ratio": 1.6996197718631179,
        "end": 260.62,
        "id": 100,
        "no_speech_prob": 0.00014202346210367978,
        "seek": 25554,
        "start": 258.7,
        "temperature": 0,
        "text": " There's two inputs.",
        "tokens": [
          50522,
          821,
          311,
          732,
          15743,
          13,
          50618
        ]
      },
      {
        "avg_logprob": -0.18446650818316607,
        "compression_ratio": 1.6996197718631179,
        "end": 262.78,
        "id": 101,
        "no_speech_prob": 0.00014202346210367978,
        "seek": 25554,
        "start": 260.62,
        "temperature": 0,
        "text": " We could go down and find the output layer.",
        "tokens": [
          50618,
          492,
          727,
          352,
          760,
          293,
          915,
          264,
          5598,
          4583,
          13,
          50726
        ]
      },
      {
        "avg_logprob": -0.18446650818316607,
        "compression_ratio": 1.6996197718631179,
        "end": 265.02,
        "id": 102,
        "no_speech_prob": 0.00014202346210367978,
        "seek": 25554,
        "start": 262.78,
        "temperature": 0,
        "text": " And the output layer has seven units.",
        "tokens": [
          50726,
          400,
          264,
          5598,
          4583,
          575,
          3407,
          6815,
          13,
          50838
        ]
      },
      {
        "avg_logprob": -0.18446650818316607,
        "compression_ratio": 1.6996197718631179,
        "end": 267.06,
        "id": 103,
        "no_speech_prob": 0.00014202346210367978,
        "seek": 25554,
        "start": 265.02,
        "temperature": 0,
        "text": " Well, that doesn't match up with three here.",
        "tokens": [
          50838,
          1042,
          11,
          300,
          1177,
          380,
          2995,
          493,
          365,
          1045,
          510,
          13,
          50940
        ]
      },
      {
        "avg_logprob": -0.18446650818316607,
        "compression_ratio": 1.6996197718631179,
        "end": 269.09999999999997,
        "id": 104,
        "no_speech_prob": 0.00014202346210367978,
        "seek": 25554,
        "start": 267.06,
        "temperature": 0,
        "text": " But if you remember, the new data",
        "tokens": [
          50940,
          583,
          498,
          291,
          1604,
          11,
          264,
          777,
          1412,
          51042
        ]
      },
      {
        "avg_logprob": -0.18446650818316607,
        "compression_ratio": 1.6996197718631179,
        "end": 274.09999999999997,
        "id": 105,
        "no_speech_prob": 0.00014202346210367978,
        "seek": 25554,
        "start": 269.09999999999997,
        "temperature": 0,
        "text": " set that I trained in this example has C, D, E, F, G, A,",
        "tokens": [
          51042,
          992,
          300,
          286,
          8895,
          294,
          341,
          1365,
          575,
          383,
          11,
          413,
          11,
          462,
          11,
          479,
          11,
          460,
          11,
          316,
          11,
          51292
        ]
      },
      {
        "avg_logprob": -0.18446650818316607,
        "compression_ratio": 1.6996197718631179,
        "end": 276.02,
        "id": 106,
        "no_speech_prob": 0.00014202346210367978,
        "seek": 25554,
        "start": 274.09999999999997,
        "temperature": 0,
        "text": " B, seven nodes.",
        "tokens": [
          51292,
          363,
          11,
          3407,
          13891,
          13,
          51388
        ]
      },
      {
        "avg_logprob": -0.18446650818316607,
        "compression_ratio": 1.6996197718631179,
        "end": 279.06,
        "id": 107,
        "no_speech_prob": 0.00014202346210367978,
        "seek": 25554,
        "start": 276.02,
        "temperature": 0,
        "text": " So that output layer has seven units to it.",
        "tokens": [
          51388,
          407,
          300,
          5598,
          4583,
          575,
          3407,
          6815,
          281,
          309,
          13,
          51540
        ]
      },
      {
        "avg_logprob": -0.18446650818316607,
        "compression_ratio": 1.6996197718631179,
        "end": 281.21999999999997,
        "id": 108,
        "no_speech_prob": 0.00014202346210367978,
        "seek": 25554,
        "start": 279.06,
        "temperature": 0,
        "text": " A lot of stuff that's in here matches up",
        "tokens": [
          51540,
          316,
          688,
          295,
          1507,
          300,
          311,
          294,
          510,
          10676,
          493,
          51648
        ]
      },
      {
        "avg_logprob": -0.18446650818316607,
        "compression_ratio": 1.6996197718631179,
        "end": 284.98,
        "id": 109,
        "no_speech_prob": 0.00014202346210367978,
        "seek": 25554,
        "start": 281.21999999999997,
        "temperature": 0,
        "text": " with the properties that you set for your ML5 neural network.",
        "tokens": [
          51648,
          365,
          264,
          7221,
          300,
          291,
          992,
          337,
          428,
          21601,
          20,
          18161,
          3209,
          13,
          51836
        ]
      },
      {
        "avg_logprob": -0.20581092490806235,
        "compression_ratio": 1.6906779661016949,
        "end": 287.58000000000004,
        "id": 110,
        "no_speech_prob": 0.00007254353840835392,
        "seek": 28498,
        "start": 284.98,
        "temperature": 0,
        "text": " So that is model.json.",
        "tokens": [
          50364,
          407,
          300,
          307,
          2316,
          13,
          73,
          3015,
          13,
          50494
        ]
      },
      {
        "avg_logprob": -0.20581092490806235,
        "compression_ratio": 1.6906779661016949,
        "end": 290.62,
        "id": 111,
        "no_speech_prob": 0.00007254353840835392,
        "seek": 28498,
        "start": 287.58000000000004,
        "temperature": 0,
        "text": " Now, what about model underscore meta.json?",
        "tokens": [
          50494,
          823,
          11,
          437,
          466,
          2316,
          37556,
          19616,
          13,
          73,
          3015,
          30,
          50646
        ]
      },
      {
        "avg_logprob": -0.20581092490806235,
        "compression_ratio": 1.6906779661016949,
        "end": 293.5,
        "id": 112,
        "no_speech_prob": 0.00007254353840835392,
        "seek": 28498,
        "start": 290.62,
        "temperature": 0,
        "text": " I mean, isn't this already all of the meta information",
        "tokens": [
          50646,
          286,
          914,
          11,
          1943,
          380,
          341,
          1217,
          439,
          295,
          264,
          19616,
          1589,
          50790
        ]
      },
      {
        "avg_logprob": -0.20581092490806235,
        "compression_ratio": 1.6906779661016949,
        "end": 294.74,
        "id": 113,
        "no_speech_prob": 0.00007254353840835392,
        "seek": 28498,
        "start": 293.5,
        "temperature": 0,
        "text": " for the model?",
        "tokens": [
          50790,
          337,
          264,
          2316,
          30,
          50852
        ]
      },
      {
        "avg_logprob": -0.20581092490806235,
        "compression_ratio": 1.6906779661016949,
        "end": 296.56,
        "id": 114,
        "no_speech_prob": 0.00007254353840835392,
        "seek": 28498,
        "start": 294.74,
        "temperature": 0,
        "text": " So while that is true, everything",
        "tokens": [
          50852,
          407,
          1339,
          300,
          307,
          2074,
          11,
          1203,
          50943
        ]
      },
      {
        "avg_logprob": -0.20581092490806235,
        "compression_ratio": 1.6906779661016949,
        "end": 299.38,
        "id": 115,
        "no_speech_prob": 0.00007254353840835392,
        "seek": 28498,
        "start": 296.56,
        "temperature": 0,
        "text": " that you're seeing here is the meta information,",
        "tokens": [
          50943,
          300,
          291,
          434,
          2577,
          510,
          307,
          264,
          19616,
          1589,
          11,
          51084
        ]
      },
      {
        "avg_logprob": -0.20581092490806235,
        "compression_ratio": 1.6906779661016949,
        "end": 303.74,
        "id": 116,
        "no_speech_prob": 0.00007254353840835392,
        "seek": 28498,
        "start": 299.38,
        "temperature": 0,
        "text": " the architecture of the model that TensorFlow.js is",
        "tokens": [
          51084,
          264,
          9482,
          295,
          264,
          2316,
          300,
          37624,
          13,
          25530,
          307,
          51302
        ]
      },
      {
        "avg_logprob": -0.20581092490806235,
        "compression_ratio": 1.6906779661016949,
        "end": 304.78000000000003,
        "id": 117,
        "no_speech_prob": 0.00007254353840835392,
        "seek": 28498,
        "start": 303.74,
        "temperature": 0,
        "text": " expecting.",
        "tokens": [
          51302,
          9650,
          13,
          51354
        ]
      },
      {
        "avg_logprob": -0.20581092490806235,
        "compression_ratio": 1.6906779661016949,
        "end": 308.38,
        "id": 118,
        "no_speech_prob": 0.00007254353840835392,
        "seek": 28498,
        "start": 304.78000000000003,
        "temperature": 0,
        "text": " But ML5 also keeps track of additional information",
        "tokens": [
          51354,
          583,
          21601,
          20,
          611,
          5965,
          2837,
          295,
          4497,
          1589,
          51534
        ]
      },
      {
        "avg_logprob": -0.20581092490806235,
        "compression_ratio": 1.6906779661016949,
        "end": 313.54,
        "id": 119,
        "no_speech_prob": 0.00007254353840835392,
        "seek": 28498,
        "start": 308.38,
        "temperature": 0,
        "text": " that's for the ML5 library only and not related to TensorFlow.js.",
        "tokens": [
          51534,
          300,
          311,
          337,
          264,
          21601,
          20,
          6405,
          787,
          293,
          406,
          4077,
          281,
          37624,
          13,
          25530,
          13,
          51792
        ]
      },
      {
        "avg_logprob": -0.194543823829064,
        "compression_ratio": 1.619047619047619,
        "end": 316.58000000000004,
        "id": 120,
        "no_speech_prob": 0.0007437003077939153,
        "seek": 31354,
        "start": 313.54,
        "temperature": 0,
        "text": " And that's what's in model underscore meta.",
        "tokens": [
          50364,
          400,
          300,
          311,
          437,
          311,
          294,
          2316,
          37556,
          19616,
          13,
          50516
        ]
      },
      {
        "avg_logprob": -0.194543823829064,
        "compression_ratio": 1.619047619047619,
        "end": 320.78000000000003,
        "id": 121,
        "no_speech_prob": 0.0007437003077939153,
        "seek": 31354,
        "start": 316.58000000000004,
        "temperature": 0,
        "text": " ML5, for example, keeps track of what the actual label names",
        "tokens": [
          50516,
          21601,
          20,
          11,
          337,
          1365,
          11,
          5965,
          2837,
          295,
          437,
          264,
          3539,
          7645,
          5288,
          50726
        ]
      },
      {
        "avg_logprob": -0.194543823829064,
        "compression_ratio": 1.619047619047619,
        "end": 321.26000000000005,
        "id": 122,
        "no_speech_prob": 0.0007437003077939153,
        "seek": 31354,
        "start": 320.78000000000003,
        "temperature": 0,
        "text": " are.",
        "tokens": [
          50726,
          366,
          13,
          50750
        ]
      },
      {
        "avg_logprob": -0.194543823829064,
        "compression_ratio": 1.619047619047619,
        "end": 324.54,
        "id": 123,
        "no_speech_prob": 0.0007437003077939153,
        "seek": 31354,
        "start": 321.26000000000005,
        "temperature": 0,
        "text": " That's not something that the neural network has.",
        "tokens": [
          50750,
          663,
          311,
          406,
          746,
          300,
          264,
          18161,
          3209,
          575,
          13,
          50914
        ]
      },
      {
        "avg_logprob": -0.194543823829064,
        "compression_ratio": 1.619047619047619,
        "end": 325.74,
        "id": 124,
        "no_speech_prob": 0.0007437003077939153,
        "seek": 31354,
        "start": 324.54,
        "temperature": 0,
        "text": " Everything's just numbers.",
        "tokens": [
          50914,
          5471,
          311,
          445,
          3547,
          13,
          50974
        ]
      },
      {
        "avg_logprob": -0.194543823829064,
        "compression_ratio": 1.619047619047619,
        "end": 328.02000000000004,
        "id": 125,
        "no_speech_prob": 0.0007437003077939153,
        "seek": 31354,
        "start": 325.74,
        "temperature": 0,
        "text": " But that's something that we want to use in our code.",
        "tokens": [
          50974,
          583,
          300,
          311,
          746,
          300,
          321,
          528,
          281,
          764,
          294,
          527,
          3089,
          13,
          51088
        ]
      },
      {
        "avg_logprob": -0.194543823829064,
        "compression_ratio": 1.619047619047619,
        "end": 330.34000000000003,
        "id": 126,
        "no_speech_prob": 0.0007437003077939153,
        "seek": 31354,
        "start": 328.02000000000004,
        "temperature": 0,
        "text": " So ML5 is keeping track of that for us.",
        "tokens": [
          51088,
          407,
          21601,
          20,
          307,
          5145,
          2837,
          295,
          300,
          337,
          505,
          13,
          51204
        ]
      },
      {
        "avg_logprob": -0.194543823829064,
        "compression_ratio": 1.619047619047619,
        "end": 333.46000000000004,
        "id": 127,
        "no_speech_prob": 0.0007437003077939153,
        "seek": 31354,
        "start": 330.34000000000003,
        "temperature": 0,
        "text": " If you recall, there was this whole normalization process",
        "tokens": [
          51204,
          759,
          291,
          9901,
          11,
          456,
          390,
          341,
          1379,
          2710,
          2144,
          1399,
          51360
        ]
      },
      {
        "avg_logprob": -0.194543823829064,
        "compression_ratio": 1.619047619047619,
        "end": 335.42,
        "id": 128,
        "no_speech_prob": 0.0007437003077939153,
        "seek": 31354,
        "start": 333.46000000000004,
        "temperature": 0,
        "text": " that I talked about in the first video",
        "tokens": [
          51360,
          300,
          286,
          2825,
          466,
          294,
          264,
          700,
          960,
          51458
        ]
      },
      {
        "avg_logprob": -0.194543823829064,
        "compression_ratio": 1.619047619047619,
        "end": 338.14000000000004,
        "id": 129,
        "no_speech_prob": 0.0007437003077939153,
        "seek": 31354,
        "start": 335.42,
        "temperature": 0,
        "text": " where you need to take the input values",
        "tokens": [
          51458,
          689,
          291,
          643,
          281,
          747,
          264,
          4846,
          4190,
          51594
        ]
      },
      {
        "avg_logprob": -0.194543823829064,
        "compression_ratio": 1.619047619047619,
        "end": 341.38,
        "id": 130,
        "no_speech_prob": 0.0007437003077939153,
        "seek": 31354,
        "start": 338.14000000000004,
        "temperature": 0,
        "text": " and squash them into a standardized range between 0",
        "tokens": [
          51594,
          293,
          30725,
          552,
          666,
          257,
          31677,
          3613,
          1296,
          1958,
          51756
        ]
      },
      {
        "avg_logprob": -0.194543823829064,
        "compression_ratio": 1.619047619047619,
        "end": 342.06,
        "id": 131,
        "no_speech_prob": 0.0007437003077939153,
        "seek": 31354,
        "start": 341.38,
        "temperature": 0,
        "text": " and 1.",
        "tokens": [
          51756,
          293,
          502,
          13,
          51790
        ]
      },
      {
        "avg_logprob": -0.22355270385742188,
        "compression_ratio": 1.627831715210356,
        "end": 343.98,
        "id": 132,
        "no_speech_prob": 0.00003219229256501421,
        "seek": 34206,
        "start": 342.06,
        "temperature": 0,
        "text": " So you need the minimum and maximum ranges",
        "tokens": [
          50364,
          407,
          291,
          643,
          264,
          7285,
          293,
          6674,
          22526,
          50460
        ]
      },
      {
        "avg_logprob": -0.22355270385742188,
        "compression_ratio": 1.627831715210356,
        "end": 346.06,
        "id": 133,
        "no_speech_prob": 0.00003219229256501421,
        "seek": 34206,
        "start": 343.98,
        "temperature": 0,
        "text": " for those data fields.",
        "tokens": [
          50460,
          337,
          729,
          1412,
          7909,
          13,
          50564
        ]
      },
      {
        "avg_logprob": -0.22355270385742188,
        "compression_ratio": 1.627831715210356,
        "end": 348.62,
        "id": 134,
        "no_speech_prob": 0.00003219229256501421,
        "seek": 34206,
        "start": 346.06,
        "temperature": 0,
        "text": " And ML5 is storing that for you in these input",
        "tokens": [
          50564,
          400,
          21601,
          20,
          307,
          26085,
          300,
          337,
          291,
          294,
          613,
          4846,
          50692
        ]
      },
      {
        "avg_logprob": -0.22355270385742188,
        "compression_ratio": 1.627831715210356,
        "end": 351.1,
        "id": 135,
        "no_speech_prob": 0.00003219229256501421,
        "seek": 34206,
        "start": 348.62,
        "temperature": 0,
        "text": " matmin and input max variables.",
        "tokens": [
          50692,
          3803,
          2367,
          293,
          4846,
          11469,
          9102,
          13,
          50816
        ]
      },
      {
        "avg_logprob": -0.22355270385742188,
        "compression_ratio": 1.627831715210356,
        "end": 352.7,
        "id": 136,
        "no_speech_prob": 0.00003219229256501421,
        "seek": 34206,
        "start": 351.1,
        "temperature": 0,
        "text": " So it's keeping track of that.",
        "tokens": [
          50816,
          407,
          309,
          311,
          5145,
          2837,
          295,
          300,
          13,
          50896
        ]
      },
      {
        "avg_logprob": -0.22355270385742188,
        "compression_ratio": 1.627831715210356,
        "end": 354.98,
        "id": 137,
        "no_speech_prob": 0.00003219229256501421,
        "seek": 34206,
        "start": 352.7,
        "temperature": 0,
        "text": " So this is just additional meta information",
        "tokens": [
          50896,
          407,
          341,
          307,
          445,
          4497,
          19616,
          1589,
          51010
        ]
      },
      {
        "avg_logprob": -0.22355270385742188,
        "compression_ratio": 1.627831715210356,
        "end": 358.14,
        "id": 138,
        "no_speech_prob": 0.00003219229256501421,
        "seek": 34206,
        "start": 354.98,
        "temperature": 0,
        "text": " related to the model specific to ML5.",
        "tokens": [
          51010,
          4077,
          281,
          264,
          2316,
          2685,
          281,
          21601,
          20,
          13,
          51168
        ]
      },
      {
        "avg_logprob": -0.22355270385742188,
        "compression_ratio": 1.627831715210356,
        "end": 359.94,
        "id": 139,
        "no_speech_prob": 0.00003219229256501421,
        "seek": 34206,
        "start": 358.14,
        "temperature": 0,
        "text": " Now, there's one more file.",
        "tokens": [
          51168,
          823,
          11,
          456,
          311,
          472,
          544,
          3991,
          13,
          51258
        ]
      },
      {
        "avg_logprob": -0.22355270385742188,
        "compression_ratio": 1.627831715210356,
        "end": 361.66,
        "id": 140,
        "no_speech_prob": 0.00003219229256501421,
        "seek": 34206,
        "start": 359.94,
        "temperature": 0,
        "text": " This file is a bit mysterious.",
        "tokens": [
          51258,
          639,
          3991,
          307,
          257,
          857,
          13831,
          13,
          51344
        ]
      },
      {
        "avg_logprob": -0.22355270385742188,
        "compression_ratio": 1.627831715210356,
        "end": 363.26,
        "id": 141,
        "no_speech_prob": 0.00003219229256501421,
        "seek": 34206,
        "start": 361.66,
        "temperature": 0,
        "text": " ModelWeights.bin.",
        "tokens": [
          51344,
          17105,
          4360,
          5761,
          13,
          13496,
          13,
          51424
        ]
      },
      {
        "avg_logprob": -0.22355270385742188,
        "compression_ratio": 1.627831715210356,
        "end": 364.42,
        "id": 142,
        "no_speech_prob": 0.00003219229256501421,
        "seek": 34206,
        "start": 363.26,
        "temperature": 0,
        "text": " It's not a JSON file.",
        "tokens": [
          51424,
          467,
          311,
          406,
          257,
          31828,
          3991,
          13,
          51482
        ]
      },
      {
        "avg_logprob": -0.22355270385742188,
        "compression_ratio": 1.627831715210356,
        "end": 365.58,
        "id": 143,
        "no_speech_prob": 0.00003219229256501421,
        "seek": 34206,
        "start": 364.42,
        "temperature": 0,
        "text": " It's a binary file.",
        "tokens": [
          51482,
          467,
          311,
          257,
          17434,
          3991,
          13,
          51540
        ]
      },
      {
        "avg_logprob": -0.22355270385742188,
        "compression_ratio": 1.627831715210356,
        "end": 368.74,
        "id": 144,
        "no_speech_prob": 0.00003219229256501421,
        "seek": 34206,
        "start": 365.58,
        "temperature": 0,
        "text": " And that's why if I tried to open it in Visual Studio Code,",
        "tokens": [
          51540,
          400,
          300,
          311,
          983,
          498,
          286,
          3031,
          281,
          1269,
          309,
          294,
          23187,
          13500,
          15549,
          11,
          51698
        ]
      },
      {
        "avg_logprob": -0.22355270385742188,
        "compression_ratio": 1.627831715210356,
        "end": 370.14,
        "id": 145,
        "no_speech_prob": 0.00003219229256501421,
        "seek": 34206,
        "start": 368.74,
        "temperature": 0,
        "text": " I'd just see a lot of gobbledygook.",
        "tokens": [
          51698,
          286,
          1116,
          445,
          536,
          257,
          688,
          295,
          352,
          6692,
          1493,
          18103,
          1212,
          13,
          51768
        ]
      },
      {
        "avg_logprob": -0.22355270385742188,
        "compression_ratio": 1.627831715210356,
        "end": 371.7,
        "id": 146,
        "no_speech_prob": 0.00003219229256501421,
        "seek": 34206,
        "start": 370.14,
        "temperature": 0,
        "text": " Because it's just binary data.",
        "tokens": [
          51768,
          1436,
          309,
          311,
          445,
          17434,
          1412,
          13,
          51846
        ]
      },
      {
        "avg_logprob": -0.2247556250865065,
        "compression_ratio": 1.9053030303030303,
        "end": 374.58,
        "id": 147,
        "no_speech_prob": 0.00011959832045249641,
        "seek": 37170,
        "start": 371.82,
        "temperature": 0,
        "text": " And what it's storing, it's storing the secret sauce",
        "tokens": [
          50370,
          400,
          437,
          309,
          311,
          26085,
          11,
          309,
          311,
          26085,
          264,
          4054,
          4880,
          50508
        ]
      },
      {
        "avg_logprob": -0.2247556250865065,
        "compression_ratio": 1.9053030303030303,
        "end": 376.82,
        "id": 148,
        "no_speech_prob": 0.00011959832045249641,
        "seek": 37170,
        "start": 374.58,
        "temperature": 0,
        "text": " of the neural network, how it's been,",
        "tokens": [
          50508,
          295,
          264,
          18161,
          3209,
          11,
          577,
          309,
          311,
          668,
          11,
          50620
        ]
      },
      {
        "avg_logprob": -0.2247556250865065,
        "compression_ratio": 1.9053030303030303,
        "end": 379.58,
        "id": 149,
        "no_speech_prob": 0.00011959832045249641,
        "seek": 37170,
        "start": 376.82,
        "temperature": 0,
        "text": " the values that result after the training process,",
        "tokens": [
          50620,
          264,
          4190,
          300,
          1874,
          934,
          264,
          3097,
          1399,
          11,
          50758
        ]
      },
      {
        "avg_logprob": -0.2247556250865065,
        "compression_ratio": 1.9053030303030303,
        "end": 382.82,
        "id": 150,
        "no_speech_prob": 0.00011959832045249641,
        "seek": 37170,
        "start": 379.58,
        "temperature": 0,
        "text": " and the weights, the weights of all of these connections.",
        "tokens": [
          50758,
          293,
          264,
          17443,
          11,
          264,
          17443,
          295,
          439,
          295,
          613,
          9271,
          13,
          50920
        ]
      },
      {
        "avg_logprob": -0.2247556250865065,
        "compression_ratio": 1.9053030303030303,
        "end": 385.38,
        "id": 151,
        "no_speech_prob": 0.00011959832045249641,
        "seek": 37170,
        "start": 382.82,
        "temperature": 0,
        "text": " So every single connection between any given node,",
        "tokens": [
          50920,
          407,
          633,
          2167,
          4984,
          1296,
          604,
          2212,
          9984,
          11,
          51048
        ]
      },
      {
        "avg_logprob": -0.2247556250865065,
        "compression_ratio": 1.9053030303030303,
        "end": 387.62,
        "id": 152,
        "no_speech_prob": 0.00011959832045249641,
        "seek": 37170,
        "start": 385.38,
        "temperature": 0,
        "text": " an input node, and a hidden node, a hidden node,",
        "tokens": [
          51048,
          364,
          4846,
          9984,
          11,
          293,
          257,
          7633,
          9984,
          11,
          257,
          7633,
          9984,
          11,
          51160
        ]
      },
      {
        "avg_logprob": -0.2247556250865065,
        "compression_ratio": 1.9053030303030303,
        "end": 391.21999999999997,
        "id": 153,
        "no_speech_prob": 0.00011959832045249641,
        "seek": 37170,
        "start": 387.62,
        "temperature": 0,
        "text": " and an output node has a weight associated with it.",
        "tokens": [
          51160,
          293,
          364,
          5598,
          9984,
          575,
          257,
          3364,
          6615,
          365,
          309,
          13,
          51340
        ]
      },
      {
        "avg_logprob": -0.2247556250865065,
        "compression_ratio": 1.9053030303030303,
        "end": 393.65999999999997,
        "id": 154,
        "no_speech_prob": 0.00011959832045249641,
        "seek": 37170,
        "start": 391.21999999999997,
        "temperature": 0,
        "text": " And those weights are like dials that the neural network",
        "tokens": [
          51340,
          400,
          729,
          17443,
          366,
          411,
          5502,
          82,
          300,
          264,
          18161,
          3209,
          51462
        ]
      },
      {
        "avg_logprob": -0.2247556250865065,
        "compression_ratio": 1.9053030303030303,
        "end": 395.9,
        "id": 155,
        "no_speech_prob": 0.00011959832045249641,
        "seek": 37170,
        "start": 393.65999999999997,
        "temperature": 0,
        "text": " is tuning as it's being trained to try",
        "tokens": [
          51462,
          307,
          15164,
          382,
          309,
          311,
          885,
          8895,
          281,
          853,
          51574
        ]
      },
      {
        "avg_logprob": -0.2247556250865065,
        "compression_ratio": 1.9053030303030303,
        "end": 399.65999999999997,
        "id": 156,
        "no_speech_prob": 0.00011959832045249641,
        "seek": 37170,
        "start": 395.9,
        "temperature": 0,
        "text": " to optimize towards getting the proper outputs to match",
        "tokens": [
          51574,
          281,
          19719,
          3030,
          1242,
          264,
          2296,
          23930,
          281,
          2995,
          51762
        ]
      },
      {
        "avg_logprob": -0.24856676796610042,
        "compression_ratio": 1.7785714285714285,
        "end": 401.78000000000003,
        "id": 157,
        "no_speech_prob": 0.000054759679187554866,
        "seek": 39966,
        "start": 400.3,
        "temperature": 0,
        "text": " with given inputs.",
        "tokens": [
          50396,
          365,
          2212,
          15743,
          13,
          50470
        ]
      },
      {
        "avg_logprob": -0.24856676796610042,
        "compression_ratio": 1.7785714285714285,
        "end": 403.54,
        "id": 158,
        "no_speech_prob": 0.000054759679187554866,
        "seek": 39966,
        "start": 401.78000000000003,
        "temperature": 0,
        "text": " So every time we train the neural network,",
        "tokens": [
          50470,
          407,
          633,
          565,
          321,
          3847,
          264,
          18161,
          3209,
          11,
          50558
        ]
      },
      {
        "avg_logprob": -0.24856676796610042,
        "compression_ratio": 1.7785714285714285,
        "end": 404.90000000000003,
        "id": 159,
        "no_speech_prob": 0.000054759679187554866,
        "seek": 39966,
        "start": 403.54,
        "temperature": 0,
        "text": " we might have different weights.",
        "tokens": [
          50558,
          321,
          1062,
          362,
          819,
          17443,
          13,
          50626
        ]
      },
      {
        "avg_logprob": -0.24856676796610042,
        "compression_ratio": 1.7785714285714285,
        "end": 406.26000000000005,
        "id": 160,
        "no_speech_prob": 0.000054759679187554866,
        "seek": 39966,
        "start": 404.90000000000003,
        "temperature": 0,
        "text": " The architecture is fixed.",
        "tokens": [
          50626,
          440,
          9482,
          307,
          6806,
          13,
          50694
        ]
      },
      {
        "avg_logprob": -0.24856676796610042,
        "compression_ratio": 1.7785714285714285,
        "end": 408.74,
        "id": 161,
        "no_speech_prob": 0.000054759679187554866,
        "seek": 39966,
        "start": 406.26000000000005,
        "temperature": 0,
        "text": " We've defined this neural network model.",
        "tokens": [
          50694,
          492,
          600,
          7642,
          341,
          18161,
          3209,
          2316,
          13,
          50818
        ]
      },
      {
        "avg_logprob": -0.24856676796610042,
        "compression_ratio": 1.7785714285714285,
        "end": 411.1,
        "id": 162,
        "no_speech_prob": 0.000054759679187554866,
        "seek": 39966,
        "start": 408.74,
        "temperature": 0,
        "text": " It has a fixed architecture and fixed meta information.",
        "tokens": [
          50818,
          467,
          575,
          257,
          6806,
          9482,
          293,
          6806,
          19616,
          1589,
          13,
          50936
        ]
      },
      {
        "avg_logprob": -0.24856676796610042,
        "compression_ratio": 1.7785714285714285,
        "end": 413.06,
        "id": 163,
        "no_speech_prob": 0.000054759679187554866,
        "seek": 39966,
        "start": 411.1,
        "temperature": 0,
        "text": " But the weights are going to be different based",
        "tokens": [
          50936,
          583,
          264,
          17443,
          366,
          516,
          281,
          312,
          819,
          2361,
          51034
        ]
      },
      {
        "avg_logprob": -0.24856676796610042,
        "compression_ratio": 1.7785714285714285,
        "end": 416.54,
        "id": 164,
        "no_speech_prob": 0.000054759679187554866,
        "seek": 39966,
        "start": 413.06,
        "temperature": 0,
        "text": " on the nuances of how the training process actually went.",
        "tokens": [
          51034,
          322,
          264,
          38775,
          295,
          577,
          264,
          3097,
          1399,
          767,
          1437,
          13,
          51208
        ]
      },
      {
        "avg_logprob": -0.24856676796610042,
        "compression_ratio": 1.7785714285714285,
        "end": 419.46000000000004,
        "id": 165,
        "no_speech_prob": 0.000054759679187554866,
        "seek": 39966,
        "start": 416.54,
        "temperature": 0,
        "text": " So now that we've established how to save the model",
        "tokens": [
          51208,
          407,
          586,
          300,
          321,
          600,
          7545,
          577,
          281,
          3155,
          264,
          2316,
          51354
        ]
      },
      {
        "avg_logprob": -0.24856676796610042,
        "compression_ratio": 1.7785714285714285,
        "end": 422.02000000000004,
        "id": 166,
        "no_speech_prob": 0.000054759679187554866,
        "seek": 39966,
        "start": 419.46000000000004,
        "temperature": 0,
        "text": " and what files you get when you save the model,",
        "tokens": [
          51354,
          293,
          437,
          7098,
          291,
          483,
          562,
          291,
          3155,
          264,
          2316,
          11,
          51482
        ]
      },
      {
        "avg_logprob": -0.24856676796610042,
        "compression_ratio": 1.7785714285714285,
        "end": 423.34000000000003,
        "id": 167,
        "no_speech_prob": 0.000054759679187554866,
        "seek": 39966,
        "start": 422.02000000000004,
        "temperature": 0,
        "text": " we're ready for the next step.",
        "tokens": [
          51482,
          321,
          434,
          1919,
          337,
          264,
          958,
          1823,
          13,
          51548
        ]
      },
      {
        "avg_logprob": -0.24856676796610042,
        "compression_ratio": 1.7785714285714285,
        "end": 426.66,
        "id": 168,
        "no_speech_prob": 0.000054759679187554866,
        "seek": 39966,
        "start": 423.34000000000003,
        "temperature": 0,
        "text": " And that is, when we first run the sketch,",
        "tokens": [
          51548,
          400,
          300,
          307,
          11,
          562,
          321,
          700,
          1190,
          264,
          12325,
          11,
          51714
        ]
      },
      {
        "avg_logprob": -0.19804024696350098,
        "compression_ratio": 1.8364485981308412,
        "end": 429.90000000000003,
        "id": 169,
        "no_speech_prob": 0.000010129977454198524,
        "seek": 42666,
        "start": 426.66,
        "temperature": 0,
        "text": " can we load all of these files into the model",
        "tokens": [
          50364,
          393,
          321,
          3677,
          439,
          295,
          613,
          7098,
          666,
          264,
          2316,
          50526
        ]
      },
      {
        "avg_logprob": -0.19804024696350098,
        "compression_ratio": 1.8364485981308412,
        "end": 434.70000000000005,
        "id": 170,
        "no_speech_prob": 0.000010129977454198524,
        "seek": 42666,
        "start": 429.90000000000003,
        "temperature": 0,
        "text": " and have what is, in essence, a pre-trained model ready to go?",
        "tokens": [
          50526,
          293,
          362,
          437,
          307,
          11,
          294,
          12801,
          11,
          257,
          659,
          12,
          17227,
          2001,
          2316,
          1919,
          281,
          352,
          30,
          50766
        ]
      },
      {
        "avg_logprob": -0.19804024696350098,
        "compression_ratio": 1.8364485981308412,
        "end": 438.3,
        "id": 171,
        "no_speech_prob": 0.000010129977454198524,
        "seek": 42666,
        "start": 434.70000000000005,
        "temperature": 0,
        "text": " And the function we need to do this is the load function.",
        "tokens": [
          50766,
          400,
          264,
          2445,
          321,
          643,
          281,
          360,
          341,
          307,
          264,
          3677,
          2445,
          13,
          50946
        ]
      },
      {
        "avg_logprob": -0.19804024696350098,
        "compression_ratio": 1.8364485981308412,
        "end": 440.82000000000005,
        "id": 172,
        "no_speech_prob": 0.000010129977454198524,
        "seek": 42666,
        "start": 438.3,
        "temperature": 0,
        "text": " So I need to say model.load.",
        "tokens": [
          50946,
          407,
          286,
          643,
          281,
          584,
          2316,
          13,
          2907,
          13,
          51072
        ]
      },
      {
        "avg_logprob": -0.19804024696350098,
        "compression_ratio": 1.8364485981308412,
        "end": 444.5,
        "id": 173,
        "no_speech_prob": 0.000010129977454198524,
        "seek": 42666,
        "start": 440.82000000000005,
        "temperature": 0,
        "text": " First things first, I want to upload those model files.",
        "tokens": [
          51072,
          2386,
          721,
          700,
          11,
          286,
          528,
          281,
          6580,
          729,
          2316,
          7098,
          13,
          51256
        ]
      },
      {
        "avg_logprob": -0.19804024696350098,
        "compression_ratio": 1.8364485981308412,
        "end": 446.66,
        "id": 174,
        "no_speech_prob": 0.000010129977454198524,
        "seek": 42666,
        "start": 444.5,
        "temperature": 0,
        "text": " So I'm going to create a folder.",
        "tokens": [
          51256,
          407,
          286,
          478,
          516,
          281,
          1884,
          257,
          10820,
          13,
          51364
        ]
      },
      {
        "avg_logprob": -0.19804024696350098,
        "compression_ratio": 1.8364485981308412,
        "end": 449.18,
        "id": 175,
        "no_speech_prob": 0.000010129977454198524,
        "seek": 42666,
        "start": 446.66,
        "temperature": 0,
        "text": " I'm going to call it model.",
        "tokens": [
          51364,
          286,
          478,
          516,
          281,
          818,
          309,
          2316,
          13,
          51490
        ]
      },
      {
        "avg_logprob": -0.19804024696350098,
        "compression_ratio": 1.8364485981308412,
        "end": 452.14000000000004,
        "id": 176,
        "no_speech_prob": 0.000010129977454198524,
        "seek": 42666,
        "start": 449.18,
        "temperature": 0,
        "text": " And then here, I'm going to add file.",
        "tokens": [
          51490,
          400,
          550,
          510,
          11,
          286,
          478,
          516,
          281,
          909,
          3991,
          13,
          51638
        ]
      },
      {
        "avg_logprob": -0.19804024696350098,
        "compression_ratio": 1.8364485981308412,
        "end": 454.98,
        "id": 177,
        "no_speech_prob": 0.000010129977454198524,
        "seek": 42666,
        "start": 452.14000000000004,
        "temperature": 0,
        "text": " And I'm going to upload these three files.",
        "tokens": [
          51638,
          400,
          286,
          478,
          516,
          281,
          6580,
          613,
          1045,
          7098,
          13,
          51780
        ]
      },
      {
        "avg_logprob": -0.24804923239718663,
        "compression_ratio": 1.93006993006993,
        "end": 457.18,
        "id": 178,
        "no_speech_prob": 0.000015689518477302045,
        "seek": 45498,
        "start": 454.98,
        "temperature": 0,
        "text": " So now we can see that in the web editor,",
        "tokens": [
          50364,
          407,
          586,
          321,
          393,
          536,
          300,
          294,
          264,
          3670,
          9839,
          11,
          50474
        ]
      },
      {
        "avg_logprob": -0.24804923239718663,
        "compression_ratio": 1.93006993006993,
        "end": 459.62,
        "id": 179,
        "no_speech_prob": 0.000015689518477302045,
        "seek": 45498,
        "start": 457.18,
        "temperature": 0,
        "text": " I have in a model directory model.json,",
        "tokens": [
          50474,
          286,
          362,
          294,
          257,
          2316,
          21120,
          2316,
          13,
          73,
          3015,
          11,
          50596
        ]
      },
      {
        "avg_logprob": -0.24804923239718663,
        "compression_ratio": 1.93006993006993,
        "end": 462.02000000000004,
        "id": 180,
        "no_speech_prob": 0.000015689518477302045,
        "seek": 45498,
        "start": 459.62,
        "temperature": 0,
        "text": " model.meta.json, and the weights.",
        "tokens": [
          50596,
          2316,
          13,
          5537,
          64,
          13,
          73,
          3015,
          11,
          293,
          264,
          17443,
          13,
          50716
        ]
      },
      {
        "avg_logprob": -0.24804923239718663,
        "compression_ratio": 1.93006993006993,
        "end": 463.82,
        "id": 181,
        "no_speech_prob": 0.000015689518477302045,
        "seek": 45498,
        "start": 462.02000000000004,
        "temperature": 0,
        "text": " So I don't want to load data anymore.",
        "tokens": [
          50716,
          407,
          286,
          500,
          380,
          528,
          281,
          3677,
          1412,
          3602,
          13,
          50806
        ]
      },
      {
        "avg_logprob": -0.24804923239718663,
        "compression_ratio": 1.93006993006993,
        "end": 466.14000000000004,
        "id": 182,
        "no_speech_prob": 0.000015689518477302045,
        "seek": 45498,
        "start": 463.82,
        "temperature": 0,
        "text": " There might be a reason why I also want to load the data.",
        "tokens": [
          50806,
          821,
          1062,
          312,
          257,
          1778,
          983,
          286,
          611,
          528,
          281,
          3677,
          264,
          1412,
          13,
          50922
        ]
      },
      {
        "avg_logprob": -0.24804923239718663,
        "compression_ratio": 1.93006993006993,
        "end": 467.3,
        "id": 183,
        "no_speech_prob": 0.000015689518477302045,
        "seek": 45498,
        "start": 466.14000000000004,
        "temperature": 0,
        "text": " But in this case, I don't.",
        "tokens": [
          50922,
          583,
          294,
          341,
          1389,
          11,
          286,
          500,
          380,
          13,
          50980
        ]
      },
      {
        "avg_logprob": -0.24804923239718663,
        "compression_ratio": 1.93006993006993,
        "end": 468.78000000000003,
        "id": 184,
        "no_speech_prob": 0.000015689518477302045,
        "seek": 45498,
        "start": 467.3,
        "temperature": 0,
        "text": " I just want the trained model.",
        "tokens": [
          50980,
          286,
          445,
          528,
          264,
          8895,
          2316,
          13,
          51054
        ]
      },
      {
        "avg_logprob": -0.24804923239718663,
        "compression_ratio": 1.93006993006993,
        "end": 470.3,
        "id": 185,
        "no_speech_prob": 0.000015689518477302045,
        "seek": 45498,
        "start": 468.78000000000003,
        "temperature": 0,
        "text": " Say model.load.",
        "tokens": [
          51054,
          6463,
          2316,
          13,
          2907,
          13,
          51130
        ]
      },
      {
        "avg_logprob": -0.24804923239718663,
        "compression_ratio": 1.93006993006993,
        "end": 472.22,
        "id": 186,
        "no_speech_prob": 0.000015689518477302045,
        "seek": 45498,
        "start": 470.3,
        "temperature": 0,
        "text": " And then I need to give it those files.",
        "tokens": [
          51130,
          400,
          550,
          286,
          643,
          281,
          976,
          309,
          729,
          7098,
          13,
          51226
        ]
      },
      {
        "avg_logprob": -0.24804923239718663,
        "compression_ratio": 1.93006993006993,
        "end": 473.82,
        "id": 187,
        "no_speech_prob": 0.000015689518477302045,
        "seek": 45498,
        "start": 472.22,
        "temperature": 0,
        "text": " I'm just going to say files right now.",
        "tokens": [
          51226,
          286,
          478,
          445,
          516,
          281,
          584,
          7098,
          558,
          586,
          13,
          51306
        ]
      },
      {
        "avg_logprob": -0.24804923239718663,
        "compression_ratio": 1.93006993006993,
        "end": 476.26,
        "id": 188,
        "no_speech_prob": 0.000015689518477302045,
        "seek": 45498,
        "start": 473.82,
        "temperature": 0,
        "text": " And then I'm going to write a callback called model.loaded.",
        "tokens": [
          51306,
          400,
          550,
          286,
          478,
          516,
          281,
          2464,
          257,
          818,
          3207,
          1219,
          2316,
          13,
          2907,
          292,
          13,
          51428
        ]
      },
      {
        "avg_logprob": -0.24804923239718663,
        "compression_ratio": 1.93006993006993,
        "end": 480.38,
        "id": 189,
        "no_speech_prob": 0.000015689518477302045,
        "seek": 45498,
        "start": 476.26,
        "temperature": 0,
        "text": " In the callback, I'm just going to give myself a message",
        "tokens": [
          51428,
          682,
          264,
          818,
          3207,
          11,
          286,
          478,
          445,
          516,
          281,
          976,
          2059,
          257,
          3636,
          51634
        ]
      },
      {
        "avg_logprob": -0.24804923239718663,
        "compression_ratio": 1.93006993006993,
        "end": 481.78000000000003,
        "id": 190,
        "no_speech_prob": 0.000015689518477302045,
        "seek": 45498,
        "start": 480.38,
        "temperature": 0,
        "text": " that the model is loaded.",
        "tokens": [
          51634,
          300,
          264,
          2316,
          307,
          13210,
          13,
          51704
        ]
      },
      {
        "avg_logprob": -0.24804923239718663,
        "compression_ratio": 1.93006993006993,
        "end": 483.18,
        "id": 191,
        "no_speech_prob": 0.000015689518477302045,
        "seek": 45498,
        "start": 481.78000000000003,
        "temperature": 0,
        "text": " And then what goes here?",
        "tokens": [
          51704,
          400,
          550,
          437,
          1709,
          510,
          30,
          51774
        ]
      },
      {
        "avg_logprob": -0.24804923239718663,
        "compression_ratio": 1.93006993006993,
        "end": 484.74,
        "id": 192,
        "no_speech_prob": 0.000015689518477302045,
        "seek": 45498,
        "start": 483.18,
        "temperature": 0,
        "text": " What goes in files?",
        "tokens": [
          51774,
          708,
          1709,
          294,
          7098,
          30,
          51852
        ]
      },
      {
        "avg_logprob": -0.1900288519198007,
        "compression_ratio": 1.7462686567164178,
        "end": 487.38,
        "id": 193,
        "no_speech_prob": 0.000024682851289981045,
        "seek": 48474,
        "start": 484.74,
        "temperature": 0,
        "text": " This is a little bit tricky because there are three files.",
        "tokens": [
          50364,
          639,
          307,
          257,
          707,
          857,
          12414,
          570,
          456,
          366,
          1045,
          7098,
          13,
          50496
        ]
      },
      {
        "avg_logprob": -0.1900288519198007,
        "compression_ratio": 1.7462686567164178,
        "end": 489.58,
        "id": 194,
        "no_speech_prob": 0.000024682851289981045,
        "seek": 48474,
        "start": 487.38,
        "temperature": 0,
        "text": " But there's an easy way to handle this.",
        "tokens": [
          50496,
          583,
          456,
          311,
          364,
          1858,
          636,
          281,
          4813,
          341,
          13,
          50606
        ]
      },
      {
        "avg_logprob": -0.1900288519198007,
        "compression_ratio": 1.7462686567164178,
        "end": 492.06,
        "id": 195,
        "no_speech_prob": 0.000024682851289981045,
        "seek": 48474,
        "start": 489.58,
        "temperature": 0,
        "text": " And if I go back to the ml5 documentation page,",
        "tokens": [
          50606,
          400,
          498,
          286,
          352,
          646,
          281,
          264,
          23271,
          20,
          14333,
          3028,
          11,
          50730
        ]
      },
      {
        "avg_logprob": -0.1900288519198007,
        "compression_ratio": 1.7462686567164178,
        "end": 492.86,
        "id": 196,
        "no_speech_prob": 0.000024682851289981045,
        "seek": 48474,
        "start": 492.06,
        "temperature": 0,
        "text": " it's right here.",
        "tokens": [
          50730,
          309,
          311,
          558,
          510,
          13,
          50770
        ]
      },
      {
        "avg_logprob": -0.1900288519198007,
        "compression_ratio": 1.7462686567164178,
        "end": 496.46000000000004,
        "id": 197,
        "no_speech_prob": 0.000024682851289981045,
        "seek": 48474,
        "start": 492.86,
        "temperature": 0,
        "text": " So I'm going to grab this little bit of code right here.",
        "tokens": [
          50770,
          407,
          286,
          478,
          516,
          281,
          4444,
          341,
          707,
          857,
          295,
          3089,
          558,
          510,
          13,
          50950
        ]
      },
      {
        "avg_logprob": -0.1900288519198007,
        "compression_ratio": 1.7462686567164178,
        "end": 500.3,
        "id": 198,
        "no_speech_prob": 0.000024682851289981045,
        "seek": 48474,
        "start": 496.46000000000004,
        "temperature": 0,
        "text": " And I'm going to paste it into my example.",
        "tokens": [
          50950,
          400,
          286,
          478,
          516,
          281,
          9163,
          309,
          666,
          452,
          1365,
          13,
          51142
        ]
      },
      {
        "avg_logprob": -0.1900288519198007,
        "compression_ratio": 1.7462686567164178,
        "end": 501.86,
        "id": 199,
        "no_speech_prob": 0.000024682851289981045,
        "seek": 48474,
        "start": 500.3,
        "temperature": 0,
        "text": " And this is actually the model info,",
        "tokens": [
          51142,
          400,
          341,
          307,
          767,
          264,
          2316,
          13614,
          11,
          51220
        ]
      },
      {
        "avg_logprob": -0.1900288519198007,
        "compression_ratio": 1.7462686567164178,
        "end": 503.02,
        "id": 200,
        "no_speech_prob": 0.000024682851289981045,
        "seek": 48474,
        "start": 501.86,
        "temperature": 0,
        "text": " which I will put right here.",
        "tokens": [
          51220,
          597,
          286,
          486,
          829,
          558,
          510,
          13,
          51278
        ]
      },
      {
        "avg_logprob": -0.1900288519198007,
        "compression_ratio": 1.7462686567164178,
        "end": 508.58,
        "id": 201,
        "no_speech_prob": 0.000024682851289981045,
        "seek": 48474,
        "start": 503.02,
        "temperature": 0,
        "text": " And the path is where I put it, which is under model.",
        "tokens": [
          51278,
          400,
          264,
          3100,
          307,
          689,
          286,
          829,
          309,
          11,
          597,
          307,
          833,
          2316,
          13,
          51556
        ]
      },
      {
        "avg_logprob": -0.1900288519198007,
        "compression_ratio": 1.7462686567164178,
        "end": 510.42,
        "id": 202,
        "no_speech_prob": 0.000024682851289981045,
        "seek": 48474,
        "start": 508.58,
        "temperature": 0,
        "text": " So now if I run this sketch, I should",
        "tokens": [
          51556,
          407,
          586,
          498,
          286,
          1190,
          341,
          12325,
          11,
          286,
          820,
          51648
        ]
      },
      {
        "avg_logprob": -0.1900288519198007,
        "compression_ratio": 1.7462686567164178,
        "end": 513.78,
        "id": 203,
        "no_speech_prob": 0.000024682851289981045,
        "seek": 48474,
        "start": 510.42,
        "temperature": 0,
        "text": " see in the console model loaded and no errors.",
        "tokens": [
          51648,
          536,
          294,
          264,
          11076,
          2316,
          13210,
          293,
          572,
          13603,
          13,
          51816
        ]
      },
      {
        "avg_logprob": -0.22410452807391132,
        "compression_ratio": 1.6591928251121075,
        "end": 516.8199999999999,
        "id": 204,
        "no_speech_prob": 0.000020145638700341806,
        "seek": 51378,
        "start": 513.78,
        "temperature": 0,
        "text": " That's what I'm hoping for.",
        "tokens": [
          50364,
          663,
          311,
          437,
          286,
          478,
          7159,
          337,
          13,
          50516
        ]
      },
      {
        "avg_logprob": -0.22410452807391132,
        "compression_ratio": 1.6591928251121075,
        "end": 518.38,
        "id": 205,
        "no_speech_prob": 0.000020145638700341806,
        "seek": 51378,
        "start": 516.8199999999999,
        "temperature": 0,
        "text": " Model loaded, no errors.",
        "tokens": [
          50516,
          17105,
          13210,
          11,
          572,
          13603,
          13,
          50594
        ]
      },
      {
        "avg_logprob": -0.22410452807391132,
        "compression_ratio": 1.6591928251121075,
        "end": 521.26,
        "id": 206,
        "no_speech_prob": 0.000020145638700341806,
        "seek": 51378,
        "start": 518.38,
        "temperature": 0,
        "text": " But did it really work?",
        "tokens": [
          50594,
          583,
          630,
          309,
          534,
          589,
          30,
          50738
        ]
      },
      {
        "avg_logprob": -0.22410452807391132,
        "compression_ratio": 1.6591928251121075,
        "end": 524.3399999999999,
        "id": 207,
        "no_speech_prob": 0.000020145638700341806,
        "seek": 51378,
        "start": 521.26,
        "temperature": 0,
        "text": " In order to test it, I've got to send my sketch straight",
        "tokens": [
          50738,
          682,
          1668,
          281,
          1500,
          309,
          11,
          286,
          600,
          658,
          281,
          2845,
          452,
          12325,
          2997,
          50892
        ]
      },
      {
        "avg_logprob": -0.22410452807391132,
        "compression_ratio": 1.6591928251121075,
        "end": 526.3,
        "id": 208,
        "no_speech_prob": 0.000020145638700341806,
        "seek": 51378,
        "start": 524.3399999999999,
        "temperature": 0,
        "text": " into prediction, the state of prediction.",
        "tokens": [
          50892,
          666,
          17630,
          11,
          264,
          1785,
          295,
          17630,
          13,
          50990
        ]
      },
      {
        "avg_logprob": -0.22410452807391132,
        "compression_ratio": 1.6591928251121075,
        "end": 529.9399999999999,
        "id": 209,
        "no_speech_prob": 0.000020145638700341806,
        "seek": 51378,
        "start": 526.3,
        "temperature": 0,
        "text": " So before, I had a collection state, a training state,",
        "tokens": [
          50990,
          407,
          949,
          11,
          286,
          632,
          257,
          5765,
          1785,
          11,
          257,
          3097,
          1785,
          11,
          51172
        ]
      },
      {
        "avg_logprob": -0.22410452807391132,
        "compression_ratio": 1.6591928251121075,
        "end": 530.9,
        "id": 210,
        "no_speech_prob": 0.000020145638700341806,
        "seek": 51378,
        "start": 529.9399999999999,
        "temperature": 0,
        "text": " and a prediction state.",
        "tokens": [
          51172,
          293,
          257,
          17630,
          1785,
          13,
          51220
        ]
      },
      {
        "avg_logprob": -0.22410452807391132,
        "compression_ratio": 1.6591928251121075,
        "end": 532.62,
        "id": 211,
        "no_speech_prob": 0.000020145638700341806,
        "seek": 51378,
        "start": 530.9,
        "temperature": 0,
        "text": " But if I'm loading the model directly,",
        "tokens": [
          51220,
          583,
          498,
          286,
          478,
          15114,
          264,
          2316,
          3838,
          11,
          51306
        ]
      },
      {
        "avg_logprob": -0.22410452807391132,
        "compression_ratio": 1.6591928251121075,
        "end": 537.3399999999999,
        "id": 212,
        "no_speech_prob": 0.000020145638700341806,
        "seek": 51378,
        "start": 532.62,
        "temperature": 0,
        "text": " I can just set the state right equal to prediction",
        "tokens": [
          51306,
          286,
          393,
          445,
          992,
          264,
          1785,
          558,
          2681,
          281,
          17630,
          51542
        ]
      },
      {
        "avg_logprob": -0.22410452807391132,
        "compression_ratio": 1.6591928251121075,
        "end": 538.62,
        "id": 213,
        "no_speech_prob": 0.000020145638700341806,
        "seek": 51378,
        "start": 537.3399999999999,
        "temperature": 0,
        "text": " and run it again and see.",
        "tokens": [
          51542,
          293,
          1190,
          309,
          797,
          293,
          536,
          13,
          51606
        ]
      },
      {
        "avg_logprob": -0.3265184285689373,
        "compression_ratio": 1.6201117318435754,
        "end": 539.12,
        "id": 214,
        "no_speech_prob": 0.00011412179446779191,
        "seek": 53862,
        "start": 538.62,
        "temperature": 0,
        "text": " Let's see.",
        "tokens": [
          50364,
          961,
          311,
          536,
          13,
          50389
        ]
      },
      {
        "avg_logprob": -0.3265184285689373,
        "compression_ratio": 1.6201117318435754,
        "end": 546.94,
        "id": 215,
        "no_speech_prob": 0.00011412179446779191,
        "seek": 53862,
        "start": 545.62,
        "temperature": 0,
        "text": " It looks like it works.",
        "tokens": [
          50714,
          467,
          1542,
          411,
          309,
          1985,
          13,
          50780
        ]
      },
      {
        "avg_logprob": -0.3265184285689373,
        "compression_ratio": 1.6201117318435754,
        "end": 548.44,
        "id": 216,
        "no_speech_prob": 0.00011412179446779191,
        "seek": 53862,
        "start": 546.94,
        "temperature": 0,
        "text": " Now, if I wanted to look at the data,",
        "tokens": [
          50780,
          823,
          11,
          498,
          286,
          1415,
          281,
          574,
          412,
          264,
          1412,
          11,
          50855
        ]
      },
      {
        "avg_logprob": -0.3265184285689373,
        "compression_ratio": 1.6201117318435754,
        "end": 550.1,
        "id": 217,
        "no_speech_prob": 0.00011412179446779191,
        "seek": 53862,
        "start": 548.44,
        "temperature": 0,
        "text": " I could also load the data.",
        "tokens": [
          50855,
          286,
          727,
          611,
          3677,
          264,
          1412,
          13,
          50938
        ]
      },
      {
        "avg_logprob": -0.3265184285689373,
        "compression_ratio": 1.6201117318435754,
        "end": 552.1,
        "id": 218,
        "no_speech_prob": 0.00011412179446779191,
        "seek": 53862,
        "start": 550.1,
        "temperature": 0,
        "text": " We could have everything in here.",
        "tokens": [
          50938,
          492,
          727,
          362,
          1203,
          294,
          510,
          13,
          51038
        ]
      },
      {
        "avg_logprob": -0.3265184285689373,
        "compression_ratio": 1.6201117318435754,
        "end": 554.74,
        "id": 219,
        "no_speech_prob": 0.00011412179446779191,
        "seek": 53862,
        "start": 552.1,
        "temperature": 0,
        "text": " I do not want to train the model.",
        "tokens": [
          51038,
          286,
          360,
          406,
          528,
          281,
          3847,
          264,
          2316,
          13,
          51170
        ]
      },
      {
        "avg_logprob": -0.3265184285689373,
        "compression_ratio": 1.6201117318435754,
        "end": 555.86,
        "id": 220,
        "no_speech_prob": 0.00011412179446779191,
        "seek": 53862,
        "start": 554.74,
        "temperature": 0,
        "text": " So let me try running this.",
        "tokens": [
          51170,
          407,
          718,
          385,
          853,
          2614,
          341,
          13,
          51226
        ]
      },
      {
        "avg_logprob": -0.3265184285689373,
        "compression_ratio": 1.6201117318435754,
        "end": 557.26,
        "id": 221,
        "no_speech_prob": 0.00011412179446779191,
        "seek": 53862,
        "start": 555.86,
        "temperature": 0,
        "text": " I should see the training data.",
        "tokens": [
          51226,
          286,
          820,
          536,
          264,
          3097,
          1412,
          13,
          51296
        ]
      },
      {
        "avg_logprob": -0.3265184285689373,
        "compression_ratio": 1.6201117318435754,
        "end": 557.94,
        "id": 222,
        "no_speech_prob": 0.00011412179446779191,
        "seek": 53862,
        "start": 557.26,
        "temperature": 0,
        "text": " But I don't.",
        "tokens": [
          51296,
          583,
          286,
          500,
          380,
          13,
          51330
        ]
      },
      {
        "avg_logprob": -0.3265184285689373,
        "compression_ratio": 1.6201117318435754,
        "end": 561.26,
        "id": 223,
        "no_speech_prob": 0.00011412179446779191,
        "seek": 53862,
        "start": 557.94,
        "temperature": 0,
        "text": " I'd load the pre-trained model and have it work.",
        "tokens": [
          51330,
          286,
          1116,
          3677,
          264,
          659,
          12,
          17227,
          2001,
          2316,
          293,
          362,
          309,
          589,
          13,
          51496
        ]
      },
      {
        "avg_logprob": -0.23960440499441965,
        "compression_ratio": 1.696,
        "end": 570.74,
        "id": 224,
        "no_speech_prob": 0.00024536854471080005,
        "seek": 56862,
        "start": 569.62,
        "temperature": 0,
        "text": " Hooray!",
        "tokens": [
          50414,
          3631,
          284,
          320,
          0,
          50470
        ]
      },
      {
        "avg_logprob": -0.23960440499441965,
        "compression_ratio": 1.696,
        "end": 572.38,
        "id": 225,
        "no_speech_prob": 0.00024536854471080005,
        "seek": 56862,
        "start": 570.74,
        "temperature": 0,
        "text": " OK.",
        "tokens": [
          50470,
          2264,
          13,
          50552
        ]
      },
      {
        "avg_logprob": -0.23960440499441965,
        "compression_ratio": 1.696,
        "end": 574.26,
        "id": 226,
        "no_speech_prob": 0.00024536854471080005,
        "seek": 56862,
        "start": 572.38,
        "temperature": 0,
        "text": " This example is complete.",
        "tokens": [
          50552,
          639,
          1365,
          307,
          3566,
          13,
          50646
        ]
      },
      {
        "avg_logprob": -0.23960440499441965,
        "compression_ratio": 1.696,
        "end": 578.34,
        "id": 227,
        "no_speech_prob": 0.00024536854471080005,
        "seek": 56862,
        "start": 574.26,
        "temperature": 0,
        "text": " We have seen all of the steps, how to collect the data,",
        "tokens": [
          50646,
          492,
          362,
          1612,
          439,
          295,
          264,
          4439,
          11,
          577,
          281,
          2500,
          264,
          1412,
          11,
          50850
        ]
      },
      {
        "avg_logprob": -0.23960440499441965,
        "compression_ratio": 1.696,
        "end": 580.92,
        "id": 228,
        "no_speech_prob": 0.00024536854471080005,
        "seek": 56862,
        "start": 578.34,
        "temperature": 0,
        "text": " how to train the model, and then how to deploy and use",
        "tokens": [
          50850,
          577,
          281,
          3847,
          264,
          2316,
          11,
          293,
          550,
          577,
          281,
          7274,
          293,
          764,
          50979
        ]
      },
      {
        "avg_logprob": -0.23960440499441965,
        "compression_ratio": 1.696,
        "end": 581.82,
        "id": 229,
        "no_speech_prob": 0.00024536854471080005,
        "seek": 56862,
        "start": 580.92,
        "temperature": 0,
        "text": " the model.",
        "tokens": [
          50979,
          264,
          2316,
          13,
          51024
        ]
      },
      {
        "avg_logprob": -0.23960440499441965,
        "compression_ratio": 1.696,
        "end": 586.18,
        "id": 230,
        "no_speech_prob": 0.00024536854471080005,
        "seek": 56862,
        "start": 581.82,
        "temperature": 0,
        "text": " Now, we've also added being able to save the data",
        "tokens": [
          51024,
          823,
          11,
          321,
          600,
          611,
          3869,
          885,
          1075,
          281,
          3155,
          264,
          1412,
          51242
        ]
      },
      {
        "avg_logprob": -0.23960440499441965,
        "compression_ratio": 1.696,
        "end": 588.58,
        "id": 231,
        "no_speech_prob": 0.00024536854471080005,
        "seek": 56862,
        "start": 586.18,
        "temperature": 0,
        "text": " after collecting it so that if we rerun the sketch later,",
        "tokens": [
          51242,
          934,
          12510,
          309,
          370,
          300,
          498,
          321,
          43819,
          409,
          264,
          12325,
          1780,
          11,
          51362
        ]
      },
      {
        "avg_logprob": -0.23960440499441965,
        "compression_ratio": 1.696,
        "end": 592.14,
        "id": 232,
        "no_speech_prob": 0.00024536854471080005,
        "seek": 56862,
        "start": 588.58,
        "temperature": 0,
        "text": " we could reload it, as well as save the actual trained model",
        "tokens": [
          51362,
          321,
          727,
          25628,
          309,
          11,
          382,
          731,
          382,
          3155,
          264,
          3539,
          8895,
          2316,
          51540
        ]
      },
      {
        "avg_logprob": -0.23960440499441965,
        "compression_ratio": 1.696,
        "end": 593.98,
        "id": 233,
        "no_speech_prob": 0.00024536854471080005,
        "seek": 56862,
        "start": 592.14,
        "temperature": 0,
        "text": " so we could go and just load it again.",
        "tokens": [
          51540,
          370,
          321,
          727,
          352,
          293,
          445,
          3677,
          309,
          797,
          13,
          51632
        ]
      },
      {
        "avg_logprob": -0.23960440499441965,
        "compression_ratio": 1.696,
        "end": 596.58,
        "id": 234,
        "no_speech_prob": 0.00024536854471080005,
        "seek": 56862,
        "start": 593.98,
        "temperature": 0,
        "text": " So one thing I might say is I've kind of done everything",
        "tokens": [
          51632,
          407,
          472,
          551,
          286,
          1062,
          584,
          307,
          286,
          600,
          733,
          295,
          1096,
          1203,
          51762
        ]
      },
      {
        "avg_logprob": -0.21647070531975732,
        "compression_ratio": 1.778501628664495,
        "end": 599.22,
        "id": 235,
        "no_speech_prob": 0.006903665140271187,
        "seek": 59658,
        "start": 596.58,
        "temperature": 0,
        "text": " all in one sketch, which is quite useful, actually.",
        "tokens": [
          50364,
          439,
          294,
          472,
          12325,
          11,
          597,
          307,
          1596,
          4420,
          11,
          767,
          13,
          50496
        ]
      },
      {
        "avg_logprob": -0.21647070531975732,
        "compression_ratio": 1.778501628664495,
        "end": 601.86,
        "id": 236,
        "no_speech_prob": 0.006903665140271187,
        "seek": 59658,
        "start": 599.22,
        "temperature": 0,
        "text": " And there's a lot of interactive possibilities there.",
        "tokens": [
          50496,
          400,
          456,
          311,
          257,
          688,
          295,
          15141,
          12178,
          456,
          13,
          50628
        ]
      },
      {
        "avg_logprob": -0.21647070531975732,
        "compression_ratio": 1.778501628664495,
        "end": 604.26,
        "id": 237,
        "no_speech_prob": 0.006903665140271187,
        "seek": 59658,
        "start": 601.86,
        "temperature": 0,
        "text": " But you might also consider breaking it apart.",
        "tokens": [
          50628,
          583,
          291,
          1062,
          611,
          1949,
          7697,
          309,
          4936,
          13,
          50748
        ]
      },
      {
        "avg_logprob": -0.21647070531975732,
        "compression_ratio": 1.778501628664495,
        "end": 607.7,
        "id": 238,
        "no_speech_prob": 0.006903665140271187,
        "seek": 59658,
        "start": 604.26,
        "temperature": 0,
        "text": " Maybe you want to have three different sketches, one that's",
        "tokens": [
          50748,
          2704,
          291,
          528,
          281,
          362,
          1045,
          819,
          34547,
          11,
          472,
          300,
          311,
          50920
        ]
      },
      {
        "avg_logprob": -0.21647070531975732,
        "compression_ratio": 1.778501628664495,
        "end": 610.9000000000001,
        "id": 239,
        "no_speech_prob": 0.006903665140271187,
        "seek": 59658,
        "start": 607.7,
        "temperature": 0,
        "text": " a data collection sketch, one that is loading the data",
        "tokens": [
          50920,
          257,
          1412,
          5765,
          12325,
          11,
          472,
          300,
          307,
          15114,
          264,
          1412,
          51080
        ]
      },
      {
        "avg_logprob": -0.21647070531975732,
        "compression_ratio": 1.778501628664495,
        "end": 612.6600000000001,
        "id": 240,
        "no_speech_prob": 0.006903665140271187,
        "seek": 59658,
        "start": 610.9000000000001,
        "temperature": 0,
        "text": " and training the model sketch, and then",
        "tokens": [
          51080,
          293,
          3097,
          264,
          2316,
          12325,
          11,
          293,
          550,
          51168
        ]
      },
      {
        "avg_logprob": -0.21647070531975732,
        "compression_ratio": 1.778501628664495,
        "end": 615.4200000000001,
        "id": 241,
        "no_speech_prob": 0.006903665140271187,
        "seek": 59658,
        "start": 612.6600000000001,
        "temperature": 0,
        "text": " one that is just a loading a pre-trained model and inference",
        "tokens": [
          51168,
          472,
          300,
          307,
          445,
          257,
          15114,
          257,
          659,
          12,
          17227,
          2001,
          2316,
          293,
          38253,
          51306
        ]
      },
      {
        "avg_logprob": -0.21647070531975732,
        "compression_ratio": 1.778501628664495,
        "end": 615.94,
        "id": 242,
        "no_speech_prob": 0.006903665140271187,
        "seek": 59658,
        "start": 615.4200000000001,
        "temperature": 0,
        "text": " sketch.",
        "tokens": [
          51306,
          12325,
          13,
          51332
        ]
      },
      {
        "avg_logprob": -0.21647070531975732,
        "compression_ratio": 1.778501628664495,
        "end": 617.86,
        "id": 243,
        "no_speech_prob": 0.006903665140271187,
        "seek": 59658,
        "start": 615.94,
        "temperature": 0,
        "text": " So that could be an exercise for you, as well,",
        "tokens": [
          51332,
          407,
          300,
          727,
          312,
          364,
          5380,
          337,
          291,
          11,
          382,
          731,
          11,
          51428
        ]
      },
      {
        "avg_logprob": -0.21647070531975732,
        "compression_ratio": 1.778501628664495,
        "end": 619.9000000000001,
        "id": 244,
        "no_speech_prob": 0.006903665140271187,
        "seek": 59658,
        "start": 617.86,
        "temperature": 0,
        "text": " to divide all these pieces up.",
        "tokens": [
          51428,
          281,
          9845,
          439,
          613,
          3755,
          493,
          13,
          51530
        ]
      },
      {
        "avg_logprob": -0.21647070531975732,
        "compression_ratio": 1.778501628664495,
        "end": 621.58,
        "id": 245,
        "no_speech_prob": 0.006903665140271187,
        "seek": 59658,
        "start": 619.9000000000001,
        "temperature": 0,
        "text": " I've got more that I want to show.",
        "tokens": [
          51530,
          286,
          600,
          658,
          544,
          300,
          286,
          528,
          281,
          855,
          13,
          51614
        ]
      },
      {
        "avg_logprob": -0.21647070531975732,
        "compression_ratio": 1.778501628664495,
        "end": 625.12,
        "id": 246,
        "no_speech_prob": 0.006903665140271187,
        "seek": 59658,
        "start": 621.58,
        "temperature": 0,
        "text": " So in the next video, now that I have all these features",
        "tokens": [
          51614,
          407,
          294,
          264,
          958,
          960,
          11,
          586,
          300,
          286,
          362,
          439,
          613,
          4122,
          51791
        ]
      },
      {
        "avg_logprob": -0.24274519986884538,
        "compression_ratio": 1.649056603773585,
        "end": 627.04,
        "id": 247,
        "no_speech_prob": 0.00024536761338822544,
        "seek": 62512,
        "start": 625.12,
        "temperature": 0,
        "text": " and I have this fully working example,",
        "tokens": [
          50364,
          293,
          286,
          362,
          341,
          4498,
          1364,
          1365,
          11,
          50460
        ]
      },
      {
        "avg_logprob": -0.24274519986884538,
        "compression_ratio": 1.649056603773585,
        "end": 629.2,
        "id": 248,
        "no_speech_prob": 0.00024536761338822544,
        "seek": 62512,
        "start": 627.04,
        "temperature": 0,
        "text": " I want to show you a regression.",
        "tokens": [
          50460,
          286,
          528,
          281,
          855,
          291,
          257,
          24590,
          13,
          50568
        ]
      },
      {
        "avg_logprob": -0.24274519986884538,
        "compression_ratio": 1.649056603773585,
        "end": 631.4,
        "id": 249,
        "no_speech_prob": 0.00024536761338822544,
        "seek": 62512,
        "start": 629.2,
        "temperature": 0,
        "text": " And what I mean by regression is the output",
        "tokens": [
          50568,
          400,
          437,
          286,
          914,
          538,
          24590,
          307,
          264,
          5598,
          50678
        ]
      },
      {
        "avg_logprob": -0.24274519986884538,
        "compression_ratio": 1.649056603773585,
        "end": 634.18,
        "id": 250,
        "no_speech_prob": 0.00024536761338822544,
        "seek": 62512,
        "start": 631.4,
        "temperature": 0,
        "text": " of the neural network, instead of being a label,",
        "tokens": [
          50678,
          295,
          264,
          18161,
          3209,
          11,
          2602,
          295,
          885,
          257,
          7645,
          11,
          50817
        ]
      },
      {
        "avg_logprob": -0.24274519986884538,
        "compression_ratio": 1.649056603773585,
        "end": 636.74,
        "id": 251,
        "no_speech_prob": 0.00024536761338822544,
        "seek": 62512,
        "start": 634.18,
        "temperature": 0,
        "text": " a single note, is a predicted number.",
        "tokens": [
          50817,
          257,
          2167,
          3637,
          11,
          307,
          257,
          19147,
          1230,
          13,
          50945
        ]
      },
      {
        "avg_logprob": -0.24274519986884538,
        "compression_ratio": 1.649056603773585,
        "end": 638.44,
        "id": 252,
        "no_speech_prob": 0.00024536761338822544,
        "seek": 62512,
        "start": 636.74,
        "temperature": 0,
        "text": " In this case, I'll use a frequency.",
        "tokens": [
          50945,
          682,
          341,
          1389,
          11,
          286,
          603,
          764,
          257,
          7893,
          13,
          51030
        ]
      },
      {
        "avg_logprob": -0.24274519986884538,
        "compression_ratio": 1.649056603773585,
        "end": 641.36,
        "id": 253,
        "no_speech_prob": 0.00024536761338822544,
        "seek": 62512,
        "start": 638.44,
        "temperature": 0,
        "text": " So hopefully, it'll make more sense what I mean by that",
        "tokens": [
          51030,
          407,
          4696,
          11,
          309,
          603,
          652,
          544,
          2020,
          437,
          286,
          914,
          538,
          300,
          51176
        ]
      },
      {
        "avg_logprob": -0.24274519986884538,
        "compression_ratio": 1.649056603773585,
        "end": 642.76,
        "id": 254,
        "no_speech_prob": 0.00024536761338822544,
        "seek": 62512,
        "start": 641.36,
        "temperature": 0,
        "text": " when I do the actual example.",
        "tokens": [
          51176,
          562,
          286,
          360,
          264,
          3539,
          1365,
          13,
          51246
        ]
      },
      {
        "avg_logprob": -0.24274519986884538,
        "compression_ratio": 1.649056603773585,
        "end": 644.84,
        "id": 255,
        "no_speech_prob": 0.00024536761338822544,
        "seek": 62512,
        "start": 642.76,
        "temperature": 0,
        "text": " And I will talk through it a bit more in the next video.",
        "tokens": [
          51246,
          400,
          286,
          486,
          751,
          807,
          309,
          257,
          857,
          544,
          294,
          264,
          958,
          960,
          13,
          51350
        ]
      },
      {
        "avg_logprob": -0.24274519986884538,
        "compression_ratio": 1.649056603773585,
        "end": 645.76,
        "id": 256,
        "no_speech_prob": 0.00024536761338822544,
        "seek": 62512,
        "start": 644.84,
        "temperature": 0,
        "text": " So thanks for watching.",
        "tokens": [
          51350,
          407,
          3231,
          337,
          1976,
          13,
          51396
        ]
      },
      {
        "avg_logprob": -0.24274519986884538,
        "compression_ratio": 1.649056603773585,
        "end": 646.68,
        "id": 257,
        "no_speech_prob": 0.00024536761338822544,
        "seek": 62512,
        "start": 645.76,
        "temperature": 0,
        "text": " And I'll see you soon.",
        "tokens": [
          51396,
          400,
          286,
          603,
          536,
          291,
          2321,
          13,
          51442
        ]
      },
      {
        "avg_logprob": -0.24274519986884538,
        "compression_ratio": 1.649056603773585,
        "end": 647.28,
        "id": 258,
        "no_speech_prob": 0.00024536761338822544,
        "seek": 62512,
        "start": 646.68,
        "temperature": 0,
        "text": " Goodbye.",
        "tokens": [
          51442,
          15528,
          13,
          51472
        ]
      },
      {
        "avg_logprob": -0.9254032770792643,
        "compression_ratio": 0.5555555555555556,
        "end": 649,
        "id": 259,
        "no_speech_prob": 0.26327943801879883,
        "seek": 64728,
        "start": 647.28,
        "temperature": 0.2,
        "text": " Thank you.",
        "tokens": [
          50372,
          1044,
          291,
          13,
          50450
        ]
      }
    ],
    "transcription": " And we're back. I am ready in this video to show you how to save the trained model with ml5. So if you recall, what I previously just did in the previous video is I added a feature to my example which will load a data set and immediately start training the model. So you can see here a whole bunch of labeled xy points. The model is now training all the way up until 200 epochs. And when it gets to the end, I've got a trained model. And I can click around and have the model guess a particular note for a particular xy coordinate in the canvas. But if I press Stop and run it again, I have to retrain the model. And this is incredibly useful because I might want to try adjusting the data set, recollecting the data, retraining the model, trying different parameters, adding more epochs, fewer epochs, all sorts of possibilities. But once I'm done, once I feel like I'm satisfied with this model, I would also like to be able to just save the model and reload it so that I don't have to do anything with the data again. In other words, we're finished with these first two steps. Collect the data, train the model. We've used Save Data and Load Data previously. And now I just want to save the model so that I can deploy it. So to do that, there's really just one other function that I need, Save. So if I just say Save, that's the model, not the data. And then also, of course, I'm going to make use of this Load function as well. So let's go back to my trusty keypress interface and add one more key. Let's use M for saving the model. Change the key to M. Change the function to Save. And then I'm going to call this, I'm just going to call it mouse notes as well. Actually, let's go, maybe I need some more arguments here. Let's go and check the ml5 reference. So indeed, it's the same as before. I need to give it a name and then a callback for when it's completed. But in this case, I don't need to know when it's completed because I'm just going to see that the files are there. OK, let's run it. Train it and save it. And now I can hit M. Files have been downloaded. Interestingly, there are now four files in my Downloads directory. Now, I should point out that this file is from before. This is the data file. And I'm just going to delete it right now because there are actually now three files. So saving the model is different than saving the data. With the data, you just have one file, the actual data in JSON format. When you're saving the model, there are three files. Let's call one model.json. The next file is model underscore meta.json. And one more file, model underscore weights.bin. Now, I think while recording this video tutorial, I just discovered a bug in ml5 because these should actually be called mouse notes dot json, mouse notes underscore meta dot json, mouse notes dot weights dot bin. But it just used the default naming model. So maybe by the time you're watching this, there'll be a new version of ml5 that fixes that. But those are the default names. And the reason why there are three files is there's a lot of information to store related to a machine learning model. If we travel back in time for a moment, you might remember a diagram I drew in this very first video as part of this series. This is the neural network diagram. Looking at this, we can see the overall architecture of the network. There are two inputs. There is a hidden layer. And there is an output layer. This architecture is described in model dot json. This is what that model dot json file actually looks like. It's a little terrifying. There's lots of lower level details related to how machine learning and neural networks work. But we can even start to pick little and choose little bits here that we can begin to understand. For example, this is a sequential neural network. There's a sequence. The inputs will come in to the hidden layer to the output. Feed forward. There is a dense layer. A dense layer means every single input is connected to every single hidden node. All of the connections, it's very dense. Everything is connected to everything. We can see that the input shape has a 2 in it. There's two inputs. We could go down and find the output layer. And the output layer has seven units. Well, that doesn't match up with three here. But if you remember, the new data set that I trained in this example has C, D, E, F, G, A, B, seven nodes. So that output layer has seven units to it. A lot of stuff that's in here matches up with the properties that you set for your ML5 neural network. So that is model.json. Now, what about model underscore meta.json? I mean, isn't this already all of the meta information for the model? So while that is true, everything that you're seeing here is the meta information, the architecture of the model that TensorFlow.js is expecting. But ML5 also keeps track of additional information that's for the ML5 library only and not related to TensorFlow.js. And that's what's in model underscore meta. ML5, for example, keeps track of what the actual label names are. That's not something that the neural network has. Everything's just numbers. But that's something that we want to use in our code. So ML5 is keeping track of that for us. If you recall, there was this whole normalization process that I talked about in the first video where you need to take the input values and squash them into a standardized range between 0 and 1. So you need the minimum and maximum ranges for those data fields. And ML5 is storing that for you in these input matmin and input max variables. So it's keeping track of that. So this is just additional meta information related to the model specific to ML5. Now, there's one more file. This file is a bit mysterious. ModelWeights.bin. It's not a JSON file. It's a binary file. And that's why if I tried to open it in Visual Studio Code, I'd just see a lot of gobbledygook. Because it's just binary data. And what it's storing, it's storing the secret sauce of the neural network, how it's been, the values that result after the training process, and the weights, the weights of all of these connections. So every single connection between any given node, an input node, and a hidden node, a hidden node, and an output node has a weight associated with it. And those weights are like dials that the neural network is tuning as it's being trained to try to optimize towards getting the proper outputs to match with given inputs. So every time we train the neural network, we might have different weights. The architecture is fixed. We've defined this neural network model. It has a fixed architecture and fixed meta information. But the weights are going to be different based on the nuances of how the training process actually went. So now that we've established how to save the model and what files you get when you save the model, we're ready for the next step. And that is, when we first run the sketch, can we load all of these files into the model and have what is, in essence, a pre-trained model ready to go? And the function we need to do this is the load function. So I need to say model.load. First things first, I want to upload those model files. So I'm going to create a folder. I'm going to call it model. And then here, I'm going to add file. And I'm going to upload these three files. So now we can see that in the web editor, I have in a model directory model.json, model.meta.json, and the weights. So I don't want to load data anymore. There might be a reason why I also want to load the data. But in this case, I don't. I just want the trained model. Say model.load. And then I need to give it those files. I'm just going to say files right now. And then I'm going to write a callback called model.loaded. In the callback, I'm just going to give myself a message that the model is loaded. And then what goes here? What goes in files? This is a little bit tricky because there are three files. But there's an easy way to handle this. And if I go back to the ml5 documentation page, it's right here. So I'm going to grab this little bit of code right here. And I'm going to paste it into my example. And this is actually the model info, which I will put right here. And the path is where I put it, which is under model. So now if I run this sketch, I should see in the console model loaded and no errors. That's what I'm hoping for. Model loaded, no errors. But did it really work? In order to test it, I've got to send my sketch straight into prediction, the state of prediction. So before, I had a collection state, a training state, and a prediction state. But if I'm loading the model directly, I can just set the state right equal to prediction and run it again and see. Let's see. It looks like it works. Now, if I wanted to look at the data, I could also load the data. We could have everything in here. I do not want to train the model. So let me try running this. I should see the training data. But I don't. I'd load the pre-trained model and have it work. Hooray! OK. This example is complete. We have seen all of the steps, how to collect the data, how to train the model, and then how to deploy and use the model. Now, we've also added being able to save the data after collecting it so that if we rerun the sketch later, we could reload it, as well as save the actual trained model so we could go and just load it again. So one thing I might say is I've kind of done everything all in one sketch, which is quite useful, actually. And there's a lot of interactive possibilities there. But you might also consider breaking it apart. Maybe you want to have three different sketches, one that's a data collection sketch, one that is loading the data and training the model sketch, and then one that is just a loading a pre-trained model and inference sketch. So that could be an exercise for you, as well, to divide all these pieces up. I've got more that I want to show. So in the next video, now that I have all these features and I have this fully working example, I want to show you a regression. And what I mean by regression is the output of the neural network, instead of being a label, a single note, is a predicted number. In this case, I'll use a frequency. So hopefully, it'll make more sense what I mean by that when I do the actual example. And I will talk through it a bit more in the next video. So thanks for watching. And I'll see you soon. Goodbye. Thank you.",
    "translation": null
  },
  "error": null,
  "status": "succeeded",
  "created_at": "2023-09-26T21:03:30.356234Z",
  "started_at": "2023-09-26T21:12:31.920186Z",
  "completed_at": "2023-09-26T21:15:30.751108Z",
  "webhook": "https://83ceaa0b612c.ngrok.app/?video_id=wUrg9Hjkhg0",
  "webhook_events_filter": [
    "completed"
  ],
  "metrics": {
    "predict_time": 178.830922
  },
  "urls": {
    "cancel": "https://api.replicate.com/v1/predictions/nqkqfwbbp7impifzbuntg43nha/cancel",
    "get": "https://api.replicate.com/v1/predictions/nqkqfwbbp7impifzbuntg43nha"
  }
}