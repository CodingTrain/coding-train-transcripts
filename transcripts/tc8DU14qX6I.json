{
  "id": "2oaewdzbbaa3jizich7q6ihuby",
  "version": "91ee9c0c3df30478510ff8c8a3a545add1ad0259ad3a9f78fba57fbc05ee64f7",
  "input": {
    "audio": "https://upcdn.io/FW25b4F/raw/coding-train/tc8DU14qX6I.m4a"
  },
  "logs": "Transcribe with large-v2 model\nDetected language: English\n  0%|          | 0/93836 [00:00<?, ?frames/s]\n  3%|▎         | 2790/93836 [00:07<03:55, 385.93frames/s]\n  6%|▌         | 5684/93836 [00:14<03:47, 386.72frames/s]\n  9%|▉         | 8628/93836 [00:22<03:42, 383.61frames/s]\n 12%|█▏        | 11508/93836 [00:29<03:23, 403.73frames/s]\n 15%|█▌        | 14372/93836 [00:35<03:11, 415.44frames/s]\n 18%|█▊        | 17236/93836 [00:41<02:57, 431.75frames/s]\n 21%|██▏       | 20142/93836 [00:48<02:54, 421.80frames/s]\n 25%|██▍       | 22998/93836 [00:55<02:43, 432.36frames/s]\n 28%|██▊       | 25978/93836 [01:01<02:33, 441.93frames/s]\n 31%|███       | 28890/93836 [01:09<02:34, 421.11frames/s]\n 34%|███▎      | 31544/93836 [01:15<02:24, 430.21frames/s]\n 37%|███▋      | 34260/93836 [01:21<02:21, 420.68frames/s]\n 40%|███▉      | 37232/93836 [01:29<02:16, 414.51frames/s]\n 43%|████▎     | 40136/93836 [01:35<02:04, 432.48frames/s]\n 46%|████▌     | 43020/93836 [01:41<01:55, 439.00frames/s]\n 48%|████▊     | 45392/93836 [01:47<01:51, 436.19frames/s]\n 52%|█████▏    | 48368/93836 [01:52<01:39, 458.95frames/s]\n 54%|█████▍    | 51128/93836 [01:59<01:37, 437.52frames/s]\n 58%|█████▊    | 54024/93836 [02:08<01:39, 400.41frames/s]\n 61%|██████    | 57004/93836 [02:15<01:28, 416.88frames/s]\n 64%|██████▎   | 59796/93836 [02:21<01:21, 415.74frames/s]\n 67%|██████▋   | 62764/93836 [02:29<01:17, 401.44frames/s]\n 70%|██████▉   | 65524/93836 [02:37<01:12, 389.97frames/s]\n 73%|███████▎  | 68460/93836 [02:43<01:01, 410.75frames/s]\n 76%|███████▌  | 71380/93836 [02:50<00:53, 416.04frames/s]\n 79%|███████▉  | 74284/93836 [02:56<00:45, 434.21frames/s]\n 82%|████████▏ | 77156/93836 [03:03<00:38, 430.11frames/s]\n 85%|████████▌ | 79848/93836 [03:10<00:33, 412.67frames/s]\n 88%|████████▊ | 82820/93836 [03:15<00:24, 453.97frames/s]\n 91%|█████████ | 85592/93836 [03:20<00:17, 473.25frames/s]\n 94%|█████████▍| 88584/93836 [03:28<00:11, 440.04frames/s]\n 98%|█████████▊| 91572/93836 [03:36<00:05, 417.11frames/s]\n 99%|█████████▉| 92898/93836 [03:40<00:02, 404.15frames/s]\n99%|█████████▉| 92898/93836 [03:46<00:02, 409.53frames/s]\n",
  "output": {
    "detected_language": "english",
    "segments": [
      {
        "avg_logprob": -0.2538572541365387,
        "compression_ratio": 1.6524822695035462,
        "end": 4.36,
        "id": 0,
        "no_speech_prob": 0.013424014672636986,
        "seek": 0,
        "start": 0,
        "temperature": 0,
        "text": " Hello, and welcome to the first tutorial video",
        "tokens": [
          50364,
          2425,
          11,
          293,
          2928,
          281,
          264,
          700,
          7073,
          960,
          50582
        ]
      },
      {
        "avg_logprob": -0.2538572541365387,
        "compression_ratio": 1.6524822695035462,
        "end": 8.16,
        "id": 1,
        "no_speech_prob": 0.013424014672636986,
        "seek": 0,
        "start": 4.36,
        "temperature": 0,
        "text": " in this new series, Working with Data and APIs in JavaScript.",
        "tokens": [
          50582,
          294,
          341,
          777,
          2638,
          11,
          18337,
          365,
          11888,
          293,
          21445,
          294,
          15778,
          13,
          50772
        ]
      },
      {
        "avg_logprob": -0.2538572541365387,
        "compression_ratio": 1.6524822695035462,
        "end": 11.040000000000001,
        "id": 2,
        "no_speech_prob": 0.013424014672636986,
        "seek": 0,
        "start": 8.16,
        "temperature": 0,
        "text": " So in this first video, I just want to look at one thing,",
        "tokens": [
          50772,
          407,
          294,
          341,
          700,
          960,
          11,
          286,
          445,
          528,
          281,
          574,
          412,
          472,
          551,
          11,
          50916
        ]
      },
      {
        "avg_logprob": -0.2538572541365387,
        "compression_ratio": 1.6524822695035462,
        "end": 12.120000000000001,
        "id": 3,
        "no_speech_prob": 0.013424014672636986,
        "seek": 0,
        "start": 11.040000000000001,
        "temperature": 0,
        "text": " the WebFetch API.",
        "tokens": [
          50916,
          264,
          9573,
          37,
          7858,
          9362,
          13,
          50970
        ]
      },
      {
        "avg_logprob": -0.2538572541365387,
        "compression_ratio": 1.6524822695035462,
        "end": 13,
        "id": 4,
        "no_speech_prob": 0.013424014672636986,
        "seek": 0,
        "start": 12.120000000000001,
        "temperature": 0,
        "text": " Fetch is a function.",
        "tokens": [
          50970,
          479,
          7858,
          307,
          257,
          2445,
          13,
          51014
        ]
      },
      {
        "avg_logprob": -0.2538572541365387,
        "compression_ratio": 1.6524822695035462,
        "end": 14.32,
        "id": 5,
        "no_speech_prob": 0.013424014672636986,
        "seek": 0,
        "start": 13,
        "temperature": 0,
        "text": " It's a wonderful function.",
        "tokens": [
          51014,
          467,
          311,
          257,
          3715,
          2445,
          13,
          51080
        ]
      },
      {
        "avg_logprob": -0.2538572541365387,
        "compression_ratio": 1.6524822695035462,
        "end": 16.68,
        "id": 6,
        "no_speech_prob": 0.013424014672636986,
        "seek": 0,
        "start": 14.32,
        "temperature": 0,
        "text": " It allows you to fetch stuff, data, images,",
        "tokens": [
          51080,
          467,
          4045,
          291,
          281,
          23673,
          1507,
          11,
          1412,
          11,
          5267,
          11,
          51198
        ]
      },
      {
        "avg_logprob": -0.2538572541365387,
        "compression_ratio": 1.6524822695035462,
        "end": 18.52,
        "id": 7,
        "no_speech_prob": 0.013424014672636986,
        "seek": 0,
        "start": 16.68,
        "temperature": 0,
        "text": " from all sorts of different kinds of places,",
        "tokens": [
          51198,
          490,
          439,
          7527,
          295,
          819,
          3685,
          295,
          3190,
          11,
          51290
        ]
      },
      {
        "avg_logprob": -0.2538572541365387,
        "compression_ratio": 1.6524822695035462,
        "end": 19.52,
        "id": 8,
        "no_speech_prob": 0.013424014672636986,
        "seek": 0,
        "start": 18.52,
        "temperature": 0,
        "text": " and do stuff with it.",
        "tokens": [
          51290,
          293,
          360,
          1507,
          365,
          309,
          13,
          51340
        ]
      },
      {
        "avg_logprob": -0.2538572541365387,
        "compression_ratio": 1.6524822695035462,
        "end": 21.84,
        "id": 9,
        "no_speech_prob": 0.013424014672636986,
        "seek": 0,
        "start": 19.52,
        "temperature": 0,
        "text": " So we're going to start with a very basic example.",
        "tokens": [
          51340,
          407,
          321,
          434,
          516,
          281,
          722,
          365,
          257,
          588,
          3875,
          1365,
          13,
          51456
        ]
      },
      {
        "avg_logprob": -0.2538572541365387,
        "compression_ratio": 1.6524822695035462,
        "end": 23.080000000000002,
        "id": 10,
        "no_speech_prob": 0.013424014672636986,
        "seek": 0,
        "start": 21.84,
        "temperature": 0,
        "text": " It's right here.",
        "tokens": [
          51456,
          467,
          311,
          558,
          510,
          13,
          51518
        ]
      },
      {
        "avg_logprob": -0.2538572541365387,
        "compression_ratio": 1.6524822695035462,
        "end": 27.900000000000002,
        "id": 11,
        "no_speech_prob": 0.013424014672636986,
        "seek": 0,
        "start": 23.080000000000002,
        "temperature": 0,
        "text": " It fetches a rainbow image, a local file, a JPEG file,",
        "tokens": [
          51518,
          467,
          15136,
          3781,
          257,
          18526,
          3256,
          11,
          257,
          2654,
          3991,
          11,
          257,
          508,
          5208,
          38,
          3991,
          11,
          51759
        ]
      },
      {
        "avg_logprob": -0.24565590752495658,
        "compression_ratio": 1.6770186335403727,
        "end": 30.58,
        "id": 12,
        "no_speech_prob": 0.0003920367453247309,
        "seek": 2790,
        "start": 27.9,
        "temperature": 0,
        "text": " and displays it on the web page itself.",
        "tokens": [
          50364,
          293,
          20119,
          309,
          322,
          264,
          3670,
          3028,
          2564,
          13,
          50498
        ]
      },
      {
        "avg_logprob": -0.24565590752495658,
        "compression_ratio": 1.6770186335403727,
        "end": 32.26,
        "id": 13,
        "no_speech_prob": 0.0003920367453247309,
        "seek": 2790,
        "start": 30.58,
        "temperature": 0,
        "text": " That's going to build.",
        "tokens": [
          50498,
          663,
          311,
          516,
          281,
          1322,
          13,
          50582
        ]
      },
      {
        "avg_logprob": -0.24565590752495658,
        "compression_ratio": 1.6770186335403727,
        "end": 34.46,
        "id": 14,
        "no_speech_prob": 0.0003920367453247309,
        "seek": 2790,
        "start": 32.26,
        "temperature": 0,
        "text": " That's the foundation on top of which",
        "tokens": [
          50582,
          663,
          311,
          264,
          7030,
          322,
          1192,
          295,
          597,
          50692
        ]
      },
      {
        "avg_logprob": -0.24565590752495658,
        "compression_ratio": 1.6770186335403727,
        "end": 36.04,
        "id": 15,
        "no_speech_prob": 0.0003920367453247309,
        "seek": 2790,
        "start": 34.46,
        "temperature": 0,
        "text": " we will build other examples that",
        "tokens": [
          50692,
          321,
          486,
          1322,
          661,
          5110,
          300,
          50771
        ]
      },
      {
        "avg_logprob": -0.24565590752495658,
        "compression_ratio": 1.6770186335403727,
        "end": 37.86,
        "id": 16,
        "no_speech_prob": 0.0003920367453247309,
        "seek": 2790,
        "start": 36.04,
        "temperature": 0,
        "text": " are grabbing a spreadsheet of data,",
        "tokens": [
          50771,
          366,
          23771,
          257,
          27733,
          295,
          1412,
          11,
          50862
        ]
      },
      {
        "avg_logprob": -0.24565590752495658,
        "compression_ratio": 1.6770186335403727,
        "end": 41.06,
        "id": 17,
        "no_speech_prob": 0.0003920367453247309,
        "seek": 2790,
        "start": 37.86,
        "temperature": 0,
        "text": " and graphing it, or reaching out to some weather service,",
        "tokens": [
          50862,
          293,
          1295,
          79,
          571,
          309,
          11,
          420,
          9906,
          484,
          281,
          512,
          5503,
          2643,
          11,
          51022
        ]
      },
      {
        "avg_logprob": -0.24565590752495658,
        "compression_ratio": 1.6770186335403727,
        "end": 42.82,
        "id": 18,
        "no_speech_prob": 0.0003920367453247309,
        "seek": 2790,
        "start": 41.06,
        "temperature": 0,
        "text": " and getting the temperature based",
        "tokens": [
          51022,
          293,
          1242,
          264,
          4292,
          2361,
          51110
        ]
      },
      {
        "avg_logprob": -0.24565590752495658,
        "compression_ratio": 1.6770186335403727,
        "end": 45.3,
        "id": 19,
        "no_speech_prob": 0.0003920367453247309,
        "seek": 2790,
        "start": 42.82,
        "temperature": 0,
        "text": " on the latitude and longitude, all sorts of possibilities.",
        "tokens": [
          51110,
          322,
          264,
          45436,
          293,
          938,
          4377,
          11,
          439,
          7527,
          295,
          12178,
          13,
          51234
        ]
      },
      {
        "avg_logprob": -0.24565590752495658,
        "compression_ratio": 1.6770186335403727,
        "end": 47.26,
        "id": 20,
        "no_speech_prob": 0.0003920367453247309,
        "seek": 2790,
        "start": 45.3,
        "temperature": 0,
        "text": " We're going to start just by looking at Fetch.",
        "tokens": [
          51234,
          492,
          434,
          516,
          281,
          722,
          445,
          538,
          1237,
          412,
          479,
          7858,
          13,
          51332
        ]
      },
      {
        "avg_logprob": -0.24565590752495658,
        "compression_ratio": 1.6770186335403727,
        "end": 49.62,
        "id": 21,
        "no_speech_prob": 0.0003920367453247309,
        "seek": 2790,
        "start": 47.26,
        "temperature": 0,
        "text": " The best place for you to learn all of the details,",
        "tokens": [
          51332,
          440,
          1151,
          1081,
          337,
          291,
          281,
          1466,
          439,
          295,
          264,
          4365,
          11,
          51450
        ]
      },
      {
        "avg_logprob": -0.24565590752495658,
        "compression_ratio": 1.6770186335403727,
        "end": 51.3,
        "id": 22,
        "no_speech_prob": 0.0003920367453247309,
        "seek": 2790,
        "start": 49.62,
        "temperature": 0,
        "text": " with everything you could ever possibly",
        "tokens": [
          51450,
          365,
          1203,
          291,
          727,
          1562,
          6264,
          51534
        ]
      },
      {
        "avg_logprob": -0.24565590752495658,
        "compression_ratio": 1.6770186335403727,
        "end": 53.879999999999995,
        "id": 23,
        "no_speech_prob": 0.0003920367453247309,
        "seek": 2790,
        "start": 51.3,
        "temperature": 0,
        "text": " want to know about the Fetch web API,",
        "tokens": [
          51534,
          528,
          281,
          458,
          466,
          264,
          479,
          7858,
          3670,
          9362,
          11,
          51663
        ]
      },
      {
        "avg_logprob": -0.24565590752495658,
        "compression_ratio": 1.6770186335403727,
        "end": 56.84,
        "id": 24,
        "no_speech_prob": 0.0003920367453247309,
        "seek": 2790,
        "start": 53.879999999999995,
        "temperature": 0,
        "text": " is on the Mozilla Developer Network site.",
        "tokens": [
          51663,
          307,
          322,
          264,
          3335,
          26403,
          44915,
          12640,
          3621,
          13,
          51811
        ]
      },
      {
        "avg_logprob": -0.24415175914764403,
        "compression_ratio": 1.7647058823529411,
        "end": 59.6,
        "id": 25,
        "no_speech_prob": 0.00021995122369844466,
        "seek": 5684,
        "start": 56.84,
        "temperature": 0,
        "text": " So that's where I tend to look up documentation stuff",
        "tokens": [
          50364,
          407,
          300,
          311,
          689,
          286,
          3928,
          281,
          574,
          493,
          14333,
          1507,
          50502
        ]
      },
      {
        "avg_logprob": -0.24415175914764403,
        "compression_ratio": 1.7647058823529411,
        "end": 61.64,
        "id": 26,
        "no_speech_prob": 0.00021995122369844466,
        "seek": 5684,
        "start": 59.6,
        "temperature": 0,
        "text": " for JavaScript stuff I want to do in the browser.",
        "tokens": [
          50502,
          337,
          15778,
          1507,
          286,
          528,
          281,
          360,
          294,
          264,
          11185,
          13,
          50604
        ]
      },
      {
        "avg_logprob": -0.24415175914764403,
        "compression_ratio": 1.7647058823529411,
        "end": 63.56,
        "id": 27,
        "no_speech_prob": 0.00021995122369844466,
        "seek": 5684,
        "start": 61.64,
        "temperature": 0,
        "text": " I'm going to show you a small slice of what's",
        "tokens": [
          50604,
          286,
          478,
          516,
          281,
          855,
          291,
          257,
          1359,
          13153,
          295,
          437,
          311,
          50700
        ]
      },
      {
        "avg_logprob": -0.24415175914764403,
        "compression_ratio": 1.7647058823529411,
        "end": 65.72,
        "id": 28,
        "no_speech_prob": 0.00021995122369844466,
        "seek": 5684,
        "start": 63.56,
        "temperature": 0,
        "text": " possible with Fetch, and build on that",
        "tokens": [
          50700,
          1944,
          365,
          479,
          7858,
          11,
          293,
          1322,
          322,
          300,
          50808
        ]
      },
      {
        "avg_logprob": -0.24415175914764403,
        "compression_ratio": 1.7647058823529411,
        "end": 67.68,
        "id": 29,
        "no_speech_prob": 0.00021995122369844466,
        "seek": 5684,
        "start": 65.72,
        "temperature": 0,
        "text": " as we go video, video, to video.",
        "tokens": [
          50808,
          382,
          321,
          352,
          960,
          11,
          960,
          11,
          281,
          960,
          13,
          50906
        ]
      },
      {
        "avg_logprob": -0.24415175914764403,
        "compression_ratio": 1.7647058823529411,
        "end": 69.84,
        "id": 30,
        "no_speech_prob": 0.00021995122369844466,
        "seek": 5684,
        "start": 67.68,
        "temperature": 0,
        "text": " There's this idea in web programming",
        "tokens": [
          50906,
          821,
          311,
          341,
          1558,
          294,
          3670,
          9410,
          51014
        ]
      },
      {
        "avg_logprob": -0.24415175914764403,
        "compression_ratio": 1.7647058823529411,
        "end": 74.56,
        "id": 31,
        "no_speech_prob": 0.00021995122369844466,
        "seek": 5684,
        "start": 69.84,
        "temperature": 0,
        "text": " of making a request, an HTTP request, a Hypertext Transfer",
        "tokens": [
          51014,
          295,
          1455,
          257,
          5308,
          11,
          364,
          33283,
          5308,
          11,
          257,
          29592,
          25111,
          35025,
          51250
        ]
      },
      {
        "avg_logprob": -0.24415175914764403,
        "compression_ratio": 1.7647058823529411,
        "end": 75.68,
        "id": 32,
        "no_speech_prob": 0.00021995122369844466,
        "seek": 5684,
        "start": 74.56,
        "temperature": 0,
        "text": " Protocol request.",
        "tokens": [
          51250,
          48753,
          5308,
          13,
          51306
        ]
      },
      {
        "avg_logprob": -0.24415175914764403,
        "compression_ratio": 1.7647058823529411,
        "end": 76.96000000000001,
        "id": 33,
        "no_speech_prob": 0.00021995122369844466,
        "seek": 5684,
        "start": 75.68,
        "temperature": 0,
        "text": " I could make a GET request.",
        "tokens": [
          51306,
          286,
          727,
          652,
          257,
          28091,
          5308,
          13,
          51370
        ]
      },
      {
        "avg_logprob": -0.24415175914764403,
        "compression_ratio": 1.7647058823529411,
        "end": 77.92,
        "id": 34,
        "no_speech_prob": 0.00021995122369844466,
        "seek": 5684,
        "start": 76.96000000000001,
        "temperature": 0,
        "text": " Will you please give me?",
        "tokens": [
          51370,
          3099,
          291,
          1767,
          976,
          385,
          30,
          51418
        ]
      },
      {
        "avg_logprob": -0.24415175914764403,
        "compression_ratio": 1.7647058823529411,
        "end": 80.04,
        "id": 35,
        "no_speech_prob": 0.00021995122369844466,
        "seek": 5684,
        "start": 77.92,
        "temperature": 0,
        "text": " Could I please, could I please get some information?",
        "tokens": [
          51418,
          7497,
          286,
          1767,
          11,
          727,
          286,
          1767,
          483,
          512,
          1589,
          30,
          51524
        ]
      },
      {
        "avg_logprob": -0.24415175914764403,
        "compression_ratio": 1.7647058823529411,
        "end": 81.44,
        "id": 36,
        "no_speech_prob": 0.00021995122369844466,
        "seek": 5684,
        "start": 80.04,
        "temperature": 0,
        "text": " I could make a POST request.",
        "tokens": [
          51524,
          286,
          727,
          652,
          257,
          430,
          28067,
          5308,
          13,
          51594
        ]
      },
      {
        "avg_logprob": -0.24415175914764403,
        "compression_ratio": 1.7647058823529411,
        "end": 84.02000000000001,
        "id": 37,
        "no_speech_prob": 0.00021995122369844466,
        "seek": 5684,
        "start": 81.44,
        "temperature": 0,
        "text": " Could I please send some information to you?",
        "tokens": [
          51594,
          7497,
          286,
          1767,
          2845,
          512,
          1589,
          281,
          291,
          30,
          51723
        ]
      },
      {
        "avg_logprob": -0.24415175914764403,
        "compression_ratio": 1.7647058823529411,
        "end": 86.28,
        "id": 38,
        "no_speech_prob": 0.00021995122369844466,
        "seek": 5684,
        "start": 84.02000000000001,
        "temperature": 0,
        "text": " And we're going to do both of those kinds of requests.",
        "tokens": [
          51723,
          400,
          321,
          434,
          516,
          281,
          360,
          1293,
          295,
          729,
          3685,
          295,
          12475,
          13,
          51836
        ]
      },
      {
        "avg_logprob": -0.18430623654966002,
        "compression_ratio": 1.744186046511628,
        "end": 89.08,
        "id": 39,
        "no_speech_prob": 0.000013845945431967266,
        "seek": 8628,
        "start": 86.28,
        "temperature": 0,
        "text": " And Fetch can actually both retrieve and send.",
        "tokens": [
          50364,
          400,
          479,
          7858,
          393,
          767,
          1293,
          30254,
          293,
          2845,
          13,
          50504
        ]
      },
      {
        "avg_logprob": -0.18430623654966002,
        "compression_ratio": 1.744186046511628,
        "end": 92.52,
        "id": 40,
        "no_speech_prob": 0.000013845945431967266,
        "seek": 8628,
        "start": 89.08,
        "temperature": 0,
        "text": " But in this example, I just want to look at a simple GET",
        "tokens": [
          50504,
          583,
          294,
          341,
          1365,
          11,
          286,
          445,
          528,
          281,
          574,
          412,
          257,
          2199,
          28091,
          50676
        ]
      },
      {
        "avg_logprob": -0.18430623654966002,
        "compression_ratio": 1.744186046511628,
        "end": 94.84,
        "id": 41,
        "no_speech_prob": 0.000013845945431967266,
        "seek": 8628,
        "start": 92.52,
        "temperature": 0,
        "text": " request with Fetch.",
        "tokens": [
          50676,
          5308,
          365,
          479,
          7858,
          13,
          50792
        ]
      },
      {
        "avg_logprob": -0.18430623654966002,
        "compression_ratio": 1.744186046511628,
        "end": 96.76,
        "id": 42,
        "no_speech_prob": 0.000013845945431967266,
        "seek": 8628,
        "start": 94.84,
        "temperature": 0,
        "text": " Let me outline these steps for you.",
        "tokens": [
          50792,
          961,
          385,
          16387,
          613,
          4439,
          337,
          291,
          13,
          50888
        ]
      },
      {
        "avg_logprob": -0.18430623654966002,
        "compression_ratio": 1.744186046511628,
        "end": 99.52,
        "id": 43,
        "no_speech_prob": 0.000013845945431967266,
        "seek": 8628,
        "start": 96.76,
        "temperature": 0,
        "text": " So our program is going to do, the first thing it's going",
        "tokens": [
          50888,
          407,
          527,
          1461,
          307,
          516,
          281,
          360,
          11,
          264,
          700,
          551,
          309,
          311,
          516,
          51026
        ]
      },
      {
        "avg_logprob": -0.18430623654966002,
        "compression_ratio": 1.744186046511628,
        "end": 103.56,
        "id": 44,
        "no_speech_prob": 0.000013845945431967266,
        "seek": 8628,
        "start": 99.52,
        "temperature": 0,
        "text": " to do is going to call the Fetch function.",
        "tokens": [
          51026,
          281,
          360,
          307,
          516,
          281,
          818,
          264,
          479,
          7858,
          2445,
          13,
          51228
        ]
      },
      {
        "avg_logprob": -0.18430623654966002,
        "compression_ratio": 1.744186046511628,
        "end": 106.36,
        "id": 45,
        "no_speech_prob": 0.000013845945431967266,
        "seek": 8628,
        "start": 103.56,
        "temperature": 0,
        "text": " And it's going to give it a single argument, which",
        "tokens": [
          51228,
          400,
          309,
          311,
          516,
          281,
          976,
          309,
          257,
          2167,
          6770,
          11,
          597,
          51368
        ]
      },
      {
        "avg_logprob": -0.18430623654966002,
        "compression_ratio": 1.744186046511628,
        "end": 108.28,
        "id": 46,
        "no_speech_prob": 0.000013845945431967266,
        "seek": 8628,
        "start": 106.36,
        "temperature": 0,
        "text": " is the path to the resource.",
        "tokens": [
          51368,
          307,
          264,
          3100,
          281,
          264,
          7684,
          13,
          51464
        ]
      },
      {
        "avg_logprob": -0.18430623654966002,
        "compression_ratio": 1.744186046511628,
        "end": 110.24000000000001,
        "id": 47,
        "no_speech_prob": 0.000013845945431967266,
        "seek": 8628,
        "start": 108.28,
        "temperature": 0,
        "text": " And in this case, it's going to be a file.",
        "tokens": [
          51464,
          400,
          294,
          341,
          1389,
          11,
          309,
          311,
          516,
          281,
          312,
          257,
          3991,
          13,
          51562
        ]
      },
      {
        "avg_logprob": -0.18430623654966002,
        "compression_ratio": 1.744186046511628,
        "end": 112.84,
        "id": 48,
        "no_speech_prob": 0.000013845945431967266,
        "seek": 8628,
        "start": 110.24000000000001,
        "temperature": 0,
        "text": " And that file is just going to be a local file called",
        "tokens": [
          51562,
          400,
          300,
          3991,
          307,
          445,
          516,
          281,
          312,
          257,
          2654,
          3991,
          1219,
          51692
        ]
      },
      {
        "avg_logprob": -0.18430623654966002,
        "compression_ratio": 1.744186046511628,
        "end": 115.08,
        "id": 49,
        "no_speech_prob": 0.000013845945431967266,
        "seek": 8628,
        "start": 112.84,
        "temperature": 0,
        "text": " rainbow.jpg.",
        "tokens": [
          51692,
          18526,
          13,
          73,
          49861,
          13,
          51804
        ]
      },
      {
        "avg_logprob": -0.20565773292824074,
        "compression_ratio": 1.7473684210526317,
        "end": 118.6,
        "id": 50,
        "no_speech_prob": 0.0000035008454233320663,
        "seek": 11508,
        "start": 115.08,
        "temperature": 0,
        "text": " The next thing we need to do is deal with the response.",
        "tokens": [
          50364,
          440,
          958,
          551,
          321,
          643,
          281,
          360,
          307,
          2028,
          365,
          264,
          4134,
          13,
          50540
        ]
      },
      {
        "avg_logprob": -0.20565773292824074,
        "compression_ratio": 1.7473684210526317,
        "end": 121.56,
        "id": 51,
        "no_speech_prob": 0.0000035008454233320663,
        "seek": 11508,
        "start": 118.6,
        "temperature": 0,
        "text": " So when we call Fetch, a response comes back,",
        "tokens": [
          50540,
          407,
          562,
          321,
          818,
          479,
          7858,
          11,
          257,
          4134,
          1487,
          646,
          11,
          50688
        ]
      },
      {
        "avg_logprob": -0.20565773292824074,
        "compression_ratio": 1.7473684210526317,
        "end": 124.2,
        "id": 52,
        "no_speech_prob": 0.0000035008454233320663,
        "seek": 11508,
        "start": 121.56,
        "temperature": 0,
        "text": " presumably with the data we're looking for somewhere in there.",
        "tokens": [
          50688,
          26742,
          365,
          264,
          1412,
          321,
          434,
          1237,
          337,
          4079,
          294,
          456,
          13,
          50820
        ]
      },
      {
        "avg_logprob": -0.20565773292824074,
        "compression_ratio": 1.7473684210526317,
        "end": 126.08,
        "id": 53,
        "no_speech_prob": 0.0000035008454233320663,
        "seek": 11508,
        "start": 124.2,
        "temperature": 0,
        "text": " I mean, it could be an error, or other things",
        "tokens": [
          50820,
          286,
          914,
          11,
          309,
          727,
          312,
          364,
          6713,
          11,
          420,
          661,
          721,
          50914
        ]
      },
      {
        "avg_logprob": -0.20565773292824074,
        "compression_ratio": 1.7473684210526317,
        "end": 127.88,
        "id": 54,
        "no_speech_prob": 0.0000035008454233320663,
        "seek": 11508,
        "start": 126.08,
        "temperature": 0,
        "text": " could come back as part of that response.",
        "tokens": [
          50914,
          727,
          808,
          646,
          382,
          644,
          295,
          300,
          4134,
          13,
          51004
        ]
      },
      {
        "avg_logprob": -0.20565773292824074,
        "compression_ratio": 1.7473684210526317,
        "end": 130.12,
        "id": 55,
        "no_speech_prob": 0.0000035008454233320663,
        "seek": 11508,
        "start": 127.88,
        "temperature": 0,
        "text": " But I'm just going to write down response.",
        "tokens": [
          51004,
          583,
          286,
          478,
          445,
          516,
          281,
          2464,
          760,
          4134,
          13,
          51116
        ]
      },
      {
        "avg_logprob": -0.20565773292824074,
        "compression_ratio": 1.7473684210526317,
        "end": 133.92,
        "id": 56,
        "no_speech_prob": 0.0000035008454233320663,
        "seek": 11508,
        "start": 130.12,
        "temperature": 0,
        "text": " Now, this involves this idea of a promise.",
        "tokens": [
          51116,
          823,
          11,
          341,
          11626,
          341,
          1558,
          295,
          257,
          6228,
          13,
          51306
        ]
      },
      {
        "avg_logprob": -0.20565773292824074,
        "compression_ratio": 1.7473684210526317,
        "end": 136.07999999999998,
        "id": 57,
        "no_speech_prob": 0.0000035008454233320663,
        "seek": 11508,
        "start": 133.92,
        "temperature": 0,
        "text": " And the Fetch function is a function",
        "tokens": [
          51306,
          400,
          264,
          479,
          7858,
          2445,
          307,
          257,
          2445,
          51414
        ]
      },
      {
        "avg_logprob": -0.20565773292824074,
        "compression_ratio": 1.7473684210526317,
        "end": 139.48,
        "id": 58,
        "no_speech_prob": 0.0000035008454233320663,
        "seek": 11508,
        "start": 136.07999999999998,
        "temperature": 0,
        "text": " that happens asynchronously, meaning we call Fetch,",
        "tokens": [
          51414,
          300,
          2314,
          42642,
          5098,
          11,
          3620,
          321,
          818,
          479,
          7858,
          11,
          51584
        ]
      },
      {
        "avg_logprob": -0.20565773292824074,
        "compression_ratio": 1.7473684210526317,
        "end": 142.12,
        "id": 59,
        "no_speech_prob": 0.0000035008454233320663,
        "seek": 11508,
        "start": 139.48,
        "temperature": 0,
        "text": " but some time passes because it takes some time",
        "tokens": [
          51584,
          457,
          512,
          565,
          11335,
          570,
          309,
          2516,
          512,
          565,
          51716
        ]
      },
      {
        "avg_logprob": -0.20565773292824074,
        "compression_ratio": 1.7473684210526317,
        "end": 143.72,
        "id": 60,
        "no_speech_prob": 0.0000035008454233320663,
        "seek": 11508,
        "start": 142.12,
        "temperature": 0,
        "text": " to retrieve that data.",
        "tokens": [
          51716,
          281,
          30254,
          300,
          1412,
          13,
          51796
        ]
      },
      {
        "avg_logprob": -0.19678158569335938,
        "compression_ratio": 1.7622641509433963,
        "end": 147.56,
        "id": 61,
        "no_speech_prob": 0.00009314559429185465,
        "seek": 14372,
        "start": 143.72,
        "temperature": 0,
        "text": " So how we get the response as part of a promise",
        "tokens": [
          50364,
          407,
          577,
          321,
          483,
          264,
          4134,
          382,
          644,
          295,
          257,
          6228,
          50556
        ]
      },
      {
        "avg_logprob": -0.19678158569335938,
        "compression_ratio": 1.7622641509433963,
        "end": 149.84,
        "id": 62,
        "no_speech_prob": 0.00009314559429185465,
        "seek": 14372,
        "start": 147.56,
        "temperature": 0,
        "text": " is a detail that I'm going to have",
        "tokens": [
          50556,
          307,
          257,
          2607,
          300,
          286,
          478,
          516,
          281,
          362,
          50670
        ]
      },
      {
        "avg_logprob": -0.19678158569335938,
        "compression_ratio": 1.7622641509433963,
        "end": 152.44,
        "id": 63,
        "no_speech_prob": 0.00009314559429185465,
        "seek": 14372,
        "start": 149.84,
        "temperature": 0,
        "text": " to look at specifically when we get into the code.",
        "tokens": [
          50670,
          281,
          574,
          412,
          4682,
          562,
          321,
          483,
          666,
          264,
          3089,
          13,
          50800
        ]
      },
      {
        "avg_logprob": -0.19678158569335938,
        "compression_ratio": 1.7622641509433963,
        "end": 155.28,
        "id": 64,
        "no_speech_prob": 0.00009314559429185465,
        "seek": 14372,
        "start": 152.44,
        "temperature": 0,
        "text": " And I will also include in this video's description",
        "tokens": [
          50800,
          400,
          286,
          486,
          611,
          4090,
          294,
          341,
          960,
          311,
          3855,
          50942
        ]
      },
      {
        "avg_logprob": -0.19678158569335938,
        "compression_ratio": 1.7622641509433963,
        "end": 158.04,
        "id": 65,
        "no_speech_prob": 0.00009314559429185465,
        "seek": 14372,
        "start": 155.28,
        "temperature": 0,
        "text": " some links to further details about how",
        "tokens": [
          50942,
          512,
          6123,
          281,
          3052,
          4365,
          466,
          577,
          51080
        ]
      },
      {
        "avg_logprob": -0.19678158569335938,
        "compression_ratio": 1.7622641509433963,
        "end": 159.48,
        "id": 66,
        "no_speech_prob": 0.00009314559429185465,
        "seek": 14372,
        "start": 158.04,
        "temperature": 0,
        "text": " to work with JavaScript promises,",
        "tokens": [
          51080,
          281,
          589,
          365,
          15778,
          16403,
          11,
          51152
        ]
      },
      {
        "avg_logprob": -0.19678158569335938,
        "compression_ratio": 1.7622641509433963,
        "end": 161.24,
        "id": 67,
        "no_speech_prob": 0.00009314559429185465,
        "seek": 14372,
        "start": 159.48,
        "temperature": 0,
        "text": " some other videos that I've made that might",
        "tokens": [
          51152,
          512,
          661,
          2145,
          300,
          286,
          600,
          1027,
          300,
          1062,
          51240
        ]
      },
      {
        "avg_logprob": -0.19678158569335938,
        "compression_ratio": 1.7622641509433963,
        "end": 162.92,
        "id": 68,
        "no_speech_prob": 0.00009314559429185465,
        "seek": 14372,
        "start": 161.24,
        "temperature": 0,
        "text": " fill in some of the gaps here.",
        "tokens": [
          51240,
          2836,
          294,
          512,
          295,
          264,
          15031,
          510,
          13,
          51324
        ]
      },
      {
        "avg_logprob": -0.19678158569335938,
        "compression_ratio": 1.7622641509433963,
        "end": 166.4,
        "id": 69,
        "no_speech_prob": 0.00009314559429185465,
        "seek": 14372,
        "start": 162.92,
        "temperature": 0,
        "text": " The response is actually a stream of data.",
        "tokens": [
          51324,
          440,
          4134,
          307,
          767,
          257,
          4309,
          295,
          1412,
          13,
          51498
        ]
      },
      {
        "avg_logprob": -0.19678158569335938,
        "compression_ratio": 1.7622641509433963,
        "end": 170.32,
        "id": 70,
        "no_speech_prob": 0.00009314559429185465,
        "seek": 14372,
        "start": 166.4,
        "temperature": 0,
        "text": " So we get the response, but we need a whole other step",
        "tokens": [
          51498,
          407,
          321,
          483,
          264,
          4134,
          11,
          457,
          321,
          643,
          257,
          1379,
          661,
          1823,
          51694
        ]
      },
      {
        "avg_logprob": -0.19678158569335938,
        "compression_ratio": 1.7622641509433963,
        "end": 172.36,
        "id": 71,
        "no_speech_prob": 0.00009314559429185465,
        "seek": 14372,
        "start": 170.32,
        "temperature": 0,
        "text": " because we need to read that data.",
        "tokens": [
          51694,
          570,
          321,
          643,
          281,
          1401,
          300,
          1412,
          13,
          51796
        ]
      },
      {
        "avg_logprob": -0.20186967719091128,
        "compression_ratio": 1.7508896797153024,
        "end": 175.36,
        "id": 72,
        "no_speech_prob": 0.000791617261711508,
        "seek": 17236,
        "start": 172.36,
        "temperature": 0,
        "text": " We need to complete that data and store it in a format",
        "tokens": [
          50364,
          492,
          643,
          281,
          3566,
          300,
          1412,
          293,
          3531,
          309,
          294,
          257,
          7877,
          50514
        ]
      },
      {
        "avg_logprob": -0.20186967719091128,
        "compression_ratio": 1.7508896797153024,
        "end": 176.56,
        "id": 73,
        "no_speech_prob": 0.000791617261711508,
        "seek": 17236,
        "start": 175.36,
        "temperature": 0,
        "text": " that we can work with.",
        "tokens": [
          50514,
          300,
          321,
          393,
          589,
          365,
          13,
          50574
        ]
      },
      {
        "avg_logprob": -0.20186967719091128,
        "compression_ratio": 1.7508896797153024,
        "end": 178.48000000000002,
        "id": 74,
        "no_speech_prob": 0.000791617261711508,
        "seek": 17236,
        "start": 176.56,
        "temperature": 0,
        "text": " And the kinds of formats we might have is like,",
        "tokens": [
          50574,
          400,
          264,
          3685,
          295,
          25879,
          321,
          1062,
          362,
          307,
          411,
          11,
          50670
        ]
      },
      {
        "avg_logprob": -0.20186967719091128,
        "compression_ratio": 1.7508896797153024,
        "end": 182,
        "id": 75,
        "no_speech_prob": 0.000791617261711508,
        "seek": 17236,
        "start": 178.48000000000002,
        "temperature": 0,
        "text": " oh, it's text data, or it's a blob.",
        "tokens": [
          50670,
          1954,
          11,
          309,
          311,
          2487,
          1412,
          11,
          420,
          309,
          311,
          257,
          46115,
          13,
          50846
        ]
      },
      {
        "avg_logprob": -0.20186967719091128,
        "compression_ratio": 1.7508896797153024,
        "end": 184.88000000000002,
        "id": 76,
        "no_speech_prob": 0.000791617261711508,
        "seek": 17236,
        "start": 182,
        "temperature": 0,
        "text": " Maybe it's image data would come in as a blob.",
        "tokens": [
          50846,
          2704,
          309,
          311,
          3256,
          1412,
          576,
          808,
          294,
          382,
          257,
          46115,
          13,
          50990
        ]
      },
      {
        "avg_logprob": -0.20186967719091128,
        "compression_ratio": 1.7508896797153024,
        "end": 186.96,
        "id": 77,
        "no_speech_prob": 0.000791617261711508,
        "seek": 17236,
        "start": 184.88000000000002,
        "temperature": 0,
        "text": " There's all sorts of there's array buffer,",
        "tokens": [
          50990,
          821,
          311,
          439,
          7527,
          295,
          456,
          311,
          10225,
          21762,
          11,
          51094
        ]
      },
      {
        "avg_logprob": -0.20186967719091128,
        "compression_ratio": 1.7508896797153024,
        "end": 189.96,
        "id": 78,
        "no_speech_prob": 0.000791617261711508,
        "seek": 17236,
        "start": 186.96,
        "temperature": 0,
        "text": " and a really, really important one is JSON.",
        "tokens": [
          51094,
          293,
          257,
          534,
          11,
          534,
          1021,
          472,
          307,
          31828,
          13,
          51244
        ]
      },
      {
        "avg_logprob": -0.20186967719091128,
        "compression_ratio": 1.7508896797153024,
        "end": 193.24,
        "id": 79,
        "no_speech_prob": 0.000791617261711508,
        "seek": 17236,
        "start": 189.96,
        "temperature": 0,
        "text": " So JSON stands for JavaScript Object Notation.",
        "tokens": [
          51244,
          407,
          31828,
          7382,
          337,
          15778,
          24753,
          1726,
          399,
          13,
          51408
        ]
      },
      {
        "avg_logprob": -0.20186967719091128,
        "compression_ratio": 1.7508896797153024,
        "end": 195.60000000000002,
        "id": 80,
        "no_speech_prob": 0.000791617261711508,
        "seek": 17236,
        "start": 193.24,
        "temperature": 0,
        "text": " And this is going to be a format for storing data",
        "tokens": [
          51408,
          400,
          341,
          307,
          516,
          281,
          312,
          257,
          7877,
          337,
          26085,
          1412,
          51526
        ]
      },
      {
        "avg_logprob": -0.20186967719091128,
        "compression_ratio": 1.7508896797153024,
        "end": 198.20000000000002,
        "id": 81,
        "no_speech_prob": 0.000791617261711508,
        "seek": 17236,
        "start": 195.60000000000002,
        "temperature": 0,
        "text": " that we're going to see again, and again, and again.",
        "tokens": [
          51526,
          300,
          321,
          434,
          516,
          281,
          536,
          797,
          11,
          293,
          797,
          11,
          293,
          797,
          13,
          51656
        ]
      },
      {
        "avg_logprob": -0.20186967719091128,
        "compression_ratio": 1.7508896797153024,
        "end": 201.42000000000002,
        "id": 82,
        "no_speech_prob": 0.000791617261711508,
        "seek": 17236,
        "start": 198.20000000000002,
        "temperature": 0,
        "text": " But in this case, we want to get it as a blob.",
        "tokens": [
          51656,
          583,
          294,
          341,
          1389,
          11,
          321,
          528,
          281,
          483,
          309,
          382,
          257,
          46115,
          13,
          51817
        ]
      },
      {
        "avg_logprob": -0.22747527412746263,
        "compression_ratio": 1.8577235772357723,
        "end": 206.85999999999999,
        "id": 83,
        "no_speech_prob": 0.0000028573044801305514,
        "seek": 20142,
        "start": 201.42,
        "temperature": 0,
        "text": " So I think we could call this third step complete data",
        "tokens": [
          50364,
          407,
          286,
          519,
          321,
          727,
          818,
          341,
          2636,
          1823,
          3566,
          1412,
          50636
        ]
      },
      {
        "avg_logprob": -0.22747527412746263,
        "compression_ratio": 1.8577235772357723,
        "end": 208.01999999999998,
        "id": 84,
        "no_speech_prob": 0.0000028573044801305514,
        "seek": 20142,
        "start": 206.85999999999999,
        "temperature": 0,
        "text": " stream.",
        "tokens": [
          50636,
          4309,
          13,
          50694
        ]
      },
      {
        "avg_logprob": -0.22747527412746263,
        "compression_ratio": 1.8577235772357723,
        "end": 209.57999999999998,
        "id": 85,
        "no_speech_prob": 0.0000028573044801305514,
        "seek": 20142,
        "start": 208.01999999999998,
        "temperature": 0,
        "text": " And when I say complete data stream,",
        "tokens": [
          50694,
          400,
          562,
          286,
          584,
          3566,
          1412,
          4309,
          11,
          50772
        ]
      },
      {
        "avg_logprob": -0.22747527412746263,
        "compression_ratio": 1.8577235772357723,
        "end": 212.7,
        "id": 86,
        "no_speech_prob": 0.0000028573044801305514,
        "seek": 20142,
        "start": 209.57999999999998,
        "temperature": 0,
        "text": " I really mean complete and grab the data that's",
        "tokens": [
          50772,
          286,
          534,
          914,
          3566,
          293,
          4444,
          264,
          1412,
          300,
          311,
          50928
        ]
      },
      {
        "avg_logprob": -0.22747527412746263,
        "compression_ratio": 1.8577235772357723,
        "end": 214.7,
        "id": 87,
        "no_speech_prob": 0.0000028573044801305514,
        "seek": 20142,
        "start": 212.7,
        "temperature": 0,
        "text": " in the body of the response.",
        "tokens": [
          50928,
          294,
          264,
          1772,
          295,
          264,
          4134,
          13,
          51028
        ]
      },
      {
        "avg_logprob": -0.22747527412746263,
        "compression_ratio": 1.8577235772357723,
        "end": 217.01999999999998,
        "id": 88,
        "no_speech_prob": 0.0000028573044801305514,
        "seek": 20142,
        "start": 214.7,
        "temperature": 0,
        "text": " So there's this concept of the response body,",
        "tokens": [
          51028,
          407,
          456,
          311,
          341,
          3410,
          295,
          264,
          4134,
          1772,
          11,
          51144
        ]
      },
      {
        "avg_logprob": -0.22747527412746263,
        "compression_ratio": 1.8577235772357723,
        "end": 220.38,
        "id": 89,
        "no_speech_prob": 0.0000028573044801305514,
        "seek": 20142,
        "start": 217.01999999999998,
        "temperature": 0,
        "text": " which is pretty important, which is where the data actually is.",
        "tokens": [
          51144,
          597,
          307,
          1238,
          1021,
          11,
          597,
          307,
          689,
          264,
          1412,
          767,
          307,
          13,
          51312
        ]
      },
      {
        "avg_logprob": -0.22747527412746263,
        "compression_ratio": 1.8577235772357723,
        "end": 223.1,
        "id": 90,
        "no_speech_prob": 0.0000028573044801305514,
        "seek": 20142,
        "start": 220.38,
        "temperature": 0,
        "text": " But it should be noted that there's lots of other metadata",
        "tokens": [
          51312,
          583,
          309,
          820,
          312,
          12964,
          300,
          456,
          311,
          3195,
          295,
          661,
          26603,
          51448
        ]
      },
      {
        "avg_logprob": -0.22747527412746263,
        "compression_ratio": 1.8577235772357723,
        "end": 226.61999999999998,
        "id": 91,
        "no_speech_prob": 0.0000028573044801305514,
        "seek": 20142,
        "start": 223.1,
        "temperature": 0,
        "text": " about the network communication that's in the response",
        "tokens": [
          51448,
          466,
          264,
          3209,
          6101,
          300,
          311,
          294,
          264,
          4134,
          51624
        ]
      },
      {
        "avg_logprob": -0.22747527412746263,
        "compression_ratio": 1.8577235772357723,
        "end": 229.98,
        "id": 92,
        "no_speech_prob": 0.0000028573044801305514,
        "seek": 20142,
        "start": 226.61999999999998,
        "temperature": 0,
        "text": " as well that you could look at in certain circumstances.",
        "tokens": [
          51624,
          382,
          731,
          300,
          291,
          727,
          574,
          412,
          294,
          1629,
          9121,
          13,
          51792
        ]
      },
      {
        "avg_logprob": -0.2296166737874349,
        "compression_ratio": 1.7295081967213115,
        "end": 232.54,
        "id": 93,
        "no_speech_prob": 0.000011300760888843797,
        "seek": 22998,
        "start": 229.98,
        "temperature": 0,
        "text": " So once we've done that, we've completed the data stream,",
        "tokens": [
          50364,
          407,
          1564,
          321,
          600,
          1096,
          300,
          11,
          321,
          600,
          7365,
          264,
          1412,
          4309,
          11,
          50492
        ]
      },
      {
        "avg_logprob": -0.2296166737874349,
        "compression_ratio": 1.7295081967213115,
        "end": 236.38,
        "id": 94,
        "no_speech_prob": 0.000011300760888843797,
        "seek": 22998,
        "start": 232.54,
        "temperature": 0,
        "text": " we've got the image blob, and then we can just,",
        "tokens": [
          50492,
          321,
          600,
          658,
          264,
          3256,
          46115,
          11,
          293,
          550,
          321,
          393,
          445,
          11,
          50684
        ]
      },
      {
        "avg_logprob": -0.2296166737874349,
        "compression_ratio": 1.7295081967213115,
        "end": 238.22,
        "id": 95,
        "no_speech_prob": 0.000011300760888843797,
        "seek": 22998,
        "start": 236.38,
        "temperature": 0,
        "text": " this is really the steps for using fetch.",
        "tokens": [
          50684,
          341,
          307,
          534,
          264,
          4439,
          337,
          1228,
          23673,
          13,
          50776
        ]
      },
      {
        "avg_logprob": -0.2296166737874349,
        "compression_ratio": 1.7295081967213115,
        "end": 240.32,
        "id": 96,
        "no_speech_prob": 0.000011300760888843797,
        "seek": 22998,
        "start": 238.22,
        "temperature": 0,
        "text": " But in this particular scenario, I",
        "tokens": [
          50776,
          583,
          294,
          341,
          1729,
          9005,
          11,
          286,
          50881
        ]
      },
      {
        "avg_logprob": -0.2296166737874349,
        "compression_ratio": 1.7295081967213115,
        "end": 248.82,
        "id": 97,
        "no_speech_prob": 0.000011300760888843797,
        "seek": 22998,
        "start": 240.32,
        "temperature": 0,
        "text": " want to make an image element, make an IMG, HTML DOM element,",
        "tokens": [
          50881,
          528,
          281,
          652,
          364,
          3256,
          4478,
          11,
          652,
          364,
          21463,
          38,
          11,
          17995,
          35727,
          4478,
          11,
          51306
        ]
      },
      {
        "avg_logprob": -0.2296166737874349,
        "compression_ratio": 1.7295081967213115,
        "end": 252.82,
        "id": 98,
        "no_speech_prob": 0.000011300760888843797,
        "seek": 22998,
        "start": 248.82,
        "temperature": 0,
        "text": " make an image element with that data.",
        "tokens": [
          51306,
          652,
          364,
          3256,
          4478,
          365,
          300,
          1412,
          13,
          51506
        ]
      },
      {
        "avg_logprob": -0.2296166737874349,
        "compression_ratio": 1.7295081967213115,
        "end": 255.06,
        "id": 99,
        "no_speech_prob": 0.000011300760888843797,
        "seek": 22998,
        "start": 252.82,
        "temperature": 0,
        "text": " So these are the steps of this example,",
        "tokens": [
          51506,
          407,
          613,
          366,
          264,
          4439,
          295,
          341,
          1365,
          11,
          51618
        ]
      },
      {
        "avg_logprob": -0.2296166737874349,
        "compression_ratio": 1.7295081967213115,
        "end": 256.96,
        "id": 100,
        "no_speech_prob": 0.000011300760888843797,
        "seek": 22998,
        "start": 255.06,
        "temperature": 0,
        "text": " and I think we can just go code this now.",
        "tokens": [
          51618,
          293,
          286,
          519,
          321,
          393,
          445,
          352,
          3089,
          341,
          586,
          13,
          51713
        ]
      },
      {
        "avg_logprob": -0.2296166737874349,
        "compression_ratio": 1.7295081967213115,
        "end": 259.78,
        "id": 101,
        "no_speech_prob": 0.000011300760888843797,
        "seek": 22998,
        "start": 256.96,
        "temperature": 0,
        "text": " So what I'm beginning with is just some boilerplate HTML.",
        "tokens": [
          51713,
          407,
          437,
          286,
          478,
          2863,
          365,
          307,
          445,
          512,
          39228,
          37008,
          17995,
          13,
          51854
        ]
      },
      {
        "avg_logprob": -0.2410545861160995,
        "compression_ratio": 1.726962457337884,
        "end": 262.41999999999996,
        "id": 102,
        "no_speech_prob": 0.00012931437231600285,
        "seek": 25978,
        "start": 259.78,
        "temperature": 0,
        "text": " Basic HTML file with nothing in it.",
        "tokens": [
          50364,
          31598,
          17995,
          3991,
          365,
          1825,
          294,
          309,
          13,
          50496
        ]
      },
      {
        "avg_logprob": -0.2410545861160995,
        "compression_ratio": 1.726962457337884,
        "end": 264.05999999999995,
        "id": 103,
        "no_speech_prob": 0.00012931437231600285,
        "seek": 25978,
        "start": 262.41999999999996,
        "temperature": 0,
        "text": " And so let's add some stuff to it.",
        "tokens": [
          50496,
          400,
          370,
          718,
          311,
          909,
          512,
          1507,
          281,
          309,
          13,
          50578
        ]
      },
      {
        "avg_logprob": -0.2410545861160995,
        "compression_ratio": 1.726962457337884,
        "end": 268.29999999999995,
        "id": 104,
        "no_speech_prob": 0.00012931437231600285,
        "seek": 25978,
        "start": 264.05999999999995,
        "temperature": 0,
        "text": " So first, let me add an image element to the body.",
        "tokens": [
          50578,
          407,
          700,
          11,
          718,
          385,
          909,
          364,
          3256,
          4478,
          281,
          264,
          1772,
          13,
          50790
        ]
      },
      {
        "avg_logprob": -0.2410545861160995,
        "compression_ratio": 1.726962457337884,
        "end": 271.14,
        "id": 105,
        "no_speech_prob": 0.00012931437231600285,
        "seek": 25978,
        "start": 268.29999999999995,
        "temperature": 0,
        "text": " And I'm going to leave the source of the image element",
        "tokens": [
          50790,
          400,
          286,
          478,
          516,
          281,
          1856,
          264,
          4009,
          295,
          264,
          3256,
          4478,
          50932
        ]
      },
      {
        "avg_logprob": -0.2410545861160995,
        "compression_ratio": 1.726962457337884,
        "end": 272.46,
        "id": 106,
        "no_speech_prob": 0.00012931437231600285,
        "seek": 25978,
        "start": 271.14,
        "temperature": 0,
        "text": " blank.",
        "tokens": [
          50932,
          8247,
          13,
          50998
        ]
      },
      {
        "avg_logprob": -0.2410545861160995,
        "compression_ratio": 1.726962457337884,
        "end": 274.02,
        "id": 107,
        "no_speech_prob": 0.00012931437231600285,
        "seek": 25978,
        "start": 272.46,
        "temperature": 0,
        "text": " But I'm going to give it an ID, and I'm",
        "tokens": [
          50998,
          583,
          286,
          478,
          516,
          281,
          976,
          309,
          364,
          7348,
          11,
          293,
          286,
          478,
          51076
        ]
      },
      {
        "avg_logprob": -0.2410545861160995,
        "compression_ratio": 1.726962457337884,
        "end": 275.58,
        "id": 108,
        "no_speech_prob": 0.00012931437231600285,
        "seek": 25978,
        "start": 274.02,
        "temperature": 0,
        "text": " going to call that rainbow.",
        "tokens": [
          51076,
          516,
          281,
          818,
          300,
          18526,
          13,
          51154
        ]
      },
      {
        "avg_logprob": -0.2410545861160995,
        "compression_ratio": 1.726962457337884,
        "end": 277.9,
        "id": 109,
        "no_speech_prob": 0.00012931437231600285,
        "seek": 25978,
        "start": 275.58,
        "temperature": 0,
        "text": " The next thing I want to do is just add a script tag",
        "tokens": [
          51154,
          440,
          958,
          551,
          286,
          528,
          281,
          360,
          307,
          445,
          909,
          257,
          5755,
          6162,
          51270
        ]
      },
      {
        "avg_logprob": -0.2410545861160995,
        "compression_ratio": 1.726962457337884,
        "end": 280,
        "id": 110,
        "no_speech_prob": 0.00012931437231600285,
        "seek": 25978,
        "start": 277.9,
        "temperature": 0,
        "text": " so I can put some JavaScript with, presumably,",
        "tokens": [
          51270,
          370,
          286,
          393,
          829,
          512,
          15778,
          365,
          11,
          26742,
          11,
          51375
        ]
      },
      {
        "avg_logprob": -0.2410545861160995,
        "compression_ratio": 1.726962457337884,
        "end": 281.38,
        "id": 111,
        "no_speech_prob": 0.00012931437231600285,
        "seek": 25978,
        "start": 280,
        "temperature": 0,
        "text": " that fetch function.",
        "tokens": [
          51375,
          300,
          23673,
          2445,
          13,
          51444
        ]
      },
      {
        "avg_logprob": -0.2410545861160995,
        "compression_ratio": 1.726962457337884,
        "end": 285.14,
        "id": 112,
        "no_speech_prob": 0.00012931437231600285,
        "seek": 25978,
        "start": 281.38,
        "temperature": 0,
        "text": " Now, ultimately, I might want to put my JavaScript",
        "tokens": [
          51444,
          823,
          11,
          6284,
          11,
          286,
          1062,
          528,
          281,
          829,
          452,
          15778,
          51632
        ]
      },
      {
        "avg_logprob": -0.2410545861160995,
        "compression_ratio": 1.726962457337884,
        "end": 286.46,
        "id": 113,
        "no_speech_prob": 0.00012931437231600285,
        "seek": 25978,
        "start": 285.14,
        "temperature": 0,
        "text": " code in a separate file.",
        "tokens": [
          51632,
          3089,
          294,
          257,
          4994,
          3991,
          13,
          51698
        ]
      },
      {
        "avg_logprob": -0.2410545861160995,
        "compression_ratio": 1.726962457337884,
        "end": 288.9,
        "id": 114,
        "no_speech_prob": 0.00012931437231600285,
        "seek": 25978,
        "start": 286.46,
        "temperature": 0,
        "text": " I might want to have a whole build system for my project,",
        "tokens": [
          51698,
          286,
          1062,
          528,
          281,
          362,
          257,
          1379,
          1322,
          1185,
          337,
          452,
          1716,
          11,
          51820
        ]
      },
      {
        "avg_logprob": -0.22881147066752117,
        "compression_ratio": 1.7316017316017316,
        "end": 290.9,
        "id": 115,
        "no_speech_prob": 0.0002492308849468827,
        "seek": 28890,
        "start": 288.9,
        "temperature": 0,
        "text": " but we're just working with the basic ideas here.",
        "tokens": [
          50364,
          457,
          321,
          434,
          445,
          1364,
          365,
          264,
          3875,
          3487,
          510,
          13,
          50464
        ]
      },
      {
        "avg_logprob": -0.22881147066752117,
        "compression_ratio": 1.7316017316017316,
        "end": 292.85999999999996,
        "id": 116,
        "no_speech_prob": 0.0002492308849468827,
        "seek": 28890,
        "start": 290.9,
        "temperature": 0,
        "text": " So I'm going to just do everything in one file,",
        "tokens": [
          50464,
          407,
          286,
          478,
          516,
          281,
          445,
          360,
          1203,
          294,
          472,
          3991,
          11,
          50562
        ]
      },
      {
        "avg_logprob": -0.22881147066752117,
        "compression_ratio": 1.7316017316017316,
        "end": 295.97999999999996,
        "id": 117,
        "no_speech_prob": 0.0002492308849468827,
        "seek": 28890,
        "start": 292.85999999999996,
        "temperature": 0,
        "text": " one HTML file that has the HTML stuff and the JavaScript",
        "tokens": [
          50562,
          472,
          17995,
          3991,
          300,
          575,
          264,
          17995,
          1507,
          293,
          264,
          15778,
          50718
        ]
      },
      {
        "avg_logprob": -0.22881147066752117,
        "compression_ratio": 1.7316017316017316,
        "end": 297.29999999999995,
        "id": 118,
        "no_speech_prob": 0.0002492308849468827,
        "seek": 28890,
        "start": 295.97999999999996,
        "temperature": 0,
        "text": " stuff in a script tag.",
        "tokens": [
          50718,
          1507,
          294,
          257,
          5755,
          6162,
          13,
          50784
        ]
      },
      {
        "avg_logprob": -0.22881147066752117,
        "compression_ratio": 1.7316017316017316,
        "end": 299.82,
        "id": 119,
        "no_speech_prob": 0.0002492308849468827,
        "seek": 28890,
        "start": 297.29999999999995,
        "temperature": 0,
        "text": " And I'm going to say fetch, and I'm",
        "tokens": [
          50784,
          400,
          286,
          478,
          516,
          281,
          584,
          23673,
          11,
          293,
          286,
          478,
          50910
        ]
      },
      {
        "avg_logprob": -0.22881147066752117,
        "compression_ratio": 1.7316017316017316,
        "end": 302.29999999999995,
        "id": 120,
        "no_speech_prob": 0.0002492308849468827,
        "seek": 28890,
        "start": 299.82,
        "temperature": 0,
        "text": " going to fetch rainbow.jpg.",
        "tokens": [
          50910,
          516,
          281,
          23673,
          18526,
          13,
          73,
          49861,
          13,
          51034
        ]
      },
      {
        "avg_logprob": -0.22881147066752117,
        "compression_ratio": 1.7316017316017316,
        "end": 303.73999999999995,
        "id": 121,
        "no_speech_prob": 0.0002492308849468827,
        "seek": 28890,
        "start": 302.29999999999995,
        "temperature": 0,
        "text": " I'm going to put that in the code,",
        "tokens": [
          51034,
          286,
          478,
          516,
          281,
          829,
          300,
          294,
          264,
          3089,
          11,
          51106
        ]
      },
      {
        "avg_logprob": -0.22881147066752117,
        "compression_ratio": 1.7316017316017316,
        "end": 308.14,
        "id": 122,
        "no_speech_prob": 0.0002492308849468827,
        "seek": 28890,
        "start": 303.73999999999995,
        "temperature": 0,
        "text": " and I'm also going to add console.log",
        "tokens": [
          51106,
          293,
          286,
          478,
          611,
          516,
          281,
          909,
          11076,
          13,
          4987,
          51326
        ]
      },
      {
        "avg_logprob": -0.22881147066752117,
        "compression_ratio": 1.7316017316017316,
        "end": 312.38,
        "id": 123,
        "no_speech_prob": 0.0002492308849468827,
        "seek": 28890,
        "start": 308.14,
        "temperature": 0,
        "text": " about to fetch a rainbow, because why not?",
        "tokens": [
          51326,
          466,
          281,
          23673,
          257,
          18526,
          11,
          570,
          983,
          406,
          30,
          51538
        ]
      },
      {
        "avg_logprob": -0.22881147066752117,
        "compression_ratio": 1.7316017316017316,
        "end": 315.44,
        "id": 124,
        "no_speech_prob": 0.0002492308849468827,
        "seek": 28890,
        "start": 312.38,
        "temperature": 0,
        "text": " So I'm using something called live server.",
        "tokens": [
          51538,
          407,
          286,
          478,
          1228,
          746,
          1219,
          1621,
          7154,
          13,
          51691
        ]
      },
      {
        "avg_logprob": -0.20753550347481065,
        "compression_ratio": 1.6758620689655173,
        "end": 319.8,
        "id": 125,
        "no_speech_prob": 0.013020126149058342,
        "seek": 31544,
        "start": 315.44,
        "temperature": 0,
        "text": " It's a node package to host this particular HTML",
        "tokens": [
          50364,
          467,
          311,
          257,
          9984,
          7372,
          281,
          3975,
          341,
          1729,
          17995,
          50582
        ]
      },
      {
        "avg_logprob": -0.20753550347481065,
        "compression_ratio": 1.6758620689655173,
        "end": 320.84,
        "id": 126,
        "no_speech_prob": 0.013020126149058342,
        "seek": 31544,
        "start": 319.8,
        "temperature": 0,
        "text": " page on my computer.",
        "tokens": [
          50582,
          3028,
          322,
          452,
          3820,
          13,
          50634
        ]
      },
      {
        "avg_logprob": -0.20753550347481065,
        "compression_ratio": 1.6758620689655173,
        "end": 323.71999999999997,
        "id": 127,
        "no_speech_prob": 0.013020126149058342,
        "seek": 31544,
        "start": 320.84,
        "temperature": 0,
        "text": " So every time that I hit Save, it updates in the browser,",
        "tokens": [
          50634,
          407,
          633,
          565,
          300,
          286,
          2045,
          15541,
          11,
          309,
          9205,
          294,
          264,
          11185,
          11,
          50778
        ]
      },
      {
        "avg_logprob": -0.20753550347481065,
        "compression_ratio": 1.6758620689655173,
        "end": 326.32,
        "id": 128,
        "no_speech_prob": 0.013020126149058342,
        "seek": 31544,
        "start": 323.71999999999997,
        "temperature": 0,
        "text": " and you can see that now I have that console log there.",
        "tokens": [
          50778,
          293,
          291,
          393,
          536,
          300,
          586,
          286,
          362,
          300,
          11076,
          3565,
          456,
          13,
          50908
        ]
      },
      {
        "avg_logprob": -0.20753550347481065,
        "compression_ratio": 1.6758620689655173,
        "end": 328.6,
        "id": 129,
        "no_speech_prob": 0.013020126149058342,
        "seek": 31544,
        "start": 326.32,
        "temperature": 0,
        "text": " Now, I said earlier that the fetch function",
        "tokens": [
          50908,
          823,
          11,
          286,
          848,
          3071,
          300,
          264,
          23673,
          2445,
          51022
        ]
      },
      {
        "avg_logprob": -0.20753550347481065,
        "compression_ratio": 1.6758620689655173,
        "end": 331.4,
        "id": 130,
        "no_speech_prob": 0.013020126149058342,
        "seek": 31544,
        "start": 328.6,
        "temperature": 0,
        "text": " returns a promise, and a promise is a way in JavaScript",
        "tokens": [
          51022,
          11247,
          257,
          6228,
          11,
          293,
          257,
          6228,
          307,
          257,
          636,
          294,
          15778,
          51162
        ]
      },
      {
        "avg_logprob": -0.20753550347481065,
        "compression_ratio": 1.6758620689655173,
        "end": 333.08,
        "id": 131,
        "no_speech_prob": 0.013020126149058342,
        "seek": 31544,
        "start": 331.4,
        "temperature": 0,
        "text": " to handle an asynchronous event.",
        "tokens": [
          51162,
          281,
          4813,
          364,
          49174,
          2280,
          13,
          51246
        ]
      },
      {
        "avg_logprob": -0.20753550347481065,
        "compression_ratio": 1.6758620689655173,
        "end": 336.08,
        "id": 132,
        "no_speech_prob": 0.013020126149058342,
        "seek": 31544,
        "start": 333.08,
        "temperature": 0,
        "text": " It gets resolved when the event is over,",
        "tokens": [
          51246,
          467,
          2170,
          20772,
          562,
          264,
          2280,
          307,
          670,
          11,
          51396
        ]
      },
      {
        "avg_logprob": -0.20753550347481065,
        "compression_ratio": 1.6758620689655173,
        "end": 337.76,
        "id": 133,
        "no_speech_prob": 0.013020126149058342,
        "seek": 31544,
        "start": 336.08,
        "temperature": 0,
        "text": " when the data is retrieved.",
        "tokens": [
          51396,
          562,
          264,
          1412,
          307,
          19817,
          937,
          13,
          51480
        ]
      },
      {
        "avg_logprob": -0.20753550347481065,
        "compression_ratio": 1.6758620689655173,
        "end": 339.92,
        "id": 134,
        "no_speech_prob": 0.013020126149058342,
        "seek": 31544,
        "start": 337.76,
        "temperature": 0,
        "text": " And I will refer you to some other videos",
        "tokens": [
          51480,
          400,
          286,
          486,
          2864,
          291,
          281,
          512,
          661,
          2145,
          51588
        ]
      },
      {
        "avg_logprob": -0.20753550347481065,
        "compression_ratio": 1.6758620689655173,
        "end": 342.6,
        "id": 135,
        "no_speech_prob": 0.013020126149058342,
        "seek": 31544,
        "start": 339.92,
        "temperature": 0,
        "text": " that go on my channel that go into promises in more depth.",
        "tokens": [
          51588,
          300,
          352,
          322,
          452,
          2269,
          300,
          352,
          666,
          16403,
          294,
          544,
          7161,
          13,
          51722
        ]
      },
      {
        "avg_logprob": -0.19428414564866286,
        "compression_ratio": 1.8293650793650793,
        "end": 345.28000000000003,
        "id": 136,
        "no_speech_prob": 0.000037636273191310465,
        "seek": 34260,
        "start": 342.6,
        "temperature": 0,
        "text": " But the quick explanation here is",
        "tokens": [
          50364,
          583,
          264,
          1702,
          10835,
          510,
          307,
          50498
        ]
      },
      {
        "avg_logprob": -0.19428414564866286,
        "compression_ratio": 1.8293650793650793,
        "end": 347.84000000000003,
        "id": 137,
        "no_speech_prob": 0.000037636273191310465,
        "seek": 34260,
        "start": 345.28000000000003,
        "temperature": 0,
        "text": " that you can use the method then.",
        "tokens": [
          50498,
          300,
          291,
          393,
          764,
          264,
          3170,
          550,
          13,
          50626
        ]
      },
      {
        "avg_logprob": -0.19428414564866286,
        "compression_ratio": 1.8293650793650793,
        "end": 350.04,
        "id": 138,
        "no_speech_prob": 0.000037636273191310465,
        "seek": 34260,
        "start": 347.84000000000003,
        "temperature": 0,
        "text": " So the method then is a place where",
        "tokens": [
          50626,
          407,
          264,
          3170,
          550,
          307,
          257,
          1081,
          689,
          50736
        ]
      },
      {
        "avg_logprob": -0.19428414564866286,
        "compression_ratio": 1.8293650793650793,
        "end": 351.96000000000004,
        "id": 139,
        "no_speech_prob": 0.000037636273191310465,
        "seek": 34260,
        "start": 350.04,
        "temperature": 0,
        "text": " I can handle this response.",
        "tokens": [
          50736,
          286,
          393,
          4813,
          341,
          4134,
          13,
          50832
        ]
      },
      {
        "avg_logprob": -0.19428414564866286,
        "compression_ratio": 1.8293650793650793,
        "end": 355.12,
        "id": 140,
        "no_speech_prob": 0.000037636273191310465,
        "seek": 34260,
        "start": 351.96000000000004,
        "temperature": 0,
        "text": " So right now, what you're seeing is I type the method then.",
        "tokens": [
          50832,
          407,
          558,
          586,
          11,
          437,
          291,
          434,
          2577,
          307,
          286,
          2010,
          264,
          3170,
          550,
          13,
          50990
        ]
      },
      {
        "avg_logprob": -0.19428414564866286,
        "compression_ratio": 1.8293650793650793,
        "end": 357.44,
        "id": 141,
        "no_speech_prob": 0.000037636273191310465,
        "seek": 34260,
        "start": 355.12,
        "temperature": 0,
        "text": " I put an argument response.",
        "tokens": [
          50990,
          286,
          829,
          364,
          6770,
          4134,
          13,
          51106
        ]
      },
      {
        "avg_logprob": -0.19428414564866286,
        "compression_ratio": 1.8293650793650793,
        "end": 359.96000000000004,
        "id": 142,
        "no_speech_prob": 0.000037636273191310465,
        "seek": 34260,
        "start": 357.44,
        "temperature": 0,
        "text": " I use the JavaScript function arrow syntax",
        "tokens": [
          51106,
          286,
          764,
          264,
          15778,
          2445,
          11610,
          28431,
          51232
        ]
      },
      {
        "avg_logprob": -0.19428414564866286,
        "compression_ratio": 1.8293650793650793,
        "end": 362.28000000000003,
        "id": 143,
        "no_speech_prob": 0.000037636273191310465,
        "seek": 34260,
        "start": 359.96000000000004,
        "temperature": 0,
        "text": " to then do something with that response.",
        "tokens": [
          51232,
          281,
          550,
          360,
          746,
          365,
          300,
          4134,
          13,
          51348
        ]
      },
      {
        "avg_logprob": -0.19428414564866286,
        "compression_ratio": 1.8293650793650793,
        "end": 364.32000000000005,
        "id": 144,
        "no_speech_prob": 0.000037636273191310465,
        "seek": 34260,
        "start": 362.28000000000003,
        "temperature": 0,
        "text": " And what I want to do with that response, honestly,",
        "tokens": [
          51348,
          400,
          437,
          286,
          528,
          281,
          360,
          365,
          300,
          4134,
          11,
          6095,
          11,
          51450
        ]
      },
      {
        "avg_logprob": -0.19428414564866286,
        "compression_ratio": 1.8293650793650793,
        "end": 366.36,
        "id": 145,
        "no_speech_prob": 0.000037636273191310465,
        "seek": 34260,
        "start": 364.32000000000005,
        "temperature": 0,
        "text": " is convert it to a blob.",
        "tokens": [
          51450,
          307,
          7620,
          309,
          281,
          257,
          46115,
          13,
          51552
        ]
      },
      {
        "avg_logprob": -0.19428414564866286,
        "compression_ratio": 1.8293650793650793,
        "end": 370.88,
        "id": 146,
        "no_speech_prob": 0.000037636273191310465,
        "seek": 34260,
        "start": 366.36,
        "temperature": 0,
        "text": " But before I do that, let me just say console.log response",
        "tokens": [
          51552,
          583,
          949,
          286,
          360,
          300,
          11,
          718,
          385,
          445,
          584,
          11076,
          13,
          4987,
          4134,
          51778
        ]
      },
      {
        "avg_logprob": -0.19428414564866286,
        "compression_ratio": 1.8293650793650793,
        "end": 372.32000000000005,
        "id": 147,
        "no_speech_prob": 0.000037636273191310465,
        "seek": 34260,
        "start": 370.88,
        "temperature": 0,
        "text": " just so I can see it.",
        "tokens": [
          51778,
          445,
          370,
          286,
          393,
          536,
          309,
          13,
          51850
        ]
      },
      {
        "avg_logprob": -0.24842770316384055,
        "compression_ratio": 1.7768240343347639,
        "end": 374.48,
        "id": 148,
        "no_speech_prob": 0.000022473983335657977,
        "seek": 37232,
        "start": 373.04,
        "temperature": 0,
        "text": " So it helps to spell things correctly.",
        "tokens": [
          50400,
          407,
          309,
          3665,
          281,
          9827,
          721,
          8944,
          13,
          50472
        ]
      },
      {
        "avg_logprob": -0.24842770316384055,
        "compression_ratio": 1.7768240343347639,
        "end": 376.08,
        "id": 149,
        "no_speech_prob": 0.000022473983335657977,
        "seek": 37232,
        "start": 374.48,
        "temperature": 0,
        "text": " Response.",
        "tokens": [
          50472,
          43937,
          13,
          50552
        ]
      },
      {
        "avg_logprob": -0.24842770316384055,
        "compression_ratio": 1.7768240343347639,
        "end": 378.28,
        "id": 150,
        "no_speech_prob": 0.000022473983335657977,
        "seek": 37232,
        "start": 376.08,
        "temperature": 0,
        "text": " And we can see there's that response.",
        "tokens": [
          50552,
          400,
          321,
          393,
          536,
          456,
          311,
          300,
          4134,
          13,
          50662
        ]
      },
      {
        "avg_logprob": -0.24842770316384055,
        "compression_ratio": 1.7768240343347639,
        "end": 380.92,
        "id": 151,
        "no_speech_prob": 0.000022473983335657977,
        "seek": 37232,
        "start": 378.28,
        "temperature": 0,
        "text": " And you can see all the metadata associated with the response",
        "tokens": [
          50662,
          400,
          291,
          393,
          536,
          439,
          264,
          26603,
          6615,
          365,
          264,
          4134,
          50794
        ]
      },
      {
        "avg_logprob": -0.24842770316384055,
        "compression_ratio": 1.7768240343347639,
        "end": 382.96,
        "id": 152,
        "no_speech_prob": 0.000022473983335657977,
        "seek": 37232,
        "start": 380.92,
        "temperature": 0,
        "text": " is here in the JavaScript console.",
        "tokens": [
          50794,
          307,
          510,
          294,
          264,
          15778,
          11076,
          13,
          50896
        ]
      },
      {
        "avg_logprob": -0.24842770316384055,
        "compression_ratio": 1.7768240343347639,
        "end": 389.2,
        "id": 153,
        "no_speech_prob": 0.000022473983335657977,
        "seek": 37232,
        "start": 382.96,
        "temperature": 0,
        "text": " And what I actually want to do is say return response.blob",
        "tokens": [
          50896,
          400,
          437,
          286,
          767,
          528,
          281,
          360,
          307,
          584,
          2736,
          4134,
          13,
          15962,
          65,
          51208
        ]
      },
      {
        "avg_logprob": -0.24842770316384055,
        "compression_ratio": 1.7768240343347639,
        "end": 392.32,
        "id": 154,
        "no_speech_prob": 0.000022473983335657977,
        "seek": 37232,
        "start": 389.2,
        "temperature": 0,
        "text": " because I want to, this is what we talked about before,",
        "tokens": [
          51208,
          570,
          286,
          528,
          281,
          11,
          341,
          307,
          437,
          321,
          2825,
          466,
          949,
          11,
          51364
        ]
      },
      {
        "avg_logprob": -0.24842770316384055,
        "compression_ratio": 1.7768240343347639,
        "end": 396.68,
        "id": 155,
        "no_speech_prob": 0.000022473983335657977,
        "seek": 37232,
        "start": 392.32,
        "temperature": 0,
        "text": " I want to convert the response into an image blob.",
        "tokens": [
          51364,
          286,
          528,
          281,
          7620,
          264,
          4134,
          666,
          364,
          3256,
          46115,
          13,
          51582
        ]
      },
      {
        "avg_logprob": -0.24842770316384055,
        "compression_ratio": 1.7768240343347639,
        "end": 399,
        "id": 156,
        "no_speech_prob": 0.000022473983335657977,
        "seek": 37232,
        "start": 396.68,
        "temperature": 0,
        "text": " That triggers another promise.",
        "tokens": [
          51582,
          663,
          22827,
          1071,
          6228,
          13,
          51698
        ]
      },
      {
        "avg_logprob": -0.24842770316384055,
        "compression_ratio": 1.7768240343347639,
        "end": 401.36,
        "id": 157,
        "no_speech_prob": 0.000022473983335657977,
        "seek": 37232,
        "start": 399,
        "temperature": 0,
        "text": " So when I have another promise, I",
        "tokens": [
          51698,
          407,
          562,
          286,
          362,
          1071,
          6228,
          11,
          286,
          51816
        ]
      },
      {
        "avg_logprob": -0.20589968941428444,
        "compression_ratio": 1.6768558951965065,
        "end": 404.76,
        "id": 158,
        "no_speech_prob": 0.0003514330892357975,
        "seek": 40136,
        "start": 401.36,
        "temperature": 0,
        "text": " can chain them by saying dot then again.",
        "tokens": [
          50364,
          393,
          5021,
          552,
          538,
          1566,
          5893,
          550,
          797,
          13,
          50534
        ]
      },
      {
        "avg_logprob": -0.20589968941428444,
        "compression_ratio": 1.6768558951965065,
        "end": 409.72,
        "id": 159,
        "no_speech_prob": 0.0003514330892357975,
        "seek": 40136,
        "start": 404.76,
        "temperature": 0,
        "text": " And now the response, if we look at that response,",
        "tokens": [
          50534,
          400,
          586,
          264,
          4134,
          11,
          498,
          321,
          574,
          412,
          300,
          4134,
          11,
          50782
        ]
      },
      {
        "avg_logprob": -0.20589968941428444,
        "compression_ratio": 1.6768558951965065,
        "end": 412.92,
        "id": 160,
        "no_speech_prob": 0.0003514330892357975,
        "seek": 40136,
        "start": 409.72,
        "temperature": 0,
        "text": " console.log response, now I have the response",
        "tokens": [
          50782,
          11076,
          13,
          4987,
          4134,
          11,
          586,
          286,
          362,
          264,
          4134,
          50942
        ]
      },
      {
        "avg_logprob": -0.20589968941428444,
        "compression_ratio": 1.6768558951965065,
        "end": 415.36,
        "id": 161,
        "no_speech_prob": 0.0003514330892357975,
        "seek": 40136,
        "start": 412.92,
        "temperature": 0,
        "text": " to the next promise.",
        "tokens": [
          50942,
          281,
          264,
          958,
          6228,
          13,
          51064
        ]
      },
      {
        "avg_logprob": -0.20589968941428444,
        "compression_ratio": 1.6768558951965065,
        "end": 420.12,
        "id": 162,
        "no_speech_prob": 0.0003514330892357975,
        "seek": 40136,
        "start": 415.36,
        "temperature": 0,
        "text": " And we can see that is also not defined because I cannot",
        "tokens": [
          51064,
          400,
          321,
          393,
          536,
          300,
          307,
          611,
          406,
          7642,
          570,
          286,
          2644,
          51302
        ]
      },
      {
        "avg_logprob": -0.20589968941428444,
        "compression_ratio": 1.6768558951965065,
        "end": 422,
        "id": 163,
        "no_speech_prob": 0.0003514330892357975,
        "seek": 40136,
        "start": 420.12,
        "temperature": 0,
        "text": " spell to save my life.",
        "tokens": [
          51302,
          9827,
          281,
          3155,
          452,
          993,
          13,
          51396
        ]
      },
      {
        "avg_logprob": -0.20589968941428444,
        "compression_ratio": 1.6768558951965065,
        "end": 423.56,
        "id": 164,
        "no_speech_prob": 0.0003514330892357975,
        "seek": 40136,
        "start": 422,
        "temperature": 0,
        "text": " That is now the blob.",
        "tokens": [
          51396,
          663,
          307,
          586,
          264,
          46115,
          13,
          51474
        ]
      },
      {
        "avg_logprob": -0.20589968941428444,
        "compression_ratio": 1.6768558951965065,
        "end": 424.96000000000004,
        "id": 165,
        "no_speech_prob": 0.0003514330892357975,
        "seek": 40136,
        "start": 423.56,
        "temperature": 0,
        "text": " So in this case, actually, maybe it",
        "tokens": [
          51474,
          407,
          294,
          341,
          1389,
          11,
          767,
          11,
          1310,
          309,
          51544
        ]
      },
      {
        "avg_logprob": -0.20589968941428444,
        "compression_ratio": 1.6768558951965065,
        "end": 428.48,
        "id": 166,
        "no_speech_prob": 0.0003514330892357975,
        "seek": 40136,
        "start": 424.96000000000004,
        "temperature": 0,
        "text": " makes more sense for me to name this variable something else.",
        "tokens": [
          51544,
          1669,
          544,
          2020,
          337,
          385,
          281,
          1315,
          341,
          7006,
          746,
          1646,
          13,
          51720
        ]
      },
      {
        "avg_logprob": -0.20589968941428444,
        "compression_ratio": 1.6768558951965065,
        "end": 430.2,
        "id": 167,
        "no_speech_prob": 0.0003514330892357975,
        "seek": 40136,
        "start": 428.48,
        "temperature": 0,
        "text": " Let me just call it blob.",
        "tokens": [
          51720,
          961,
          385,
          445,
          818,
          309,
          46115,
          13,
          51806
        ]
      },
      {
        "avg_logprob": -0.21359320609800278,
        "compression_ratio": 1.570754716981132,
        "end": 434.15999999999997,
        "id": 168,
        "no_speech_prob": 0.000009368704013468232,
        "seek": 43020,
        "start": 430.2,
        "temperature": 0,
        "text": " And now what you'll see is the sequence is as follows.",
        "tokens": [
          50364,
          400,
          586,
          437,
          291,
          603,
          536,
          307,
          264,
          8310,
          307,
          382,
          10002,
          13,
          50562
        ]
      },
      {
        "avg_logprob": -0.21359320609800278,
        "compression_ratio": 1.570754716981132,
        "end": 435.76,
        "id": 169,
        "no_speech_prob": 0.000009368704013468232,
        "seek": 43020,
        "start": 434.15999999999997,
        "temperature": 0,
        "text": " First, fetch the rainbow.",
        "tokens": [
          50562,
          2386,
          11,
          23673,
          264,
          18526,
          13,
          50642
        ]
      },
      {
        "avg_logprob": -0.21359320609800278,
        "compression_ratio": 1.570754716981132,
        "end": 440.76,
        "id": 170,
        "no_speech_prob": 0.000009368704013468232,
        "seek": 43020,
        "start": 435.76,
        "temperature": 0,
        "text": " Second, look at the resolved promise",
        "tokens": [
          50642,
          5736,
          11,
          574,
          412,
          264,
          20772,
          6228,
          50892
        ]
      },
      {
        "avg_logprob": -0.21359320609800278,
        "compression_ratio": 1.570754716981132,
        "end": 444.28,
        "id": 171,
        "no_speech_prob": 0.000009368704013468232,
        "seek": 43020,
        "start": 440.76,
        "temperature": 0,
        "text": " and then convert that complete reading the stream of data",
        "tokens": [
          50892,
          293,
          550,
          7620,
          300,
          3566,
          3760,
          264,
          4309,
          295,
          1412,
          51068
        ]
      },
      {
        "avg_logprob": -0.21359320609800278,
        "compression_ratio": 1.570754716981132,
        "end": 445.15999999999997,
        "id": 172,
        "no_speech_prob": 0.000009368704013468232,
        "seek": 43020,
        "start": 444.28,
        "temperature": 0,
        "text": " into a blob.",
        "tokens": [
          51068,
          666,
          257,
          46115,
          13,
          51112
        ]
      },
      {
        "avg_logprob": -0.21359320609800278,
        "compression_ratio": 1.570754716981132,
        "end": 448.52,
        "id": 173,
        "no_speech_prob": 0.000009368704013468232,
        "seek": 43020,
        "start": 445.15999999999997,
        "temperature": 0,
        "text": " So presumably, once I'm there, all I have to do is say,",
        "tokens": [
          51112,
          407,
          26742,
          11,
          1564,
          286,
          478,
          456,
          11,
          439,
          286,
          362,
          281,
          360,
          307,
          584,
          11,
          51280
        ]
      },
      {
        "avg_logprob": -0.21359320609800278,
        "compression_ratio": 1.570754716981132,
        "end": 450.71999999999997,
        "id": 174,
        "no_speech_prob": 0.000009368704013468232,
        "seek": 43020,
        "start": 448.52,
        "temperature": 0,
        "text": " remember I have this image element.",
        "tokens": [
          51280,
          1604,
          286,
          362,
          341,
          3256,
          4478,
          13,
          51390
        ]
      },
      {
        "avg_logprob": -0.21359320609800278,
        "compression_ratio": 1.570754716981132,
        "end": 453.91999999999996,
        "id": 175,
        "no_speech_prob": 0.000009368704013468232,
        "seek": 43020,
        "start": 450.71999999999997,
        "temperature": 0,
        "text": " I have this image DOM element with an ID of rainbow.",
        "tokens": [
          51390,
          286,
          362,
          341,
          3256,
          35727,
          4478,
          365,
          364,
          7348,
          295,
          18526,
          13,
          51550
        ]
      },
      {
        "avg_logprob": -0.24148844728375427,
        "compression_ratio": 1.6495327102803738,
        "end": 460.76,
        "id": 176,
        "no_speech_prob": 0.000011125601304229349,
        "seek": 45392,
        "start": 453.92,
        "temperature": 0,
        "text": " I can say document.getElementByID,",
        "tokens": [
          50364,
          286,
          393,
          584,
          4166,
          13,
          847,
          36,
          3054,
          27690,
          2777,
          11,
          50706
        ]
      },
      {
        "avg_logprob": -0.24148844728375427,
        "compression_ratio": 1.6495327102803738,
        "end": 466.16,
        "id": 177,
        "no_speech_prob": 0.000011125601304229349,
        "seek": 45392,
        "start": 460.76,
        "temperature": 0,
        "text": " give it the ID rainbow, and dot source equals that blob.",
        "tokens": [
          50706,
          976,
          309,
          264,
          7348,
          18526,
          11,
          293,
          5893,
          4009,
          6915,
          300,
          46115,
          13,
          50976
        ]
      },
      {
        "avg_logprob": -0.24148844728375427,
        "compression_ratio": 1.6495327102803738,
        "end": 467.92,
        "id": 178,
        "no_speech_prob": 0.000011125601304229349,
        "seek": 45392,
        "start": 466.16,
        "temperature": 0,
        "text": " So this is me just taking the data",
        "tokens": [
          50976,
          407,
          341,
          307,
          385,
          445,
          1940,
          264,
          1412,
          51064
        ]
      },
      {
        "avg_logprob": -0.24148844728375427,
        "compression_ratio": 1.6495327102803738,
        "end": 470.52000000000004,
        "id": 179,
        "no_speech_prob": 0.000011125601304229349,
        "seek": 45392,
        "start": 467.92,
        "temperature": 0,
        "text": " of that image, which comes in as a blob,",
        "tokens": [
          51064,
          295,
          300,
          3256,
          11,
          597,
          1487,
          294,
          382,
          257,
          46115,
          11,
          51194
        ]
      },
      {
        "avg_logprob": -0.24148844728375427,
        "compression_ratio": 1.6495327102803738,
        "end": 475.32,
        "id": 180,
        "no_speech_prob": 0.000011125601304229349,
        "seek": 45392,
        "start": 470.52000000000004,
        "temperature": 0,
        "text": " and placing it into the source attribute of the image DOM",
        "tokens": [
          51194,
          293,
          17221,
          309,
          666,
          264,
          4009,
          19667,
          295,
          264,
          3256,
          35727,
          51434
        ]
      },
      {
        "avg_logprob": -0.24148844728375427,
        "compression_ratio": 1.6495327102803738,
        "end": 476.44,
        "id": 181,
        "no_speech_prob": 0.000011125601304229349,
        "seek": 45392,
        "start": 475.32,
        "temperature": 0,
        "text": " element.",
        "tokens": [
          51434,
          4478,
          13,
          51490
        ]
      },
      {
        "avg_logprob": -0.24148844728375427,
        "compression_ratio": 1.6495327102803738,
        "end": 480.16,
        "id": 182,
        "no_speech_prob": 0.000011125601304229349,
        "seek": 45392,
        "start": 476.44,
        "temperature": 0,
        "text": " You can see that doesn't work because the blob, the data",
        "tokens": [
          51490,
          509,
          393,
          536,
          300,
          1177,
          380,
          589,
          570,
          264,
          46115,
          11,
          264,
          1412,
          51676
        ]
      },
      {
        "avg_logprob": -0.24148844728375427,
        "compression_ratio": 1.6495327102803738,
        "end": 483.68,
        "id": 183,
        "no_speech_prob": 0.000011125601304229349,
        "seek": 45392,
        "start": 480.16,
        "temperature": 0,
        "text": " blob, isn't in the format that the image DOM element expects.",
        "tokens": [
          51676,
          46115,
          11,
          1943,
          380,
          294,
          264,
          7877,
          300,
          264,
          3256,
          35727,
          4478,
          33280,
          13,
          51852
        ]
      },
      {
        "avg_logprob": -0.2293810314602322,
        "compression_ratio": 1.530909090909091,
        "end": 486.44,
        "id": 184,
        "no_speech_prob": 0.000720830459613353,
        "seek": 48368,
        "start": 483.98,
        "temperature": 0,
        "text": " Luckily for us, there happens to be a function called create",
        "tokens": [
          50379,
          19726,
          337,
          505,
          11,
          456,
          2314,
          281,
          312,
          257,
          2445,
          1219,
          1884,
          50502
        ]
      },
      {
        "avg_logprob": -0.2293810314602322,
        "compression_ratio": 1.530909090909091,
        "end": 488.88,
        "id": 185,
        "no_speech_prob": 0.000720830459613353,
        "seek": 48368,
        "start": 486.44,
        "temperature": 0,
        "text": " object URL that takes a blob object",
        "tokens": [
          50502,
          2657,
          12905,
          300,
          2516,
          257,
          46115,
          2657,
          50624
        ]
      },
      {
        "avg_logprob": -0.2293810314602322,
        "compression_ratio": 1.530909090909091,
        "end": 491.52,
        "id": 186,
        "no_speech_prob": 0.000720830459613353,
        "seek": 48368,
        "start": 488.88,
        "temperature": 0,
        "text": " and turns it into the format that an image DOM",
        "tokens": [
          50624,
          293,
          4523,
          309,
          666,
          264,
          7877,
          300,
          364,
          3256,
          35727,
          50756
        ]
      },
      {
        "avg_logprob": -0.2293810314602322,
        "compression_ratio": 1.530909090909091,
        "end": 493.32,
        "id": 187,
        "no_speech_prob": 0.000720830459613353,
        "seek": 48368,
        "start": 491.52,
        "temperature": 0,
        "text": " element would expect.",
        "tokens": [
          50756,
          4478,
          576,
          2066,
          13,
          50846
        ]
      },
      {
        "avg_logprob": -0.2293810314602322,
        "compression_ratio": 1.530909090909091,
        "end": 495.28000000000003,
        "id": 188,
        "no_speech_prob": 0.000720830459613353,
        "seek": 48368,
        "start": 493.32,
        "temperature": 0,
        "text": " You can look up more about this function also",
        "tokens": [
          50846,
          509,
          393,
          574,
          493,
          544,
          466,
          341,
          2445,
          611,
          50944
        ]
      },
      {
        "avg_logprob": -0.2293810314602322,
        "compression_ratio": 1.530909090909091,
        "end": 497.6,
        "id": 189,
        "no_speech_prob": 0.000720830459613353,
        "seek": 48368,
        "start": 495.28000000000003,
        "temperature": 0,
        "text": " on the MDN JavaScript docs.",
        "tokens": [
          50944,
          322,
          264,
          22521,
          45,
          15778,
          45623,
          13,
          51060
        ]
      },
      {
        "avg_logprob": -0.2293810314602322,
        "compression_ratio": 1.530909090909091,
        "end": 502.56,
        "id": 190,
        "no_speech_prob": 0.000720830459613353,
        "seek": 48368,
        "start": 497.6,
        "temperature": 0,
        "text": " But all I need to do here is say URL.createObjectURL,",
        "tokens": [
          51060,
          583,
          439,
          286,
          643,
          281,
          360,
          510,
          307,
          584,
          12905,
          13,
          14066,
          473,
          45483,
          1020,
          7932,
          43,
          11,
          51308
        ]
      },
      {
        "avg_logprob": -0.2293810314602322,
        "compression_ratio": 1.530909090909091,
        "end": 505.08,
        "id": 191,
        "no_speech_prob": 0.000720830459613353,
        "seek": 48368,
        "start": 502.56,
        "temperature": 0,
        "text": " pass in the blob.",
        "tokens": [
          51308,
          1320,
          294,
          264,
          46115,
          13,
          51434
        ]
      },
      {
        "avg_logprob": -0.2293810314602322,
        "compression_ratio": 1.530909090909091,
        "end": 506.16,
        "id": 192,
        "no_speech_prob": 0.000720830459613353,
        "seek": 48368,
        "start": 505.08,
        "temperature": 0,
        "text": " And there it is.",
        "tokens": [
          51434,
          400,
          456,
          309,
          307,
          13,
          51488
        ]
      },
      {
        "avg_logprob": -0.2293810314602322,
        "compression_ratio": 1.530909090909091,
        "end": 507.64,
        "id": 193,
        "no_speech_prob": 0.000720830459613353,
        "seek": 48368,
        "start": 506.16,
        "temperature": 0,
        "text": " We have our rainbow.",
        "tokens": [
          51488,
          492,
          362,
          527,
          18526,
          13,
          51562
        ]
      },
      {
        "avg_logprob": -0.2293810314602322,
        "compression_ratio": 1.530909090909091,
        "end": 509.44,
        "id": 194,
        "no_speech_prob": 0.000720830459613353,
        "seek": 48368,
        "start": 507.64,
        "temperature": 0,
        "text": " It's kind of a large image.",
        "tokens": [
          51562,
          467,
          311,
          733,
          295,
          257,
          2416,
          3256,
          13,
          51652
        ]
      },
      {
        "avg_logprob": -0.2293810314602322,
        "compression_ratio": 1.530909090909091,
        "end": 511.28000000000003,
        "id": 195,
        "no_speech_prob": 0.000720830459613353,
        "seek": 48368,
        "start": 509.44,
        "temperature": 0,
        "text": " So I'm just going to add another attribute,",
        "tokens": [
          51652,
          407,
          286,
          478,
          445,
          516,
          281,
          909,
          1071,
          19667,
          11,
          51744
        ]
      },
      {
        "avg_logprob": -0.2219577566550596,
        "compression_ratio": 1.6798561151079137,
        "end": 515.12,
        "id": 196,
        "no_speech_prob": 0.00009461243462283164,
        "seek": 51128,
        "start": 511.55999999999995,
        "temperature": 0,
        "text": " width equals 480.",
        "tokens": [
          50378,
          11402,
          6915,
          1017,
          4702,
          13,
          50556
        ]
      },
      {
        "avg_logprob": -0.2219577566550596,
        "compression_ratio": 1.6798561151079137,
        "end": 516.76,
        "id": 197,
        "no_speech_prob": 0.00009461243462283164,
        "seek": 51128,
        "start": 515.12,
        "temperature": 0,
        "text": " So it sort of makes the image smaller.",
        "tokens": [
          50556,
          407,
          309,
          1333,
          295,
          1669,
          264,
          3256,
          4356,
          13,
          50638
        ]
      },
      {
        "avg_logprob": -0.2219577566550596,
        "compression_ratio": 1.6798561151079137,
        "end": 517.48,
        "id": 198,
        "no_speech_prob": 0.00009461243462283164,
        "seek": 51128,
        "start": 516.76,
        "temperature": 0,
        "text": " And there we go.",
        "tokens": [
          50638,
          400,
          456,
          321,
          352,
          13,
          50674
        ]
      },
      {
        "avg_logprob": -0.2219577566550596,
        "compression_ratio": 1.6798561151079137,
        "end": 519.12,
        "id": 199,
        "no_speech_prob": 0.00009461243462283164,
        "seek": 51128,
        "start": 517.48,
        "temperature": 0,
        "text": " Oh, we have a beautiful rainbow.",
        "tokens": [
          50674,
          876,
          11,
          321,
          362,
          257,
          2238,
          18526,
          13,
          50756
        ]
      },
      {
        "avg_logprob": -0.2219577566550596,
        "compression_ratio": 1.6798561151079137,
        "end": 522.4,
        "id": 200,
        "no_speech_prob": 0.00009461243462283164,
        "seek": 51128,
        "start": 519.12,
        "temperature": 0,
        "text": " So this really wraps up this particular tutorial.",
        "tokens": [
          50756,
          407,
          341,
          534,
          25831,
          493,
          341,
          1729,
          7073,
          13,
          50920
        ]
      },
      {
        "avg_logprob": -0.2219577566550596,
        "compression_ratio": 1.6798561151079137,
        "end": 524,
        "id": 201,
        "no_speech_prob": 0.00009461243462283164,
        "seek": 51128,
        "start": 522.4,
        "temperature": 0,
        "text": " We've got the whole thing.",
        "tokens": [
          50920,
          492,
          600,
          658,
          264,
          1379,
          551,
          13,
          51000
        ]
      },
      {
        "avg_logprob": -0.2219577566550596,
        "compression_ratio": 1.6798561151079137,
        "end": 529.12,
        "id": 202,
        "no_speech_prob": 0.00009461243462283164,
        "seek": 51128,
        "start": 524,
        "temperature": 0,
        "text": " We now know how to call fetch, how to read the response",
        "tokens": [
          51000,
          492,
          586,
          458,
          577,
          281,
          818,
          23673,
          11,
          577,
          281,
          1401,
          264,
          4134,
          51256
        ]
      },
      {
        "avg_logprob": -0.2219577566550596,
        "compression_ratio": 1.6798561151079137,
        "end": 531.56,
        "id": 203,
        "no_speech_prob": 0.00009461243462283164,
        "seek": 51128,
        "start": 529.12,
        "temperature": 0,
        "text": " when the promise resolves, how to turn",
        "tokens": [
          51256,
          562,
          264,
          6228,
          7923,
          977,
          11,
          577,
          281,
          1261,
          51378
        ]
      },
      {
        "avg_logprob": -0.2219577566550596,
        "compression_ratio": 1.6798561151079137,
        "end": 534,
        "id": 204,
        "no_speech_prob": 0.00009461243462283164,
        "seek": 51128,
        "start": 531.56,
        "temperature": 0,
        "text": " the data of that response into an image blob",
        "tokens": [
          51378,
          264,
          1412,
          295,
          300,
          4134,
          666,
          364,
          3256,
          46115,
          51500
        ]
      },
      {
        "avg_logprob": -0.2219577566550596,
        "compression_ratio": 1.6798561151079137,
        "end": 535.52,
        "id": 205,
        "no_speech_prob": 0.00009461243462283164,
        "seek": 51128,
        "start": 534,
        "temperature": 0,
        "text": " and add it into an image ELT.",
        "tokens": [
          51500,
          293,
          909,
          309,
          666,
          364,
          3256,
          14426,
          51,
          13,
          51576
        ]
      },
      {
        "avg_logprob": -0.2219577566550596,
        "compression_ratio": 1.6798561151079137,
        "end": 536.8399999999999,
        "id": 206,
        "no_speech_prob": 0.00009461243462283164,
        "seek": 51128,
        "start": 535.52,
        "temperature": 0,
        "text": " But here's the thing.",
        "tokens": [
          51576,
          583,
          510,
          311,
          264,
          551,
          13,
          51642
        ]
      },
      {
        "avg_logprob": -0.2219577566550596,
        "compression_ratio": 1.6798561151079137,
        "end": 538.04,
        "id": 207,
        "no_speech_prob": 0.00009461243462283164,
        "seek": 51128,
        "start": 536.8399999999999,
        "temperature": 0,
        "text": " There's a couple other things that I've",
        "tokens": [
          51642,
          821,
          311,
          257,
          1916,
          661,
          721,
          300,
          286,
          600,
          51702
        ]
      },
      {
        "avg_logprob": -0.2219577566550596,
        "compression_ratio": 1.6798561151079137,
        "end": 540.24,
        "id": 208,
        "no_speech_prob": 0.00009461243462283164,
        "seek": 51128,
        "start": 538.04,
        "temperature": 0,
        "text": " missed that would be nice to include in this video.",
        "tokens": [
          51702,
          6721,
          300,
          576,
          312,
          1481,
          281,
          4090,
          294,
          341,
          960,
          13,
          51812
        ]
      },
      {
        "avg_logprob": -0.22654465604419552,
        "compression_ratio": 1.7153284671532847,
        "end": 543.4,
        "id": 209,
        "no_speech_prob": 0.0007096625631675124,
        "seek": 54024,
        "start": 540.24,
        "temperature": 0,
        "text": " Number one is I've done nothing to handle errors.",
        "tokens": [
          50364,
          5118,
          472,
          307,
          286,
          600,
          1096,
          1825,
          281,
          4813,
          13603,
          13,
          50522
        ]
      },
      {
        "avg_logprob": -0.22654465604419552,
        "compression_ratio": 1.7153284671532847,
        "end": 544.92,
        "id": 210,
        "no_speech_prob": 0.0007096625631675124,
        "seek": 54024,
        "start": 543.4,
        "temperature": 0,
        "text": " So what if something goes wrong?",
        "tokens": [
          50522,
          407,
          437,
          498,
          746,
          1709,
          2085,
          30,
          50598
        ]
      },
      {
        "avg_logprob": -0.22654465604419552,
        "compression_ratio": 1.7153284671532847,
        "end": 547.48,
        "id": 211,
        "no_speech_prob": 0.0007096625631675124,
        "seek": 54024,
        "start": 544.92,
        "temperature": 0,
        "text": " I should handle an error, at least log the error",
        "tokens": [
          50598,
          286,
          820,
          4813,
          364,
          6713,
          11,
          412,
          1935,
          3565,
          264,
          6713,
          50726
        ]
      },
      {
        "avg_logprob": -0.22654465604419552,
        "compression_ratio": 1.7153284671532847,
        "end": 550.28,
        "id": 212,
        "no_speech_prob": 0.0007096625631675124,
        "seek": 54024,
        "start": 547.48,
        "temperature": 0,
        "text": " to the console so I can see that something went wrong.",
        "tokens": [
          50726,
          281,
          264,
          11076,
          370,
          286,
          393,
          536,
          300,
          746,
          1437,
          2085,
          13,
          50866
        ]
      },
      {
        "avg_logprob": -0.22654465604419552,
        "compression_ratio": 1.7153284671532847,
        "end": 555.36,
        "id": 213,
        "no_speech_prob": 0.0007096625631675124,
        "seek": 54024,
        "start": 550.28,
        "temperature": 0,
        "text": " Number two is I want to introduce the JavaScript",
        "tokens": [
          50866,
          5118,
          732,
          307,
          286,
          528,
          281,
          5366,
          264,
          15778,
          51120
        ]
      },
      {
        "avg_logprob": -0.22654465604419552,
        "compression_ratio": 1.7153284671532847,
        "end": 558.36,
        "id": 214,
        "no_speech_prob": 0.0007096625631675124,
        "seek": 54024,
        "start": 555.36,
        "temperature": 0,
        "text": " keywords async and await.",
        "tokens": [
          51120,
          21009,
          382,
          34015,
          293,
          19670,
          13,
          51270
        ]
      },
      {
        "avg_logprob": -0.22654465604419552,
        "compression_ratio": 1.7153284671532847,
        "end": 561.4,
        "id": 215,
        "no_speech_prob": 0.0007096625631675124,
        "seek": 54024,
        "start": 558.36,
        "temperature": 0,
        "text": " And these are new features of the JavaScript language",
        "tokens": [
          51270,
          400,
          613,
          366,
          777,
          4122,
          295,
          264,
          15778,
          2856,
          51422
        ]
      },
      {
        "avg_logprob": -0.22654465604419552,
        "compression_ratio": 1.7153284671532847,
        "end": 564.52,
        "id": 216,
        "no_speech_prob": 0.0007096625631675124,
        "seek": 54024,
        "start": 561.4,
        "temperature": 0,
        "text": " that allow us to handle promises in a slightly more",
        "tokens": [
          51422,
          300,
          2089,
          505,
          281,
          4813,
          16403,
          294,
          257,
          4748,
          544,
          51578
        ]
      },
      {
        "avg_logprob": -0.22654465604419552,
        "compression_ratio": 1.7153284671532847,
        "end": 566.6800000000001,
        "id": 217,
        "no_speech_prob": 0.0007096625631675124,
        "seek": 54024,
        "start": 564.52,
        "temperature": 0,
        "text": " readable and elegant way.",
        "tokens": [
          51578,
          49857,
          293,
          21117,
          636,
          13,
          51686
        ]
      },
      {
        "avg_logprob": -0.22654465604419552,
        "compression_ratio": 1.7153284671532847,
        "end": 567.76,
        "id": 218,
        "no_speech_prob": 0.0007096625631675124,
        "seek": 54024,
        "start": 566.6800000000001,
        "temperature": 0,
        "text": " So I'm going to show you.",
        "tokens": [
          51686,
          407,
          286,
          478,
          516,
          281,
          855,
          291,
          13,
          51740
        ]
      },
      {
        "avg_logprob": -0.22654465604419552,
        "compression_ratio": 1.7153284671532847,
        "end": 570.04,
        "id": 219,
        "no_speech_prob": 0.0007096625631675124,
        "seek": 54024,
        "start": 567.76,
        "temperature": 0,
        "text": " I'm going to rewrite this particular example using",
        "tokens": [
          51740,
          286,
          478,
          516,
          281,
          28132,
          341,
          1729,
          1365,
          1228,
          51854
        ]
      },
      {
        "avg_logprob": -0.21550070784474148,
        "compression_ratio": 1.707142857142857,
        "end": 574.9599999999999,
        "id": 220,
        "no_speech_prob": 0.0001214812436955981,
        "seek": 57004,
        "start": 570.8,
        "temperature": 0,
        "text": " async and await instead of the.then method.",
        "tokens": [
          50402,
          382,
          34015,
          293,
          19670,
          2602,
          295,
          264,
          2411,
          19096,
          3170,
          13,
          50610
        ]
      },
      {
        "avg_logprob": -0.21550070784474148,
        "compression_ratio": 1.707142857142857,
        "end": 577.28,
        "id": 221,
        "no_speech_prob": 0.0001214812436955981,
        "seek": 57004,
        "start": 574.9599999999999,
        "temperature": 0,
        "text": " And I will also refer you to some other videos",
        "tokens": [
          50610,
          400,
          286,
          486,
          611,
          2864,
          291,
          281,
          512,
          661,
          2145,
          50726
        ]
      },
      {
        "avg_logprob": -0.21550070784474148,
        "compression_ratio": 1.707142857142857,
        "end": 580.4399999999999,
        "id": 222,
        "no_speech_prob": 0.0001214812436955981,
        "seek": 57004,
        "start": 577.28,
        "temperature": 0,
        "text": " I've made that go into these keywords with a bit more depth.",
        "tokens": [
          50726,
          286,
          600,
          1027,
          300,
          352,
          666,
          613,
          21009,
          365,
          257,
          857,
          544,
          7161,
          13,
          50884
        ]
      },
      {
        "avg_logprob": -0.21550070784474148,
        "compression_ratio": 1.707142857142857,
        "end": 582.52,
        "id": 223,
        "no_speech_prob": 0.0001214812436955981,
        "seek": 57004,
        "start": 580.4399999999999,
        "temperature": 0,
        "text": " Let's first add some error handling.",
        "tokens": [
          50884,
          961,
          311,
          700,
          909,
          512,
          6713,
          13175,
          13,
          50988
        ]
      },
      {
        "avg_logprob": -0.21550070784474148,
        "compression_ratio": 1.707142857142857,
        "end": 585.5999999999999,
        "id": 224,
        "no_speech_prob": 0.0001214812436955981,
        "seek": 57004,
        "start": 582.52,
        "temperature": 0,
        "text": " One of the nice things about working with chaining promises",
        "tokens": [
          50988,
          1485,
          295,
          264,
          1481,
          721,
          466,
          1364,
          365,
          417,
          3686,
          16403,
          51142
        ]
      },
      {
        "avg_logprob": -0.21550070784474148,
        "compression_ratio": 1.707142857142857,
        "end": 588.48,
        "id": 225,
        "no_speech_prob": 0.0001214812436955981,
        "seek": 57004,
        "start": 585.5999999999999,
        "temperature": 0,
        "text": " that we have fetch, then this, then this, then this,",
        "tokens": [
          51142,
          300,
          321,
          362,
          23673,
          11,
          550,
          341,
          11,
          550,
          341,
          11,
          550,
          341,
          11,
          51286
        ]
      },
      {
        "avg_logprob": -0.21550070784474148,
        "compression_ratio": 1.707142857142857,
        "end": 590.8,
        "id": 226,
        "no_speech_prob": 0.0001214812436955981,
        "seek": 57004,
        "start": 588.48,
        "temperature": 0,
        "text": " is an error can happen anywhere throughout there.",
        "tokens": [
          51286,
          307,
          364,
          6713,
          393,
          1051,
          4992,
          3710,
          456,
          13,
          51402
        ]
      },
      {
        "avg_logprob": -0.21550070784474148,
        "compression_ratio": 1.707142857142857,
        "end": 593.5,
        "id": 227,
        "no_speech_prob": 0.0001214812436955981,
        "seek": 57004,
        "start": 590.8,
        "temperature": 0,
        "text": " And I don't have to handle each error in a specific way",
        "tokens": [
          51402,
          400,
          286,
          500,
          380,
          362,
          281,
          4813,
          1184,
          6713,
          294,
          257,
          2685,
          636,
          51537
        ]
      },
      {
        "avg_logprob": -0.21550070784474148,
        "compression_ratio": 1.707142857142857,
        "end": 595.28,
        "id": 228,
        "no_speech_prob": 0.0001214812436955981,
        "seek": 57004,
        "start": 593.5,
        "temperature": 0,
        "text": " with each part of that sequence.",
        "tokens": [
          51537,
          365,
          1184,
          644,
          295,
          300,
          8310,
          13,
          51626
        ]
      },
      {
        "avg_logprob": -0.21550070784474148,
        "compression_ratio": 1.707142857142857,
        "end": 597.9599999999999,
        "id": 229,
        "no_speech_prob": 0.0001214812436955981,
        "seek": 57004,
        "start": 595.28,
        "temperature": 0,
        "text": " I can just put at the very end.catch.",
        "tokens": [
          51626,
          286,
          393,
          445,
          829,
          412,
          264,
          588,
          917,
          2411,
          66,
          852,
          13,
          51760
        ]
      },
      {
        "avg_logprob": -0.2114477219924428,
        "compression_ratio": 1.7773851590106007,
        "end": 602.0400000000001,
        "id": 230,
        "no_speech_prob": 0.00005144194437889382,
        "seek": 59796,
        "start": 597.96,
        "temperature": 0,
        "text": " So like.then, which handles the resolution of the promise,",
        "tokens": [
          50364,
          407,
          411,
          2411,
          19096,
          11,
          597,
          18722,
          264,
          8669,
          295,
          264,
          6228,
          11,
          50568
        ]
      },
      {
        "avg_logprob": -0.2114477219924428,
        "compression_ratio": 1.7773851590106007,
        "end": 605.8000000000001,
        "id": 231,
        "no_speech_prob": 0.00005144194437889382,
        "seek": 59796,
        "start": 602.0400000000001,
        "temperature": 0,
        "text": ".catch handles stuff that goes wrong when stuff goes",
        "tokens": [
          50568,
          2411,
          66,
          852,
          18722,
          1507,
          300,
          1709,
          2085,
          562,
          1507,
          1709,
          50756
        ]
      },
      {
        "avg_logprob": -0.2114477219924428,
        "compression_ratio": 1.7773851590106007,
        "end": 607.08,
        "id": 232,
        "no_speech_prob": 0.00005144194437889382,
        "seek": 59796,
        "start": 605.8000000000001,
        "temperature": 0,
        "text": " wrong during that promise.",
        "tokens": [
          50756,
          2085,
          1830,
          300,
          6228,
          13,
          50820
        ]
      },
      {
        "avg_logprob": -0.2114477219924428,
        "compression_ratio": 1.7773851590106007,
        "end": 608.4000000000001,
        "id": 233,
        "no_speech_prob": 0.00005144194437889382,
        "seek": 59796,
        "start": 607.08,
        "temperature": 0,
        "text": " So I can write catch.",
        "tokens": [
          50820,
          407,
          286,
          393,
          2464,
          3745,
          13,
          50886
        ]
      },
      {
        "avg_logprob": -0.2114477219924428,
        "compression_ratio": 1.7773851590106007,
        "end": 609.96,
        "id": 234,
        "no_speech_prob": 0.00005144194437889382,
        "seek": 59796,
        "start": 608.4000000000001,
        "temperature": 0,
        "text": " I can give an argument error.",
        "tokens": [
          50886,
          286,
          393,
          976,
          364,
          6770,
          6713,
          13,
          50964
        ]
      },
      {
        "avg_logprob": -0.2114477219924428,
        "compression_ratio": 1.7773851590106007,
        "end": 616.72,
        "id": 235,
        "no_speech_prob": 0.00005144194437889382,
        "seek": 59796,
        "start": 609.96,
        "temperature": 0,
        "text": " And I can just say something like console.error, error.",
        "tokens": [
          50964,
          400,
          286,
          393,
          445,
          584,
          746,
          411,
          11076,
          13,
          260,
          2874,
          11,
          6713,
          13,
          51302
        ]
      },
      {
        "avg_logprob": -0.2114477219924428,
        "compression_ratio": 1.7773851590106007,
        "end": 618.24,
        "id": 236,
        "no_speech_prob": 0.00005144194437889382,
        "seek": 59796,
        "start": 616.72,
        "temperature": 0,
        "text": " So this is a bit more long-winded",
        "tokens": [
          51302,
          407,
          341,
          307,
          257,
          857,
          544,
          938,
          12,
          12199,
          292,
          51378
        ]
      },
      {
        "avg_logprob": -0.2114477219924428,
        "compression_ratio": 1.7773851590106007,
        "end": 619.24,
        "id": 237,
        "no_speech_prob": 0.00005144194437889382,
        "seek": 59796,
        "start": 618.24,
        "temperature": 0,
        "text": " than it needs to be.",
        "tokens": [
          51378,
          813,
          309,
          2203,
          281,
          312,
          13,
          51428
        ]
      },
      {
        "avg_logprob": -0.2114477219924428,
        "compression_ratio": 1.7773851590106007,
        "end": 620.76,
        "id": 238,
        "no_speech_prob": 0.00005144194437889382,
        "seek": 59796,
        "start": 619.24,
        "temperature": 0,
        "text": " There are ways of condensing this.",
        "tokens": [
          51428,
          821,
          366,
          2098,
          295,
          2224,
          22481,
          341,
          13,
          51504
        ]
      },
      {
        "avg_logprob": -0.2114477219924428,
        "compression_ratio": 1.7773851590106007,
        "end": 622.4200000000001,
        "id": 239,
        "no_speech_prob": 0.00005144194437889382,
        "seek": 59796,
        "start": 620.76,
        "temperature": 0,
        "text": " But in a moment, I'm going to do that all",
        "tokens": [
          51504,
          583,
          294,
          257,
          1623,
          11,
          286,
          478,
          516,
          281,
          360,
          300,
          439,
          51587
        ]
      },
      {
        "avg_logprob": -0.2114477219924428,
        "compression_ratio": 1.7773851590106007,
        "end": 623.84,
        "id": 240,
        "no_speech_prob": 0.00005144194437889382,
        "seek": 59796,
        "start": 622.4200000000001,
        "temperature": 0,
        "text": " with just async and await.",
        "tokens": [
          51587,
          365,
          445,
          382,
          34015,
          293,
          19670,
          13,
          51658
        ]
      },
      {
        "avg_logprob": -0.2114477219924428,
        "compression_ratio": 1.7773851590106007,
        "end": 625.24,
        "id": 241,
        "no_speech_prob": 0.00005144194437889382,
        "seek": 59796,
        "start": 623.84,
        "temperature": 0,
        "text": " I'm not going to worry about it too much.",
        "tokens": [
          51658,
          286,
          478,
          406,
          516,
          281,
          3292,
          466,
          309,
          886,
          709,
          13,
          51728
        ]
      },
      {
        "avg_logprob": -0.2114477219924428,
        "compression_ratio": 1.7773851590106007,
        "end": 627.64,
        "id": 242,
        "no_speech_prob": 0.00005144194437889382,
        "seek": 59796,
        "start": 625.24,
        "temperature": 0,
        "text": " I'm going to make an error happen just so we can see it.",
        "tokens": [
          51728,
          286,
          478,
          516,
          281,
          652,
          364,
          6713,
          1051,
          445,
          370,
          321,
          393,
          536,
          309,
          13,
          51848
        ]
      },
      {
        "avg_logprob": -0.25955850106698497,
        "compression_ratio": 1.6797153024911031,
        "end": 629.36,
        "id": 243,
        "no_speech_prob": 0.00007254319643834606,
        "seek": 62764,
        "start": 627.64,
        "temperature": 0,
        "text": " To be sure that this is really happening,",
        "tokens": [
          50364,
          1407,
          312,
          988,
          300,
          341,
          307,
          534,
          2737,
          11,
          50450
        ]
      },
      {
        "avg_logprob": -0.25955850106698497,
        "compression_ratio": 1.6797153024911031,
        "end": 632,
        "id": 244,
        "no_speech_prob": 0.00007254319643834606,
        "seek": 62764,
        "start": 629.36,
        "temperature": 0,
        "text": " though, let me put another console log here.",
        "tokens": [
          50450,
          1673,
          11,
          718,
          385,
          829,
          1071,
          11076,
          3565,
          510,
          13,
          50582
        ]
      },
      {
        "avg_logprob": -0.25955850106698497,
        "compression_ratio": 1.6797153024911031,
        "end": 635.6,
        "id": 245,
        "no_speech_prob": 0.00007254319643834606,
        "seek": 62764,
        "start": 632,
        "temperature": 0,
        "text": " This will be my own message, just error exclamation point.",
        "tokens": [
          50582,
          639,
          486,
          312,
          452,
          1065,
          3636,
          11,
          445,
          6713,
          1624,
          43233,
          935,
          13,
          50762
        ]
      },
      {
        "avg_logprob": -0.25955850106698497,
        "compression_ratio": 1.6797153024911031,
        "end": 638.36,
        "id": 246,
        "no_speech_prob": 0.00007254319643834606,
        "seek": 62764,
        "start": 635.6,
        "temperature": 0,
        "text": " And then what I'll do is let me misspell the name",
        "tokens": [
          50762,
          400,
          550,
          437,
          286,
          603,
          360,
          307,
          718,
          385,
          1713,
          49241,
          264,
          1315,
          50900
        ]
      },
      {
        "avg_logprob": -0.25955850106698497,
        "compression_ratio": 1.6797153024911031,
        "end": 640.24,
        "id": 247,
        "no_speech_prob": 0.00007254319643834606,
        "seek": 62764,
        "start": 638.36,
        "temperature": 0,
        "text": " of the ID of the DOM element.",
        "tokens": [
          50900,
          295,
          264,
          7348,
          295,
          264,
          35727,
          4478,
          13,
          50994
        ]
      },
      {
        "avg_logprob": -0.25955850106698497,
        "compression_ratio": 1.6797153024911031,
        "end": 643.52,
        "id": 248,
        "no_speech_prob": 0.00007254319643834606,
        "seek": 62764,
        "start": 640.24,
        "temperature": 0,
        "text": " I will write rainbow with two O's.",
        "tokens": [
          50994,
          286,
          486,
          2464,
          18526,
          365,
          732,
          422,
          311,
          13,
          51158
        ]
      },
      {
        "avg_logprob": -0.25955850106698497,
        "compression_ratio": 1.6797153024911031,
        "end": 645.6,
        "id": 249,
        "no_speech_prob": 0.00007254319643834606,
        "seek": 62764,
        "start": 643.52,
        "temperature": 0,
        "text": " And then now if I go back to the browser,",
        "tokens": [
          51158,
          400,
          550,
          586,
          498,
          286,
          352,
          646,
          281,
          264,
          11185,
          11,
          51262
        ]
      },
      {
        "avg_logprob": -0.25955850106698497,
        "compression_ratio": 1.6797153024911031,
        "end": 648.56,
        "id": 250,
        "no_speech_prob": 0.00007254319643834606,
        "seek": 62764,
        "start": 645.6,
        "temperature": 0,
        "text": " you're going to see my message error printed out.",
        "tokens": [
          51262,
          291,
          434,
          516,
          281,
          536,
          452,
          3636,
          6713,
          13567,
          484,
          13,
          51410
        ]
      },
      {
        "avg_logprob": -0.25955850106698497,
        "compression_ratio": 1.6797153024911031,
        "end": 651.28,
        "id": 251,
        "no_speech_prob": 0.00007254319643834606,
        "seek": 62764,
        "start": 648.56,
        "temperature": 0,
        "text": " And also, you can see the actual error messages there.",
        "tokens": [
          51410,
          400,
          611,
          11,
          291,
          393,
          536,
          264,
          3539,
          6713,
          7897,
          456,
          13,
          51546
        ]
      },
      {
        "avg_logprob": -0.25955850106698497,
        "compression_ratio": 1.6797153024911031,
        "end": 654.36,
        "id": 252,
        "no_speech_prob": 0.00007254319643834606,
        "seek": 62764,
        "start": 651.28,
        "temperature": 0,
        "text": " So this is a really nice way of handling errors.",
        "tokens": [
          51546,
          407,
          341,
          307,
          257,
          534,
          1481,
          636,
          295,
          13175,
          13603,
          13,
          51700
        ]
      },
      {
        "avg_logprob": -0.25955850106698497,
        "compression_ratio": 1.6797153024911031,
        "end": 655.24,
        "id": 253,
        "no_speech_prob": 0.00007254319643834606,
        "seek": 62764,
        "start": 654.36,
        "temperature": 0,
        "text": " One more thing.",
        "tokens": [
          51700,
          1485,
          544,
          551,
          13,
          51744
        ]
      },
      {
        "avg_logprob": -0.18845664623171784,
        "compression_ratio": 1.8132780082987552,
        "end": 658.28,
        "id": 254,
        "no_speech_prob": 0.00006014130849507637,
        "seek": 65524,
        "start": 655.24,
        "temperature": 0,
        "text": " Let's look at how I can condense the code",
        "tokens": [
          50364,
          961,
          311,
          574,
          412,
          577,
          286,
          393,
          2224,
          1288,
          264,
          3089,
          50516
        ]
      },
      {
        "avg_logprob": -0.18845664623171784,
        "compression_ratio": 1.8132780082987552,
        "end": 661.88,
        "id": 255,
        "no_speech_prob": 0.00006014130849507637,
        "seek": 65524,
        "start": 658.28,
        "temperature": 0,
        "text": " and make it a bit more readable using async and await.",
        "tokens": [
          50516,
          293,
          652,
          309,
          257,
          857,
          544,
          49857,
          1228,
          382,
          34015,
          293,
          19670,
          13,
          50696
        ]
      },
      {
        "avg_logprob": -0.18845664623171784,
        "compression_ratio": 1.8132780082987552,
        "end": 663.52,
        "id": 256,
        "no_speech_prob": 0.00006014130849507637,
        "seek": 65524,
        "start": 661.88,
        "temperature": 0,
        "text": " The way this works is if I'm going",
        "tokens": [
          50696,
          440,
          636,
          341,
          1985,
          307,
          498,
          286,
          478,
          516,
          50778
        ]
      },
      {
        "avg_logprob": -0.18845664623171784,
        "compression_ratio": 1.8132780082987552,
        "end": 666.76,
        "id": 257,
        "no_speech_prob": 0.00006014130849507637,
        "seek": 65524,
        "start": 663.52,
        "temperature": 0,
        "text": " to use this new keyword await, I can only",
        "tokens": [
          50778,
          281,
          764,
          341,
          777,
          20428,
          19670,
          11,
          286,
          393,
          787,
          50940
        ]
      },
      {
        "avg_logprob": -0.18845664623171784,
        "compression_ratio": 1.8132780082987552,
        "end": 671.2,
        "id": 258,
        "no_speech_prob": 0.00006014130849507637,
        "seek": 65524,
        "start": 666.76,
        "temperature": 0,
        "text": " use the keyword await in the context of an async function.",
        "tokens": [
          50940,
          764,
          264,
          20428,
          19670,
          294,
          264,
          4319,
          295,
          364,
          382,
          34015,
          2445,
          13,
          51162
        ]
      },
      {
        "avg_logprob": -0.18845664623171784,
        "compression_ratio": 1.8132780082987552,
        "end": 674.08,
        "id": 259,
        "no_speech_prob": 0.00006014130849507637,
        "seek": 65524,
        "start": 671.2,
        "temperature": 0,
        "text": " Async is a keyword that specifies this function is",
        "tokens": [
          51162,
          1018,
          34015,
          307,
          257,
          20428,
          300,
          1608,
          11221,
          341,
          2445,
          307,
          51306
        ]
      },
      {
        "avg_logprob": -0.18845664623171784,
        "compression_ratio": 1.8132780082987552,
        "end": 675.92,
        "id": 260,
        "no_speech_prob": 0.00006014130849507637,
        "seek": 65524,
        "start": 674.08,
        "temperature": 0,
        "text": " going to happen asynchronously.",
        "tokens": [
          51306,
          516,
          281,
          1051,
          42642,
          5098,
          13,
          51398
        ]
      },
      {
        "avg_logprob": -0.18845664623171784,
        "compression_ratio": 1.8132780082987552,
        "end": 678.84,
        "id": 261,
        "no_speech_prob": 0.00006014130849507637,
        "seek": 65524,
        "start": 675.92,
        "temperature": 0,
        "text": " So essentially, what I want to do is write a new function.",
        "tokens": [
          51398,
          407,
          4476,
          11,
          437,
          286,
          528,
          281,
          360,
          307,
          2464,
          257,
          777,
          2445,
          13,
          51544
        ]
      },
      {
        "avg_logprob": -0.18845664623171784,
        "compression_ratio": 1.8132780082987552,
        "end": 681.24,
        "id": 262,
        "no_speech_prob": 0.00006014130849507637,
        "seek": 65524,
        "start": 678.84,
        "temperature": 0,
        "text": " I'm going to call it async.",
        "tokens": [
          51544,
          286,
          478,
          516,
          281,
          818,
          309,
          382,
          34015,
          13,
          51664
        ]
      },
      {
        "avg_logprob": -0.18845664623171784,
        "compression_ratio": 1.8132780082987552,
        "end": 684.6,
        "id": 263,
        "no_speech_prob": 0.00006014130849507637,
        "seek": 65524,
        "start": 681.24,
        "temperature": 0,
        "text": " I'm going to call it catchRainbow.",
        "tokens": [
          51664,
          286,
          478,
          516,
          281,
          818,
          309,
          3745,
          49,
          491,
          8202,
          13,
          51832
        ]
      },
      {
        "avg_logprob": -0.1876088731429156,
        "compression_ratio": 1.7252747252747254,
        "end": 686.48,
        "id": 264,
        "no_speech_prob": 0.0003199964121449739,
        "seek": 68460,
        "start": 684.76,
        "temperature": 0,
        "text": " And I've got to declare it as a function.",
        "tokens": [
          50372,
          400,
          286,
          600,
          658,
          281,
          19710,
          309,
          382,
          257,
          2445,
          13,
          50458
        ]
      },
      {
        "avg_logprob": -0.1876088731429156,
        "compression_ratio": 1.7252747252747254,
        "end": 689.24,
        "id": 265,
        "no_speech_prob": 0.0003199964121449739,
        "seek": 68460,
        "start": 686.48,
        "temperature": 0,
        "text": " So I could use the keyword function or the arrow syntax.",
        "tokens": [
          50458,
          407,
          286,
          727,
          764,
          264,
          20428,
          2445,
          420,
          264,
          11610,
          28431,
          13,
          50596
        ]
      },
      {
        "avg_logprob": -0.1876088731429156,
        "compression_ratio": 1.7252747252747254,
        "end": 691.36,
        "id": 266,
        "no_speech_prob": 0.0003199964121449739,
        "seek": 68460,
        "start": 689.24,
        "temperature": 0,
        "text": " I'm kind of using them interchangeably here.",
        "tokens": [
          50596,
          286,
          478,
          733,
          295,
          1228,
          552,
          30358,
          1188,
          510,
          13,
          50702
        ]
      },
      {
        "avg_logprob": -0.1876088731429156,
        "compression_ratio": 1.7252747252747254,
        "end": 694.88,
        "id": 267,
        "no_speech_prob": 0.0003199964121449739,
        "seek": 68460,
        "start": 691.36,
        "temperature": 0,
        "text": " But I'm going to make an async function called catchRainbow.",
        "tokens": [
          50702,
          583,
          286,
          478,
          516,
          281,
          652,
          364,
          382,
          34015,
          2445,
          1219,
          3745,
          49,
          491,
          8202,
          13,
          50878
        ]
      },
      {
        "avg_logprob": -0.1876088731429156,
        "compression_ratio": 1.7252747252747254,
        "end": 698.84,
        "id": 268,
        "no_speech_prob": 0.0003199964121449739,
        "seek": 68460,
        "start": 694.88,
        "temperature": 0,
        "text": " Now when I call fetch, instead of having to use.then,",
        "tokens": [
          50878,
          823,
          562,
          286,
          818,
          23673,
          11,
          2602,
          295,
          1419,
          281,
          764,
          2411,
          19096,
          11,
          51076
        ]
      },
      {
        "avg_logprob": -0.1876088731429156,
        "compression_ratio": 1.7252747252747254,
        "end": 702.72,
        "id": 269,
        "no_speech_prob": 0.0003199964121449739,
        "seek": 68460,
        "start": 698.84,
        "temperature": 0,
        "text": " I can still say fetchRainbow.jpg.",
        "tokens": [
          51076,
          286,
          393,
          920,
          584,
          23673,
          49,
          491,
          8202,
          13,
          73,
          49861,
          13,
          51270
        ]
      },
      {
        "avg_logprob": -0.1876088731429156,
        "compression_ratio": 1.7252747252747254,
        "end": 705.72,
        "id": 270,
        "no_speech_prob": 0.0003199964121449739,
        "seek": 68460,
        "start": 702.72,
        "temperature": 0,
        "text": " I can set the result of the fetch function",
        "tokens": [
          51270,
          286,
          393,
          992,
          264,
          1874,
          295,
          264,
          23673,
          2445,
          51420
        ]
      },
      {
        "avg_logprob": -0.1876088731429156,
        "compression_ratio": 1.7252747252747254,
        "end": 708,
        "id": 271,
        "no_speech_prob": 0.0003199964121449739,
        "seek": 68460,
        "start": 705.72,
        "temperature": 0,
        "text": " to a variable called response.",
        "tokens": [
          51420,
          281,
          257,
          7006,
          1219,
          4134,
          13,
          51534
        ]
      },
      {
        "avg_logprob": -0.1876088731429156,
        "compression_ratio": 1.7252747252747254,
        "end": 710.9200000000001,
        "id": 272,
        "no_speech_prob": 0.0003199964121449739,
        "seek": 68460,
        "start": 708,
        "temperature": 0,
        "text": " So this is like the dream of how I like to write code.",
        "tokens": [
          51534,
          407,
          341,
          307,
          411,
          264,
          3055,
          295,
          577,
          286,
          411,
          281,
          2464,
          3089,
          13,
          51680
        ]
      },
      {
        "avg_logprob": -0.1876088731429156,
        "compression_ratio": 1.7252747252747254,
        "end": 713.8000000000001,
        "id": 273,
        "no_speech_prob": 0.0003199964121449739,
        "seek": 68460,
        "start": 710.9200000000001,
        "temperature": 0,
        "text": " I just want to say the response equals the result",
        "tokens": [
          51680,
          286,
          445,
          528,
          281,
          584,
          264,
          4134,
          6915,
          264,
          1874,
          51824
        ]
      },
      {
        "avg_logprob": -0.18256632995605468,
        "compression_ratio": 1.7755905511811023,
        "end": 715.4799999999999,
        "id": 274,
        "no_speech_prob": 0.00021654347074218094,
        "seek": 71380,
        "start": 713.8,
        "temperature": 0,
        "text": " of the fetch function.",
        "tokens": [
          50364,
          295,
          264,
          23673,
          2445,
          13,
          50448
        ]
      },
      {
        "avg_logprob": -0.18256632995605468,
        "compression_ratio": 1.7755905511811023,
        "end": 718.88,
        "id": 275,
        "no_speech_prob": 0.00021654347074218094,
        "seek": 71380,
        "start": 715.4799999999999,
        "temperature": 0,
        "text": " But this is where the await keyword comes in.",
        "tokens": [
          50448,
          583,
          341,
          307,
          689,
          264,
          19670,
          20428,
          1487,
          294,
          13,
          50618
        ]
      },
      {
        "avg_logprob": -0.18256632995605468,
        "compression_ratio": 1.7755905511811023,
        "end": 722,
        "id": 276,
        "no_speech_prob": 0.00021654347074218094,
        "seek": 71380,
        "start": 718.88,
        "temperature": 0,
        "text": " Because fetch is an asynchronous function,",
        "tokens": [
          50618,
          1436,
          23673,
          307,
          364,
          49174,
          2445,
          11,
          50774
        ]
      },
      {
        "avg_logprob": -0.18256632995605468,
        "compression_ratio": 1.7755905511811023,
        "end": 723.56,
        "id": 277,
        "no_speech_prob": 0.00021654347074218094,
        "seek": 71380,
        "start": 722,
        "temperature": 0,
        "text": " I have to add await.",
        "tokens": [
          50774,
          286,
          362,
          281,
          909,
          19670,
          13,
          50852
        ]
      },
      {
        "avg_logprob": -0.18256632995605468,
        "compression_ratio": 1.7755905511811023,
        "end": 725.9599999999999,
        "id": 278,
        "no_speech_prob": 0.00021654347074218094,
        "seek": 71380,
        "start": 723.56,
        "temperature": 0,
        "text": " So this is really just syntactic sugar.",
        "tokens": [
          50852,
          407,
          341,
          307,
          534,
          445,
          23980,
          19892,
          5076,
          13,
          50972
        ]
      },
      {
        "avg_logprob": -0.18256632995605468,
        "compression_ratio": 1.7755905511811023,
        "end": 729.24,
        "id": 279,
        "no_speech_prob": 0.00021654347074218094,
        "seek": 71380,
        "start": 725.9599999999999,
        "temperature": 0,
        "text": " It's the same exact thing that I was doing with.then.",
        "tokens": [
          50972,
          467,
          311,
          264,
          912,
          1900,
          551,
          300,
          286,
          390,
          884,
          365,
          2411,
          19096,
          13,
          51136
        ]
      },
      {
        "avg_logprob": -0.18256632995605468,
        "compression_ratio": 1.7755905511811023,
        "end": 731.8399999999999,
        "id": 280,
        "no_speech_prob": 0.00021654347074218094,
        "seek": 71380,
        "start": 729.24,
        "temperature": 0,
        "text": " But because I'm in an async function,",
        "tokens": [
          51136,
          583,
          570,
          286,
          478,
          294,
          364,
          382,
          34015,
          2445,
          11,
          51266
        ]
      },
      {
        "avg_logprob": -0.18256632995605468,
        "compression_ratio": 1.7755905511811023,
        "end": 734.92,
        "id": 281,
        "no_speech_prob": 0.00021654347074218094,
        "seek": 71380,
        "start": 731.8399999999999,
        "temperature": 0,
        "text": " I'm allowed to write the code in a single line",
        "tokens": [
          51266,
          286,
          478,
          4350,
          281,
          2464,
          264,
          3089,
          294,
          257,
          2167,
          1622,
          51420
        ]
      },
      {
        "avg_logprob": -0.18256632995605468,
        "compression_ratio": 1.7755905511811023,
        "end": 737.28,
        "id": 282,
        "no_speech_prob": 0.00021654347074218094,
        "seek": 71380,
        "start": 734.92,
        "temperature": 0,
        "text": " by saying await the result of fetch",
        "tokens": [
          51420,
          538,
          1566,
          19670,
          264,
          1874,
          295,
          23673,
          51538
        ]
      },
      {
        "avg_logprob": -0.18256632995605468,
        "compression_ratio": 1.7755905511811023,
        "end": 740.4799999999999,
        "id": 283,
        "no_speech_prob": 0.00021654347074218094,
        "seek": 71380,
        "start": 737.28,
        "temperature": 0,
        "text": " and store that result in this variable called response.",
        "tokens": [
          51538,
          293,
          3531,
          300,
          1874,
          294,
          341,
          7006,
          1219,
          4134,
          13,
          51698
        ]
      },
      {
        "avg_logprob": -0.18256632995605468,
        "compression_ratio": 1.7755905511811023,
        "end": 742.8399999999999,
        "id": 284,
        "no_speech_prob": 0.00021654347074218094,
        "seek": 71380,
        "start": 740.4799999999999,
        "temperature": 0,
        "text": " And then I can do the same thing with the blob.",
        "tokens": [
          51698,
          400,
          550,
          286,
          393,
          360,
          264,
          912,
          551,
          365,
          264,
          46115,
          13,
          51816
        ]
      },
      {
        "avg_logprob": -0.20573025984729795,
        "compression_ratio": 1.773109243697479,
        "end": 748.64,
        "id": 285,
        "no_speech_prob": 0.000012606889868038706,
        "seek": 74284,
        "start": 742.84,
        "temperature": 0,
        "text": " I can say const blob equals await response.blob.",
        "tokens": [
          50364,
          286,
          393,
          584,
          1817,
          46115,
          6915,
          19670,
          4134,
          13,
          15962,
          65,
          13,
          50654
        ]
      },
      {
        "avg_logprob": -0.20573025984729795,
        "compression_ratio": 1.773109243697479,
        "end": 751.9200000000001,
        "id": 286,
        "no_speech_prob": 0.000012606889868038706,
        "seek": 74284,
        "start": 748.64,
        "temperature": 0,
        "text": " And so now in two lines of code, I've said fetch.",
        "tokens": [
          50654,
          400,
          370,
          586,
          294,
          732,
          3876,
          295,
          3089,
          11,
          286,
          600,
          848,
          23673,
          13,
          50818
        ]
      },
      {
        "avg_logprob": -0.20573025984729795,
        "compression_ratio": 1.773109243697479,
        "end": 753,
        "id": 287,
        "no_speech_prob": 0.000012606889868038706,
        "seek": 74284,
        "start": 751.9200000000001,
        "temperature": 0,
        "text": " I've gotten a response.",
        "tokens": [
          50818,
          286,
          600,
          5768,
          257,
          4134,
          13,
          50872
        ]
      },
      {
        "avg_logprob": -0.20573025984729795,
        "compression_ratio": 1.773109243697479,
        "end": 754,
        "id": 288,
        "no_speech_prob": 0.000012606889868038706,
        "seek": 74284,
        "start": 753,
        "temperature": 0,
        "text": " I've said.blob.",
        "tokens": [
          50872,
          286,
          600,
          848,
          2411,
          15962,
          65,
          13,
          50922
        ]
      },
      {
        "avg_logprob": -0.20573025984729795,
        "compression_ratio": 1.773109243697479,
        "end": 755.08,
        "id": 289,
        "no_speech_prob": 0.000012606889868038706,
        "seek": 74284,
        "start": 754,
        "temperature": 0,
        "text": " I've gotten the blob.",
        "tokens": [
          50922,
          286,
          600,
          5768,
          264,
          46115,
          13,
          50976
        ]
      },
      {
        "avg_logprob": -0.20573025984729795,
        "compression_ratio": 1.773109243697479,
        "end": 757.5600000000001,
        "id": 290,
        "no_speech_prob": 0.000012606889868038706,
        "seek": 74284,
        "start": 755.08,
        "temperature": 0,
        "text": " And then this other line that I have down here,",
        "tokens": [
          50976,
          400,
          550,
          341,
          661,
          1622,
          300,
          286,
          362,
          760,
          510,
          11,
          51100
        ]
      },
      {
        "avg_logprob": -0.20573025984729795,
        "compression_ratio": 1.773109243697479,
        "end": 762.44,
        "id": 291,
        "no_speech_prob": 0.000012606889868038706,
        "seek": 74284,
        "start": 757.5600000000001,
        "temperature": 0,
        "text": " which puts the blob as a URL into the DOM element,",
        "tokens": [
          51100,
          597,
          8137,
          264,
          46115,
          382,
          257,
          12905,
          666,
          264,
          35727,
          4478,
          11,
          51344
        ]
      },
      {
        "avg_logprob": -0.20573025984729795,
        "compression_ratio": 1.773109243697479,
        "end": 764.88,
        "id": 292,
        "no_speech_prob": 0.000012606889868038706,
        "seek": 74284,
        "start": 762.44,
        "temperature": 0,
        "text": " I can just take that and put that up there.",
        "tokens": [
          51344,
          286,
          393,
          445,
          747,
          300,
          293,
          829,
          300,
          493,
          456,
          13,
          51466
        ]
      },
      {
        "avg_logprob": -0.20573025984729795,
        "compression_ratio": 1.773109243697479,
        "end": 768.52,
        "id": 293,
        "no_speech_prob": 0.000012606889868038706,
        "seek": 74284,
        "start": 764.88,
        "temperature": 0,
        "text": " And you see all of that, all of those lines of code of.then,",
        "tokens": [
          51466,
          400,
          291,
          536,
          439,
          295,
          300,
          11,
          439,
          295,
          729,
          3876,
          295,
          3089,
          295,
          2411,
          19096,
          11,
          51648
        ]
      },
      {
        "avg_logprob": -0.20573025984729795,
        "compression_ratio": 1.773109243697479,
        "end": 769.64,
        "id": 294,
        "no_speech_prob": 0.000012606889868038706,
        "seek": 74284,
        "start": 768.52,
        "temperature": 0,
        "text": ".then,.then.",
        "tokens": [
          51648,
          2411,
          19096,
          11,
          2411,
          19096,
          13,
          51704
        ]
      },
      {
        "avg_logprob": -0.20573025984729795,
        "compression_ratio": 1.773109243697479,
        "end": 771.5600000000001,
        "id": 295,
        "no_speech_prob": 0.000012606889868038706,
        "seek": 74284,
        "start": 769.64,
        "temperature": 0,
        "text": " And I'm going to keep this catch, by the way,",
        "tokens": [
          51704,
          400,
          286,
          478,
          516,
          281,
          1066,
          341,
          3745,
          11,
          538,
          264,
          636,
          11,
          51800
        ]
      },
      {
        "avg_logprob": -0.23438518959403837,
        "compression_ratio": 1.7132616487455197,
        "end": 773.4399999999999,
        "id": 296,
        "no_speech_prob": 0.00010071378346765414,
        "seek": 77156,
        "start": 771.56,
        "temperature": 0,
        "text": " because I'm going to need this for something.",
        "tokens": [
          50364,
          570,
          286,
          478,
          516,
          281,
          643,
          341,
          337,
          746,
          13,
          50458
        ]
      },
      {
        "avg_logprob": -0.23438518959403837,
        "compression_ratio": 1.7132616487455197,
        "end": 774.5999999999999,
        "id": 297,
        "no_speech_prob": 0.00010071378346765414,
        "seek": 77156,
        "start": 773.4399999999999,
        "temperature": 0,
        "text": " I'll show you in a second.",
        "tokens": [
          50458,
          286,
          603,
          855,
          291,
          294,
          257,
          1150,
          13,
          50516
        ]
      },
      {
        "avg_logprob": -0.23438518959403837,
        "compression_ratio": 1.7132616487455197,
        "end": 778.8,
        "id": 298,
        "no_speech_prob": 0.00010071378346765414,
        "seek": 77156,
        "start": 774.5999999999999,
        "temperature": 0,
        "text": " All of this gets to go away.",
        "tokens": [
          50516,
          1057,
          295,
          341,
          2170,
          281,
          352,
          1314,
          13,
          50726
        ]
      },
      {
        "avg_logprob": -0.23438518959403837,
        "compression_ratio": 1.7132616487455197,
        "end": 779.7199999999999,
        "id": 299,
        "no_speech_prob": 0.00010071378346765414,
        "seek": 77156,
        "start": 778.8,
        "temperature": 0,
        "text": " Now, look at that.",
        "tokens": [
          50726,
          823,
          11,
          574,
          412,
          300,
          13,
          50772
        ]
      },
      {
        "avg_logprob": -0.23438518959403837,
        "compression_ratio": 1.7132616487455197,
        "end": 780.9599999999999,
        "id": 300,
        "no_speech_prob": 0.00010071378346765414,
        "seek": 77156,
        "start": 779.7199999999999,
        "temperature": 0,
        "text": " It's just three lines of code.",
        "tokens": [
          50772,
          467,
          311,
          445,
          1045,
          3876,
          295,
          3089,
          13,
          50834
        ]
      },
      {
        "avg_logprob": -0.23438518959403837,
        "compression_ratio": 1.7132616487455197,
        "end": 783.76,
        "id": 301,
        "no_speech_prob": 0.00010071378346765414,
        "seek": 77156,
        "start": 780.9599999999999,
        "temperature": 0,
        "text": " And all I have to do now is say, catch rainbow.",
        "tokens": [
          50834,
          400,
          439,
          286,
          362,
          281,
          360,
          586,
          307,
          584,
          11,
          3745,
          18526,
          13,
          50974
        ]
      },
      {
        "avg_logprob": -0.23438518959403837,
        "compression_ratio": 1.7132616487455197,
        "end": 785.1999999999999,
        "id": 302,
        "no_speech_prob": 0.00010071378346765414,
        "seek": 77156,
        "start": 783.76,
        "temperature": 0,
        "text": " Just call that function.",
        "tokens": [
          50974,
          1449,
          818,
          300,
          2445,
          13,
          51046
        ]
      },
      {
        "avg_logprob": -0.23438518959403837,
        "compression_ratio": 1.7132616487455197,
        "end": 787.4399999999999,
        "id": 303,
        "no_speech_prob": 0.00010071378346765414,
        "seek": 77156,
        "start": 785.1999999999999,
        "temperature": 0,
        "text": " So I put all the stuff that's asynchronous",
        "tokens": [
          51046,
          407,
          286,
          829,
          439,
          264,
          1507,
          300,
          311,
          49174,
          51158
        ]
      },
      {
        "avg_logprob": -0.23438518959403837,
        "compression_ratio": 1.7132616487455197,
        "end": 789.3599999999999,
        "id": 304,
        "no_speech_prob": 0.00010071378346765414,
        "seek": 77156,
        "start": 787.4399999999999,
        "temperature": 0,
        "text": " inside a function that's async, and then just",
        "tokens": [
          51158,
          1854,
          257,
          2445,
          300,
          311,
          382,
          34015,
          11,
          293,
          550,
          445,
          51254
        ]
      },
      {
        "avg_logprob": -0.23438518959403837,
        "compression_ratio": 1.7132616487455197,
        "end": 790.56,
        "id": 305,
        "no_speech_prob": 0.00010071378346765414,
        "seek": 77156,
        "start": 789.3599999999999,
        "temperature": 0,
        "text": " call that function.",
        "tokens": [
          51254,
          818,
          300,
          2445,
          13,
          51314
        ]
      },
      {
        "avg_logprob": -0.23438518959403837,
        "compression_ratio": 1.7132616487455197,
        "end": 792.76,
        "id": 306,
        "no_speech_prob": 0.00010071378346765414,
        "seek": 77156,
        "start": 790.56,
        "temperature": 0,
        "text": " And now if we go over here, look, it's still working.",
        "tokens": [
          51314,
          400,
          586,
          498,
          321,
          352,
          670,
          510,
          11,
          574,
          11,
          309,
          311,
          920,
          1364,
          13,
          51424
        ]
      },
      {
        "avg_logprob": -0.23438518959403837,
        "compression_ratio": 1.7132616487455197,
        "end": 794.04,
        "id": 307,
        "no_speech_prob": 0.00010071378346765414,
        "seek": 77156,
        "start": 792.76,
        "temperature": 0,
        "text": " There's our rainbow.",
        "tokens": [
          51424,
          821,
          311,
          527,
          18526,
          13,
          51488
        ]
      },
      {
        "avg_logprob": -0.23438518959403837,
        "compression_ratio": 1.7132616487455197,
        "end": 796.3599999999999,
        "id": 308,
        "no_speech_prob": 0.00010071378346765414,
        "seek": 77156,
        "start": 794.04,
        "temperature": 0,
        "text": " But the reason why I kept that is",
        "tokens": [
          51488,
          583,
          264,
          1778,
          983,
          286,
          4305,
          300,
          307,
          51604
        ]
      },
      {
        "avg_logprob": -0.23438518959403837,
        "compression_ratio": 1.7132616487455197,
        "end": 798.4799999999999,
        "id": 309,
        "no_speech_prob": 0.00010071378346765414,
        "seek": 77156,
        "start": 796.3599999999999,
        "temperature": 0,
        "text": " I do want to keep this catch thing,",
        "tokens": [
          51604,
          286,
          360,
          528,
          281,
          1066,
          341,
          3745,
          551,
          11,
          51710
        ]
      },
      {
        "avg_logprob": -0.21304790790264422,
        "compression_ratio": 1.679245283018868,
        "end": 802.76,
        "id": 310,
        "no_speech_prob": 0.000022474039724329486,
        "seek": 79848,
        "start": 798.48,
        "temperature": 0,
        "text": " because if there is an error, let me once again",
        "tokens": [
          50364,
          570,
          498,
          456,
          307,
          364,
          6713,
          11,
          718,
          385,
          1564,
          797,
          50578
        ]
      },
      {
        "avg_logprob": -0.21304790790264422,
        "compression_ratio": 1.679245283018868,
        "end": 806.4,
        "id": 311,
        "no_speech_prob": 0.000022474039724329486,
        "seek": 79848,
        "start": 802.76,
        "temperature": 0,
        "text": " misspell that name of the DOM element,",
        "tokens": [
          50578,
          1713,
          49241,
          300,
          1315,
          295,
          264,
          35727,
          4478,
          11,
          50760
        ]
      },
      {
        "avg_logprob": -0.21304790790264422,
        "compression_ratio": 1.679245283018868,
        "end": 810.44,
        "id": 312,
        "no_speech_prob": 0.000022474039724329486,
        "seek": 79848,
        "start": 806.4,
        "temperature": 0,
        "text": " we can see I have now handled that error.",
        "tokens": [
          50760,
          321,
          393,
          536,
          286,
          362,
          586,
          18033,
          300,
          6713,
          13,
          50962
        ]
      },
      {
        "avg_logprob": -0.21304790790264422,
        "compression_ratio": 1.679245283018868,
        "end": 813.88,
        "id": 313,
        "no_speech_prob": 0.000022474039724329486,
        "seek": 79848,
        "start": 810.44,
        "temperature": 0,
        "text": " I can attach the error to the async function",
        "tokens": [
          50962,
          286,
          393,
          5085,
          264,
          6713,
          281,
          264,
          382,
          34015,
          2445,
          51134
        ]
      },
      {
        "avg_logprob": -0.21304790790264422,
        "compression_ratio": 1.679245283018868,
        "end": 814.84,
        "id": 314,
        "no_speech_prob": 0.000022474039724329486,
        "seek": 79848,
        "start": 813.88,
        "temperature": 0,
        "text": " that I've written.",
        "tokens": [
          51134,
          300,
          286,
          600,
          3720,
          13,
          51182
        ]
      },
      {
        "avg_logprob": -0.21304790790264422,
        "compression_ratio": 1.679245283018868,
        "end": 818.24,
        "id": 315,
        "no_speech_prob": 0.000022474039724329486,
        "seek": 79848,
        "start": 814.84,
        "temperature": 0,
        "text": " And in fact, that function, by definition,",
        "tokens": [
          51182,
          400,
          294,
          1186,
          11,
          300,
          2445,
          11,
          538,
          7123,
          11,
          51352
        ]
      },
      {
        "avg_logprob": -0.21304790790264422,
        "compression_ratio": 1.679245283018868,
        "end": 821.32,
        "id": 316,
        "no_speech_prob": 0.000022474039724329486,
        "seek": 79848,
        "start": 818.24,
        "temperature": 0,
        "text": " an async function, by definition, returns a promise.",
        "tokens": [
          51352,
          364,
          382,
          34015,
          2445,
          11,
          538,
          7123,
          11,
          11247,
          257,
          6228,
          13,
          51506
        ]
      },
      {
        "avg_logprob": -0.21304790790264422,
        "compression_ratio": 1.679245283018868,
        "end": 823.2,
        "id": 317,
        "no_speech_prob": 0.000022474039724329486,
        "seek": 79848,
        "start": 821.32,
        "temperature": 0,
        "text": " So I can also say.then.",
        "tokens": [
          51506,
          407,
          286,
          393,
          611,
          584,
          2411,
          19096,
          13,
          51600
        ]
      },
      {
        "avg_logprob": -0.21304790790264422,
        "compression_ratio": 1.679245283018868,
        "end": 828.2,
        "id": 318,
        "no_speech_prob": 0.000022474039724329486,
        "seek": 79848,
        "start": 823.2,
        "temperature": 0,
        "text": " And if I wanted to do something to indicate",
        "tokens": [
          51600,
          400,
          498,
          286,
          1415,
          281,
          360,
          746,
          281,
          13330,
          51850
        ]
      },
      {
        "avg_logprob": -0.23573533544000588,
        "compression_ratio": 1.5069767441860464,
        "end": 833.5600000000001,
        "id": 319,
        "no_speech_prob": 0.00001952582533704117,
        "seek": 82820,
        "start": 828.2,
        "temperature": 0,
        "text": " that it's done, I could say, for example, console.log.yay.",
        "tokens": [
          50364,
          300,
          309,
          311,
          1096,
          11,
          286,
          727,
          584,
          11,
          337,
          1365,
          11,
          11076,
          13,
          4987,
          13,
          88,
          320,
          13,
          50632
        ]
      },
      {
        "avg_logprob": -0.23573533544000588,
        "compression_ratio": 1.5069767441860464,
        "end": 835.0400000000001,
        "id": 320,
        "no_speech_prob": 0.00001952582533704117,
        "seek": 82820,
        "start": 833.5600000000001,
        "temperature": 0,
        "text": " And now, oops.",
        "tokens": [
          50632,
          400,
          586,
          11,
          34166,
          13,
          50706
        ]
      },
      {
        "avg_logprob": -0.23573533544000588,
        "compression_ratio": 1.5069767441860464,
        "end": 837.72,
        "id": 321,
        "no_speech_prob": 0.00001952582533704117,
        "seek": 82820,
        "start": 835.0400000000001,
        "temperature": 0,
        "text": " Oh, I've got to spell rainbow correctly.",
        "tokens": [
          50706,
          876,
          11,
          286,
          600,
          658,
          281,
          9827,
          18526,
          8944,
          13,
          50840
        ]
      },
      {
        "avg_logprob": -0.23573533544000588,
        "compression_ratio": 1.5069767441860464,
        "end": 839.1600000000001,
        "id": 322,
        "no_speech_prob": 0.00001952582533704117,
        "seek": 82820,
        "start": 837.72,
        "temperature": 0,
        "text": " And there we go.",
        "tokens": [
          50840,
          400,
          456,
          321,
          352,
          13,
          50912
        ]
      },
      {
        "avg_logprob": -0.23573533544000588,
        "compression_ratio": 1.5069767441860464,
        "end": 841.0400000000001,
        "id": 323,
        "no_speech_prob": 0.00001952582533704117,
        "seek": 82820,
        "start": 839.1600000000001,
        "temperature": 0,
        "text": " We can see console.log.yay.",
        "tokens": [
          50912,
          492,
          393,
          536,
          11076,
          13,
          4987,
          13,
          88,
          320,
          13,
          51006
        ]
      },
      {
        "avg_logprob": -0.23573533544000588,
        "compression_ratio": 1.5069767441860464,
        "end": 843.6400000000001,
        "id": 324,
        "no_speech_prob": 0.00001952582533704117,
        "seek": 82820,
        "start": 841.0400000000001,
        "temperature": 0,
        "text": " So now I have the whole thing.",
        "tokens": [
          51006,
          407,
          586,
          286,
          362,
          264,
          1379,
          551,
          13,
          51136
        ]
      },
      {
        "avg_logprob": -0.23573533544000588,
        "compression_ratio": 1.5069767441860464,
        "end": 849.44,
        "id": 325,
        "no_speech_prob": 0.00001952582533704117,
        "seek": 82820,
        "start": 843.6400000000001,
        "temperature": 0,
        "text": " I have written an async function that makes the fetch request.",
        "tokens": [
          51136,
          286,
          362,
          3720,
          364,
          382,
          34015,
          2445,
          300,
          1669,
          264,
          23673,
          5308,
          13,
          51426
        ]
      },
      {
        "avg_logprob": -0.23573533544000588,
        "compression_ratio": 1.5069767441860464,
        "end": 854.4200000000001,
        "id": 326,
        "no_speech_prob": 0.00001952582533704117,
        "seek": 82820,
        "start": 849.44,
        "temperature": 0,
        "text": " It turns the body of what comes back in the HTTP response",
        "tokens": [
          51426,
          467,
          4523,
          264,
          1772,
          295,
          437,
          1487,
          646,
          294,
          264,
          33283,
          4134,
          51675
        ]
      },
      {
        "avg_logprob": -0.23573533544000588,
        "compression_ratio": 1.5069767441860464,
        "end": 855.9200000000001,
        "id": 327,
        "no_speech_prob": 0.00001952582533704117,
        "seek": 82820,
        "start": 854.4200000000001,
        "temperature": 0,
        "text": " into a blob.",
        "tokens": [
          51675,
          666,
          257,
          46115,
          13,
          51750
        ]
      },
      {
        "avg_logprob": -0.1967896069485717,
        "compression_ratio": 1.6813880126182965,
        "end": 857.8,
        "id": 328,
        "no_speech_prob": 0.00011774421000154689,
        "seek": 85592,
        "start": 855.92,
        "temperature": 0,
        "text": " It converts that blob into the format",
        "tokens": [
          50364,
          467,
          38874,
          300,
          46115,
          666,
          264,
          7877,
          50458
        ]
      },
      {
        "avg_logprob": -0.1967896069485717,
        "compression_ratio": 1.6813880126182965,
        "end": 859.68,
        "id": 329,
        "no_speech_prob": 0.00011774421000154689,
        "seek": 85592,
        "start": 857.8,
        "temperature": 0,
        "text": " that the DOM element expects.",
        "tokens": [
          50458,
          300,
          264,
          35727,
          4478,
          33280,
          13,
          50552
        ]
      },
      {
        "avg_logprob": -0.1967896069485717,
        "compression_ratio": 1.6813880126182965,
        "end": 862.52,
        "id": 330,
        "no_speech_prob": 0.00011774421000154689,
        "seek": 85592,
        "start": 859.68,
        "temperature": 0,
        "text": " And I've put a little message saying, yay, it worked,",
        "tokens": [
          50552,
          400,
          286,
          600,
          829,
          257,
          707,
          3636,
          1566,
          11,
          23986,
          11,
          309,
          2732,
          11,
          50694
        ]
      },
      {
        "avg_logprob": -0.1967896069485717,
        "compression_ratio": 1.6813880126182965,
        "end": 864.36,
        "id": 331,
        "no_speech_prob": 0.00011774421000154689,
        "seek": 85592,
        "start": 862.52,
        "temperature": 0,
        "text": " or an error if there's an error.",
        "tokens": [
          50694,
          420,
          364,
          6713,
          498,
          456,
          311,
          364,
          6713,
          13,
          50786
        ]
      },
      {
        "avg_logprob": -0.1967896069485717,
        "compression_ratio": 1.6813880126182965,
        "end": 865,
        "id": 332,
        "no_speech_prob": 0.00011774421000154689,
        "seek": 85592,
        "start": 864.36,
        "temperature": 0,
        "text": " And that's it.",
        "tokens": [
          50786,
          400,
          300,
          311,
          309,
          13,
          50818
        ]
      },
      {
        "avg_logprob": -0.1967896069485717,
        "compression_ratio": 1.6813880126182965,
        "end": 866.8399999999999,
        "id": 333,
        "no_speech_prob": 0.00011774421000154689,
        "seek": 85592,
        "start": 865,
        "temperature": 0,
        "text": " That's this full example.",
        "tokens": [
          50818,
          663,
          311,
          341,
          1577,
          1365,
          13,
          50910
        ]
      },
      {
        "avg_logprob": -0.1967896069485717,
        "compression_ratio": 1.6813880126182965,
        "end": 869.24,
        "id": 334,
        "no_speech_prob": 0.00011774421000154689,
        "seek": 85592,
        "start": 866.8399999999999,
        "temperature": 0,
        "text": " Now that we know how to use fetch in the generic sense,",
        "tokens": [
          50910,
          823,
          300,
          321,
          458,
          577,
          281,
          764,
          23673,
          294,
          264,
          19577,
          2020,
          11,
          51030
        ]
      },
      {
        "avg_logprob": -0.1967896069485717,
        "compression_ratio": 1.6813880126182965,
        "end": 871.64,
        "id": 335,
        "no_speech_prob": 0.00011774421000154689,
        "seek": 85592,
        "start": 869.24,
        "temperature": 0,
        "text": " and we're working with an image here, in the next video,",
        "tokens": [
          51030,
          293,
          321,
          434,
          1364,
          365,
          364,
          3256,
          510,
          11,
          294,
          264,
          958,
          960,
          11,
          51150
        ]
      },
      {
        "avg_logprob": -0.1967896069485717,
        "compression_ratio": 1.6813880126182965,
        "end": 873.9,
        "id": 336,
        "no_speech_prob": 0.00011774421000154689,
        "seek": 85592,
        "start": 871.64,
        "temperature": 0,
        "text": " I'm going to use fetch with a data set.",
        "tokens": [
          51150,
          286,
          478,
          516,
          281,
          764,
          23673,
          365,
          257,
          1412,
          992,
          13,
          51263
        ]
      },
      {
        "avg_logprob": -0.1967896069485717,
        "compression_ratio": 1.6813880126182965,
        "end": 876.8199999999999,
        "id": 337,
        "no_speech_prob": 0.00011774421000154689,
        "seek": 85592,
        "start": 873.9,
        "temperature": 0,
        "text": " I'm going to grab a comma-separated value",
        "tokens": [
          51263,
          286,
          478,
          516,
          281,
          4444,
          257,
          22117,
          12,
          405,
          2181,
          770,
          2158,
          51409
        ]
      },
      {
        "avg_logprob": -0.1967896069485717,
        "compression_ratio": 1.6813880126182965,
        "end": 879.16,
        "id": 338,
        "no_speech_prob": 0.00011774421000154689,
        "seek": 85592,
        "start": 876.8199999999999,
        "temperature": 0,
        "text": " file, a CSV file.",
        "tokens": [
          51409,
          3991,
          11,
          257,
          48814,
          3991,
          13,
          51526
        ]
      },
      {
        "avg_logprob": -0.1967896069485717,
        "compression_ratio": 1.6813880126182965,
        "end": 880.56,
        "id": 339,
        "no_speech_prob": 0.00011774421000154689,
        "seek": 85592,
        "start": 879.16,
        "temperature": 0,
        "text": " This is a file from NASA.",
        "tokens": [
          51526,
          639,
          307,
          257,
          3991,
          490,
          12077,
          13,
          51596
        ]
      },
      {
        "avg_logprob": -0.1967896069485717,
        "compression_ratio": 1.6813880126182965,
        "end": 884,
        "id": 340,
        "no_speech_prob": 0.00011774421000154689,
        "seek": 85592,
        "start": 880.56,
        "temperature": 0,
        "text": " It's global world temperatures from 1880 to present.",
        "tokens": [
          51596,
          467,
          311,
          4338,
          1002,
          12633,
          490,
          2443,
          4702,
          281,
          1974,
          13,
          51768
        ]
      },
      {
        "avg_logprob": -0.1967896069485717,
        "compression_ratio": 1.6813880126182965,
        "end": 885.8399999999999,
        "id": 341,
        "no_speech_prob": 0.00011774421000154689,
        "seek": 85592,
        "start": 884,
        "temperature": 0,
        "text": " And I'm going to show you how to graph those.",
        "tokens": [
          51768,
          400,
          286,
          478,
          516,
          281,
          855,
          291,
          577,
          281,
          4295,
          729,
          13,
          51860
        ]
      },
      {
        "avg_logprob": -0.21876930608981993,
        "compression_ratio": 1.829192546583851,
        "end": 888.84,
        "id": 342,
        "no_speech_prob": 0.0003569666878320277,
        "seek": 88584,
        "start": 886.2800000000001,
        "temperature": 0,
        "text": " But before we get there, I might give you a little exercise.",
        "tokens": [
          50386,
          583,
          949,
          321,
          483,
          456,
          11,
          286,
          1062,
          976,
          291,
          257,
          707,
          5380,
          13,
          50514
        ]
      },
      {
        "avg_logprob": -0.21876930608981993,
        "compression_ratio": 1.829192546583851,
        "end": 890.08,
        "id": 343,
        "no_speech_prob": 0.0003569666878320277,
        "seek": 88584,
        "start": 888.84,
        "temperature": 0,
        "text": " You can just move on to the next video",
        "tokens": [
          50514,
          509,
          393,
          445,
          1286,
          322,
          281,
          264,
          958,
          960,
          50576
        ]
      },
      {
        "avg_logprob": -0.21876930608981993,
        "compression_ratio": 1.829192546583851,
        "end": 891.4,
        "id": 344,
        "no_speech_prob": 0.0003569666878320277,
        "seek": 88584,
        "start": 890.08,
        "temperature": 0,
        "text": " to see actually working with the data",
        "tokens": [
          50576,
          281,
          536,
          767,
          1364,
          365,
          264,
          1412,
          50642
        ]
      },
      {
        "avg_logprob": -0.21876930608981993,
        "compression_ratio": 1.829192546583851,
        "end": 892.88,
        "id": 345,
        "no_speech_prob": 0.0003569666878320277,
        "seek": 88584,
        "start": 891.4,
        "temperature": 0,
        "text": " and how you might chart data file",
        "tokens": [
          50642,
          293,
          577,
          291,
          1062,
          6927,
          1412,
          3991,
          50716
        ]
      },
      {
        "avg_logprob": -0.21876930608981993,
        "compression_ratio": 1.829192546583851,
        "end": 893.96,
        "id": 346,
        "no_speech_prob": 0.0003569666878320277,
        "seek": 88584,
        "start": 892.88,
        "temperature": 0,
        "text": " and how you might chart that.",
        "tokens": [
          50716,
          293,
          577,
          291,
          1062,
          6927,
          300,
          13,
          50770
        ]
      },
      {
        "avg_logprob": -0.21876930608981993,
        "compression_ratio": 1.829192546583851,
        "end": 896.1600000000001,
        "id": 347,
        "no_speech_prob": 0.0003569666878320277,
        "seek": 88584,
        "start": 893.96,
        "temperature": 0,
        "text": " But as a little exercise, you might try some things.",
        "tokens": [
          50770,
          583,
          382,
          257,
          707,
          5380,
          11,
          291,
          1062,
          853,
          512,
          721,
          13,
          50880
        ]
      },
      {
        "avg_logprob": -0.21876930608981993,
        "compression_ratio": 1.829192546583851,
        "end": 899.6,
        "id": 348,
        "no_speech_prob": 0.0003569666878320277,
        "seek": 88584,
        "start": 896.1600000000001,
        "temperature": 0,
        "text": " For example, could you fetch more than one image?",
        "tokens": [
          50880,
          1171,
          1365,
          11,
          727,
          291,
          23673,
          544,
          813,
          472,
          3256,
          30,
          51052
        ]
      },
      {
        "avg_logprob": -0.21876930608981993,
        "compression_ratio": 1.829192546583851,
        "end": 900.12,
        "id": 349,
        "no_speech_prob": 0.0003569666878320277,
        "seek": 88584,
        "start": 899.6,
        "temperature": 0,
        "text": " How would you?",
        "tokens": [
          51052,
          1012,
          576,
          291,
          30,
          51078
        ]
      },
      {
        "avg_logprob": -0.21876930608981993,
        "compression_ratio": 1.829192546583851,
        "end": 902.44,
        "id": 350,
        "no_speech_prob": 0.0003569666878320277,
        "seek": 88584,
        "start": 900.12,
        "temperature": 0,
        "text": " What if you had an array of images?",
        "tokens": [
          51078,
          708,
          498,
          291,
          632,
          364,
          10225,
          295,
          5267,
          30,
          51194
        ]
      },
      {
        "avg_logprob": -0.21876930608981993,
        "compression_ratio": 1.829192546583851,
        "end": 905.0400000000001,
        "id": 351,
        "no_speech_prob": 0.0003569666878320277,
        "seek": 88584,
        "start": 902.44,
        "temperature": 0,
        "text": " What if your file was not an image file,",
        "tokens": [
          51194,
          708,
          498,
          428,
          3991,
          390,
          406,
          364,
          3256,
          3991,
          11,
          51324
        ]
      },
      {
        "avg_logprob": -0.21876930608981993,
        "compression_ratio": 1.829192546583851,
        "end": 907.48,
        "id": 352,
        "no_speech_prob": 0.0003569666878320277,
        "seek": 88584,
        "start": 905.0400000000001,
        "temperature": 0,
        "text": " but maybe it was just a plain text file?",
        "tokens": [
          51324,
          457,
          1310,
          309,
          390,
          445,
          257,
          11121,
          2487,
          3991,
          30,
          51446
        ]
      },
      {
        "avg_logprob": -0.21876930608981993,
        "compression_ratio": 1.829192546583851,
        "end": 909.8000000000001,
        "id": 353,
        "no_speech_prob": 0.0003569666878320277,
        "seek": 88584,
        "start": 907.48,
        "temperature": 0,
        "text": " Could you put the text that was in that file",
        "tokens": [
          51446,
          7497,
          291,
          829,
          264,
          2487,
          300,
          390,
          294,
          300,
          3991,
          51562
        ]
      },
      {
        "avg_logprob": -0.21876930608981993,
        "compression_ratio": 1.829192546583851,
        "end": 911.12,
        "id": 354,
        "no_speech_prob": 0.0003569666878320277,
        "seek": 88584,
        "start": 909.8000000000001,
        "temperature": 0,
        "text": " in a paragraph element?",
        "tokens": [
          51562,
          294,
          257,
          18865,
          4478,
          30,
          51628
        ]
      },
      {
        "avg_logprob": -0.21876930608981993,
        "compression_ratio": 1.829192546583851,
        "end": 913.1,
        "id": 355,
        "no_speech_prob": 0.0003569666878320277,
        "seek": 88584,
        "start": 911.12,
        "temperature": 0,
        "text": " So try one or two of those exercises.",
        "tokens": [
          51628,
          407,
          853,
          472,
          420,
          732,
          295,
          729,
          11900,
          13,
          51727
        ]
      },
      {
        "avg_logprob": -0.21876930608981993,
        "compression_ratio": 1.829192546583851,
        "end": 915.72,
        "id": 356,
        "no_speech_prob": 0.0003569666878320277,
        "seek": 88584,
        "start": 913.1,
        "temperature": 0,
        "text": " I will also include in the GitHub repository",
        "tokens": [
          51727,
          286,
          486,
          611,
          4090,
          294,
          264,
          23331,
          25841,
          51858
        ]
      },
      {
        "avg_logprob": -0.32397519217597115,
        "compression_ratio": 1.5057471264367817,
        "end": 918.36,
        "id": 357,
        "no_speech_prob": 0.0004878536856267601,
        "seek": 91572,
        "start": 916.6,
        "temperature": 0,
        "text": " that's associated with this video series,",
        "tokens": [
          50408,
          300,
          311,
          6615,
          365,
          341,
          960,
          2638,
          11,
          50496
        ]
      },
      {
        "avg_logprob": -0.32397519217597115,
        "compression_ratio": 1.5057471264367817,
        "end": 920.32,
        "id": 358,
        "no_speech_prob": 0.0004878536856267601,
        "seek": 91572,
        "start": 918.36,
        "temperature": 0,
        "text": " I will include solutions to those two exercises.",
        "tokens": [
          50496,
          286,
          486,
          4090,
          6547,
          281,
          729,
          732,
          11900,
          13,
          50594
        ]
      },
      {
        "avg_logprob": -0.32397519217597115,
        "compression_ratio": 1.5057471264367817,
        "end": 923.1600000000001,
        "id": 359,
        "no_speech_prob": 0.0004878536856267601,
        "seek": 91572,
        "start": 920.32,
        "temperature": 0,
        "text": " So try multiple images, try a text file,",
        "tokens": [
          50594,
          407,
          853,
          3866,
          5267,
          11,
          853,
          257,
          2487,
          3991,
          11,
          50736
        ]
      },
      {
        "avg_logprob": -0.32397519217597115,
        "compression_ratio": 1.5057471264367817,
        "end": 925.88,
        "id": 360,
        "no_speech_prob": 0.0004878536856267601,
        "seek": 91572,
        "start": 923.1600000000001,
        "temperature": 0,
        "text": " and see if you can get those to appear in your web page.",
        "tokens": [
          50736,
          293,
          536,
          498,
          291,
          393,
          483,
          729,
          281,
          4204,
          294,
          428,
          3670,
          3028,
          13,
          50872
        ]
      },
      {
        "avg_logprob": -0.32397519217597115,
        "compression_ratio": 1.5057471264367817,
        "end": 927.48,
        "id": 361,
        "no_speech_prob": 0.0004878536856267601,
        "seek": 91572,
        "start": 925.88,
        "temperature": 0,
        "text": " So thanks for watching this tutorial,",
        "tokens": [
          50872,
          407,
          3231,
          337,
          1976,
          341,
          7073,
          11,
          50952
        ]
      },
      {
        "avg_logprob": -0.32397519217597115,
        "compression_ratio": 1.5057471264367817,
        "end": 928.98,
        "id": 362,
        "no_speech_prob": 0.0004878536856267601,
        "seek": 91572,
        "start": 927.48,
        "temperature": 0,
        "text": " and I'll see you in the next video.",
        "tokens": [
          50952,
          293,
          286,
          603,
          536,
          291,
          294,
          264,
          958,
          960,
          13,
          51027
        ]
      }
    ],
    "transcription": " Hello, and welcome to the first tutorial video in this new series, Working with Data and APIs in JavaScript. So in this first video, I just want to look at one thing, the WebFetch API. Fetch is a function. It's a wonderful function. It allows you to fetch stuff, data, images, from all sorts of different kinds of places, and do stuff with it. So we're going to start with a very basic example. It's right here. It fetches a rainbow image, a local file, a JPEG file, and displays it on the web page itself. That's going to build. That's the foundation on top of which we will build other examples that are grabbing a spreadsheet of data, and graphing it, or reaching out to some weather service, and getting the temperature based on the latitude and longitude, all sorts of possibilities. We're going to start just by looking at Fetch. The best place for you to learn all of the details, with everything you could ever possibly want to know about the Fetch web API, is on the Mozilla Developer Network site. So that's where I tend to look up documentation stuff for JavaScript stuff I want to do in the browser. I'm going to show you a small slice of what's possible with Fetch, and build on that as we go video, video, to video. There's this idea in web programming of making a request, an HTTP request, a Hypertext Transfer Protocol request. I could make a GET request. Will you please give me? Could I please, could I please get some information? I could make a POST request. Could I please send some information to you? And we're going to do both of those kinds of requests. And Fetch can actually both retrieve and send. But in this example, I just want to look at a simple GET request with Fetch. Let me outline these steps for you. So our program is going to do, the first thing it's going to do is going to call the Fetch function. And it's going to give it a single argument, which is the path to the resource. And in this case, it's going to be a file. And that file is just going to be a local file called rainbow.jpg. The next thing we need to do is deal with the response. So when we call Fetch, a response comes back, presumably with the data we're looking for somewhere in there. I mean, it could be an error, or other things could come back as part of that response. But I'm just going to write down response. Now, this involves this idea of a promise. And the Fetch function is a function that happens asynchronously, meaning we call Fetch, but some time passes because it takes some time to retrieve that data. So how we get the response as part of a promise is a detail that I'm going to have to look at specifically when we get into the code. And I will also include in this video's description some links to further details about how to work with JavaScript promises, some other videos that I've made that might fill in some of the gaps here. The response is actually a stream of data. So we get the response, but we need a whole other step because we need to read that data. We need to complete that data and store it in a format that we can work with. And the kinds of formats we might have is like, oh, it's text data, or it's a blob. Maybe it's image data would come in as a blob. There's all sorts of there's array buffer, and a really, really important one is JSON. So JSON stands for JavaScript Object Notation. And this is going to be a format for storing data that we're going to see again, and again, and again. But in this case, we want to get it as a blob. So I think we could call this third step complete data stream. And when I say complete data stream, I really mean complete and grab the data that's in the body of the response. So there's this concept of the response body, which is pretty important, which is where the data actually is. But it should be noted that there's lots of other metadata about the network communication that's in the response as well that you could look at in certain circumstances. So once we've done that, we've completed the data stream, we've got the image blob, and then we can just, this is really the steps for using fetch. But in this particular scenario, I want to make an image element, make an IMG, HTML DOM element, make an image element with that data. So these are the steps of this example, and I think we can just go code this now. So what I'm beginning with is just some boilerplate HTML. Basic HTML file with nothing in it. And so let's add some stuff to it. So first, let me add an image element to the body. And I'm going to leave the source of the image element blank. But I'm going to give it an ID, and I'm going to call that rainbow. The next thing I want to do is just add a script tag so I can put some JavaScript with, presumably, that fetch function. Now, ultimately, I might want to put my JavaScript code in a separate file. I might want to have a whole build system for my project, but we're just working with the basic ideas here. So I'm going to just do everything in one file, one HTML file that has the HTML stuff and the JavaScript stuff in a script tag. And I'm going to say fetch, and I'm going to fetch rainbow.jpg. I'm going to put that in the code, and I'm also going to add console.log about to fetch a rainbow, because why not? So I'm using something called live server. It's a node package to host this particular HTML page on my computer. So every time that I hit Save, it updates in the browser, and you can see that now I have that console log there. Now, I said earlier that the fetch function returns a promise, and a promise is a way in JavaScript to handle an asynchronous event. It gets resolved when the event is over, when the data is retrieved. And I will refer you to some other videos that go on my channel that go into promises in more depth. But the quick explanation here is that you can use the method then. So the method then is a place where I can handle this response. So right now, what you're seeing is I type the method then. I put an argument response. I use the JavaScript function arrow syntax to then do something with that response. And what I want to do with that response, honestly, is convert it to a blob. But before I do that, let me just say console.log response just so I can see it. So it helps to spell things correctly. Response. And we can see there's that response. And you can see all the metadata associated with the response is here in the JavaScript console. And what I actually want to do is say return response.blob because I want to, this is what we talked about before, I want to convert the response into an image blob. That triggers another promise. So when I have another promise, I can chain them by saying dot then again. And now the response, if we look at that response, console.log response, now I have the response to the next promise. And we can see that is also not defined because I cannot spell to save my life. That is now the blob. So in this case, actually, maybe it makes more sense for me to name this variable something else. Let me just call it blob. And now what you'll see is the sequence is as follows. First, fetch the rainbow. Second, look at the resolved promise and then convert that complete reading the stream of data into a blob. So presumably, once I'm there, all I have to do is say, remember I have this image element. I have this image DOM element with an ID of rainbow. I can say document.getElementByID, give it the ID rainbow, and dot source equals that blob. So this is me just taking the data of that image, which comes in as a blob, and placing it into the source attribute of the image DOM element. You can see that doesn't work because the blob, the data blob, isn't in the format that the image DOM element expects. Luckily for us, there happens to be a function called create object URL that takes a blob object and turns it into the format that an image DOM element would expect. You can look up more about this function also on the MDN JavaScript docs. But all I need to do here is say URL.createObjectURL, pass in the blob. And there it is. We have our rainbow. It's kind of a large image. So I'm just going to add another attribute, width equals 480. So it sort of makes the image smaller. And there we go. Oh, we have a beautiful rainbow. So this really wraps up this particular tutorial. We've got the whole thing. We now know how to call fetch, how to read the response when the promise resolves, how to turn the data of that response into an image blob and add it into an image ELT. But here's the thing. There's a couple other things that I've missed that would be nice to include in this video. Number one is I've done nothing to handle errors. So what if something goes wrong? I should handle an error, at least log the error to the console so I can see that something went wrong. Number two is I want to introduce the JavaScript keywords async and await. And these are new features of the JavaScript language that allow us to handle promises in a slightly more readable and elegant way. So I'm going to show you. I'm going to rewrite this particular example using async and await instead of the.then method. And I will also refer you to some other videos I've made that go into these keywords with a bit more depth. Let's first add some error handling. One of the nice things about working with chaining promises that we have fetch, then this, then this, then this, is an error can happen anywhere throughout there. And I don't have to handle each error in a specific way with each part of that sequence. I can just put at the very end.catch. So like.then, which handles the resolution of the promise,.catch handles stuff that goes wrong when stuff goes wrong during that promise. So I can write catch. I can give an argument error. And I can just say something like console.error, error. So this is a bit more long-winded than it needs to be. There are ways of condensing this. But in a moment, I'm going to do that all with just async and await. I'm not going to worry about it too much. I'm going to make an error happen just so we can see it. To be sure that this is really happening, though, let me put another console log here. This will be my own message, just error exclamation point. And then what I'll do is let me misspell the name of the ID of the DOM element. I will write rainbow with two O's. And then now if I go back to the browser, you're going to see my message error printed out. And also, you can see the actual error messages there. So this is a really nice way of handling errors. One more thing. Let's look at how I can condense the code and make it a bit more readable using async and await. The way this works is if I'm going to use this new keyword await, I can only use the keyword await in the context of an async function. Async is a keyword that specifies this function is going to happen asynchronously. So essentially, what I want to do is write a new function. I'm going to call it async. I'm going to call it catchRainbow. And I've got to declare it as a function. So I could use the keyword function or the arrow syntax. I'm kind of using them interchangeably here. But I'm going to make an async function called catchRainbow. Now when I call fetch, instead of having to use.then, I can still say fetchRainbow.jpg. I can set the result of the fetch function to a variable called response. So this is like the dream of how I like to write code. I just want to say the response equals the result of the fetch function. But this is where the await keyword comes in. Because fetch is an asynchronous function, I have to add await. So this is really just syntactic sugar. It's the same exact thing that I was doing with.then. But because I'm in an async function, I'm allowed to write the code in a single line by saying await the result of fetch and store that result in this variable called response. And then I can do the same thing with the blob. I can say const blob equals await response.blob. And so now in two lines of code, I've said fetch. I've gotten a response. I've said.blob. I've gotten the blob. And then this other line that I have down here, which puts the blob as a URL into the DOM element, I can just take that and put that up there. And you see all of that, all of those lines of code of.then,.then,.then. And I'm going to keep this catch, by the way, because I'm going to need this for something. I'll show you in a second. All of this gets to go away. Now, look at that. It's just three lines of code. And all I have to do now is say, catch rainbow. Just call that function. So I put all the stuff that's asynchronous inside a function that's async, and then just call that function. And now if we go over here, look, it's still working. There's our rainbow. But the reason why I kept that is I do want to keep this catch thing, because if there is an error, let me once again misspell that name of the DOM element, we can see I have now handled that error. I can attach the error to the async function that I've written. And in fact, that function, by definition, an async function, by definition, returns a promise. So I can also say.then. And if I wanted to do something to indicate that it's done, I could say, for example, console.log.yay. And now, oops. Oh, I've got to spell rainbow correctly. And there we go. We can see console.log.yay. So now I have the whole thing. I have written an async function that makes the fetch request. It turns the body of what comes back in the HTTP response into a blob. It converts that blob into the format that the DOM element expects. And I've put a little message saying, yay, it worked, or an error if there's an error. And that's it. That's this full example. Now that we know how to use fetch in the generic sense, and we're working with an image here, in the next video, I'm going to use fetch with a data set. I'm going to grab a comma-separated value file, a CSV file. This is a file from NASA. It's global world temperatures from 1880 to present. And I'm going to show you how to graph those. But before we get there, I might give you a little exercise. You can just move on to the next video to see actually working with the data and how you might chart data file and how you might chart that. But as a little exercise, you might try some things. For example, could you fetch more than one image? How would you? What if you had an array of images? What if your file was not an image file, but maybe it was just a plain text file? Could you put the text that was in that file in a paragraph element? So try one or two of those exercises. I will also include in the GitHub repository that's associated with this video series, I will include solutions to those two exercises. So try multiple images, try a text file, and see if you can get those to appear in your web page. So thanks for watching this tutorial, and I'll see you in the next video.",
    "translation": null
  },
  "error": null,
  "status": "succeeded",
  "created_at": "2023-09-26T21:49:16.250444Z",
  "started_at": "2023-09-26T21:49:16.271767Z",
  "completed_at": "2023-09-26T21:53:06.670157Z",
  "webhook": "https://83ceaa0b612c.ngrok.app/?video_id=tc8DU14qX6I",
  "webhook_events_filter": [
    "completed"
  ],
  "metrics": {
    "predict_time": 230.39839
  },
  "urls": {
    "cancel": "https://api.replicate.com/v1/predictions/2oaewdzbbaa3jizich7q6ihuby/cancel",
    "get": "https://api.replicate.com/v1/predictions/2oaewdzbbaa3jizich7q6ihuby"
  }
}